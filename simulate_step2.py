import json
import os
import sys
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv(".env")

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(os.getcwd())

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì ‘ê·¼)
LOG_DIR = "/home/workspace/PROJECT_WELLO_BEFE/planning-platform/backend/logs/planning_20251206/190558_e3471a9a"
STEP1_RESULT_PATH = os.path.join(LOG_DIR, "step1_result.json")
STEP2_1_PROMPT_PATH = os.path.join(LOG_DIR, "step2_1_prompt.json")
STEP2_1_RESULT_PATH = os.path.join(LOG_DIR, "step2_1_result.json")

def load_json_log(path):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
        # prompt.jsonì˜ ê²½ìš° content ì•ˆì— ì‹¤ì œ JSONì´ ë¬¸ìì—´ë¡œ ë“¤ì–´ìˆì„ ìˆ˜ ìˆìŒ
        if "content" in data and isinstance(data["content"], str):
             try:
                 return json.loads(data["content"])
             except:
                 pass
        return data

def extract_rag_evidence(prompt_json_path):
    """Step 2-1 í”„ë¡¬í”„íŠ¸ì—ì„œ [Critical Evidence] ì„¹ì…˜ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    with open(prompt_json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
        prompt_text = data.get("prompt", "")
        
        start_marker = "# [Critical Evidence: ê²€ìƒ‰ëœ ì˜í•™ ê°€ì´ë“œë¼ì¸]"
        end_marker = "**Evidence & Citation Rules" # í˜¹ì€ ë‹¤ìŒ ì„¹ì…˜ ì‹œì‘ ë¶€ë¶„
        
        if start_marker in prompt_text:
            start_idx = prompt_text.find(start_marker)
            # ë‹¤ìŒ ì„¹ì…˜ ì°¾ê¸° (ëŒ€ì¶© Evidence ì„¹ì…˜ì´ ëë‚˜ëŠ” ì§€ì )
            end_idx = prompt_text.find("# ğŸ¯ Role", start_idx)
            if end_idx == -1:
                end_idx = len(prompt_text)
            
            evidence_section = prompt_text[start_idx:end_idx]
            print(f"[INFO] RAG Evidence ì¶”ì¶œ ì„±ê³µ ({len(evidence_section)}ì)")
            return evidence_section
        else:
            print("[WARN] RAG Evidence ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ""

def simulate_step2_prompt_creation(step1_result, step2_1_result, rag_evidence_text):
    """
    Step 2-2 í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤. (RAG ì£¼ì… ë²„ì „)
    ê¸°ì¡´ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì§€ ì•Šê³ , ë¡œì§ì„ ê°€ì ¸ì™€ì„œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    """
    print("\n[INFO] Step 2-2 í”„ë¡¬í”„íŠ¸ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘...")
    
    # 1. System Instruction (ìˆ˜ì •ëœ ë²„ì „ ì‹œë®¬ë ˆì´ì…˜)
    system_instruction = """
# ğŸ›‘ SYSTEM INSTRUCTION (ì ˆëŒ€ ê·œì¹™)

1. **RAG ìš°ì„  ì›ì¹™**: ì œê³µëœ [Critical Evidence]ì˜ ê°€ì´ë“œë¼ì¸ì„ ìµœìš°ì„ ìœ¼ë¡œ ì ìš©í•˜ì„¸ìš”.
   - ë§Œì•½ Evidenceê°€ "ì‹¬í˜ˆê´€ ìœ„í—˜"ì„ ê²½ê³ í•œë‹¤ë©´, ì•” ê²€ì§„ë³´ë‹¤ ì‹¬í˜ˆê´€ ì •ë°€ê²€ì‚¬ë¥¼ Priority 2ë¡œ ì˜¬ë¦¬ì„¸ìš”.
   - Evidenceì— ì—†ëŠ” ë‚´ìš©ì„ ì–µì§€ë¡œ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

2. **ë§Œì„±ì§ˆí™˜ ìš°ì„  ì›ì¹™**: 
   - í™˜ìì˜ Risk Profileì— 'ê³ í˜ˆì••/ë‹¹ë‡¨/ë¹„ë§Œ' ë“± ë§Œì„±ì§ˆí™˜ ìœ„í—˜ì´ ìˆë‹¤ë©´, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•©ë³‘ì¦ ê²€ì‚¬ë¥¼ 'ì•” ê²€ì§„'ë³´ë‹¤ ë¨¼ì € ì¶”ì²œí•˜ì„¸ìš”.
   - **Bridge Strategy ì ìš© ì‹œ**: ì•” ê´€ë ¨ ì˜ˆì‹œë³´ë‹¤ 'í˜ˆê´€/ëŒ€ì‚¬/í™œë ¥' ê´€ë ¨ ë…¼ë¦¬ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ì„¸ìš”.

3. **Tone & Manner (ì§„ë£Œì‹¤ ëŒ€í™”ì²´)**:
   - "ê¶Œì¥ë©ë‹ˆë‹¤" (X) -> "ì œê°€ ë³´ê¸°ì—” ì´ ê²€ì‚¬ê°€ ê¼­ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤" (O)
   - ë”±ë”±í•œ ê¸°ê³„ì  ë§íˆ¬ë¥¼ ë²„ë¦¬ê³ , í™˜ìë¥¼ ê±±ì •í•˜ëŠ” 'ì£¼ì¹˜ì˜'ì˜ ë”°ëœ»í•˜ì§€ë§Œ ë‹¨í˜¸í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
"""

    # 2. RAG Evidence ì£¼ì… (ì—¬ê¸°ê°€ í•µì‹¬!)
    rag_section = ""
    if rag_evidence_text:
        rag_section = f"""
{rag_evidence_text}

**âš ï¸ ì´ ì„¹ì…˜ì˜ ì¸ìš©êµ¬ë¥¼ 'strategies'ì˜ 'reason'ê³¼ 'evidence' í•„ë“œì— ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.**
"""
    else:
        rag_section = "\n# [Critical Evidence]\n(ê²€ìƒ‰ëœ ì¦ê±° ì—†ìŒ - ì¼ë°˜ ì˜í•™ ì§€ì‹ ì‚¬ìš©)\n"

    # 3. Context ì¡°ë¦½
    prompt_parts = [
        system_instruction,
        rag_section,
        "\n# ğŸ¯ Role (ë‹¹ì‹ ì˜ ì—­í• )\në‹¹ì‹ ì€ ëŒ€í•™ë³‘ì› ê²€ì§„ì„¼í„°ì¥ì´ì ì˜ˆë°©ì˜í•™ ì „ë¬¸ì˜ì…ë‹ˆë‹¤.\n",
        "\n# ğŸ“‹ Context\n",
        f"## STEP 1 ë¶„ì„ ê²°ê³¼\n{json.dumps(step1_result, ensure_ascii=False, indent=2)}\n",
        f"## STEP 2-1 ê²°ê³¼ (Priority 1)\n{json.dumps(step2_1_result, ensure_ascii=False, indent=2)}\n"
    ]
    
    # 4. Task ë° Output Format (Anchor ì§€ì‹œë¬¸ ë³´ê°• ì‹œë®¬ë ˆì´ì…˜)
    task_section = """
# ğŸ¯ Task - Upselling ì „ëµ ìˆ˜ë¦½

STEP 1ì˜ ìœ„í—˜ ìš”ì¸ê³¼ STEP 2-1ì˜ ê¸°ë³¸ ê²€ì‚¬ë¥¼ ì—°ê²°í•˜ì—¬, **"ì™œ ì •ë°€ ê²€ì‚¬ê°€ í•„ìš”í•œì§€"** ì„¤ë“í•˜ëŠ” ë…¼ë¦¬(Bridge Strategy)ë¥¼ ì™„ì„±í•˜ì„¸ìš”.

## âš ï¸ Bridge Strategy ì‘ì„± ê·œì¹™ (Few-shot Examples)

**ì˜ëª»ëœ ì˜ˆ (ë‹¨ìˆœ ë‚˜ì—´):**
- anchor: "ê³ í˜ˆì••ì´ ìˆìŠµë‹ˆë‹¤."
- gap: "ë” ìì„¸íˆ ë´ì•¼ í•©ë‹ˆë‹¤."
- offer: "ì´ˆìŒíŒŒë¥¼ í•˜ì„¸ìš”."

**âœ… ì˜¬ë°”ë¥¸ ì˜ˆ 1 (ì„ìƒì  ì—°ê²°):**
- anchor: "í˜„ì¬ í˜ˆì••ì´ 140/90ìœ¼ë¡œ ë†’ê²Œ ì¸¡ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” í˜ˆê´€ë²½ì— ë†’ì€ ì••ë ¥ì´ ê°€í•´ì§€ê³  ìˆë‹¤ëŠ” ì‹ í˜¸ì…ë‹ˆë‹¤."
- gap: "í•˜ì§€ë§Œ í˜ˆì•• ìˆ˜ì¹˜ë§Œìœ¼ë¡œëŠ” í˜ˆê´€ ë‚´ë¶€ê°€ ì–¼ë§ˆë‚˜ ë‘êº¼ì›Œì¡ŒëŠ”ì§€, ì°Œêº¼ê¸°(í”Œë¼í¬)ê°€ ìŒ“ì—¬ ë‡Œì¡¸ì¤‘ ìœ„í—˜ì´ ì–¼ë§ˆë‚˜ ë†’ì€ì§€ëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
- offer: "ê²½ë™ë§¥ ì´ˆìŒíŒŒë¥¼ í†µí•´ í˜ˆê´€ ì†ì„ ì§ì ‘ ë“¤ì—¬ë‹¤ë³´ê³ , ë‡Œì¡¸ì¤‘ì„ ì˜ˆë°©í•  ê³¨ë“ íƒ€ì„ì„ ì¡ì•„ì•¼ í•©ë‹ˆë‹¤."

**âœ… ì˜¬ë°”ë¥¸ ì˜ˆ 2 (ì¦ìƒ ì—°ê²°):**
- anchor: "ë¬¸ì§„ì—ì„œ 'ê°€ë” ê°€ìŠ´ì´ ë‹µë‹µí•˜ë‹¤'ê³  í•˜ì…¨ê³ , ê°€ì¡±ë ¥ì— ì‹¬ê·¼ê²½ìƒ‰ì´ ìˆìŠµë‹ˆë‹¤."
- gap: "ê¸°ë³¸ ì‹¬ì „ë„ ê²€ì‚¬ëŠ” 'ê²€ì‚¬í•˜ëŠ” ìˆœê°„'ì˜ ì´ìƒë§Œ ì¡ì•„ë‚¼ ë¿, í˜ˆê´€ì´ 70% ì´ìƒ ë§‰íˆê¸° ì „ê¹Œì§€ëŠ” ì •ìƒìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤."
- offer: "ê´€ìƒë™ë§¥ ì„íšŒí™” CTë¡œ ì‹¬ì¥ í˜ˆê´€ì˜ 'ì§„ì§œ ë‚˜ì´'ë¥¼ í™•ì¸í•´ë³´ëŠ” ê²ƒì´ ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•ì…ë‹ˆë‹¤."

---

# Output Format (JSON)
(ê¸°ì¡´ JSON í¬ë§· ìœ ì§€)
"""
    prompt_parts.append(task_section)
    
    return "\n".join(prompt_parts)

    
    return "\n".join(prompt_parts)

async def call_llm_with_prompt(prompt_text):
    """ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì‹¤ì œ LLMì„ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    print("\n[INFO] LLM í˜¸ì¶œ ì‹œì‘ (Gemini-2.0-flash)...")
    
    try:
        # ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ëª¨ë“ˆ ë¡œë“œ
        from app.services.gemini_service import gemini_service, GeminiRequest
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        await gemini_service.initialize()
        
        # ìš”ì²­ ê°ì²´ ìƒì„±
        request = GeminiRequest(
            prompt=prompt_text,
            model="gemini-2.0-flash",
            temperature=0.5,
            max_tokens=4000,
            response_format={"type": "json_object"}
        )
        
        # API í˜¸ì¶œ (ë¡œê¹… ì—†ì´ ì§ì ‘ í˜¸ì¶œ)
        # GeminiService.generate_content ë©”ì„œë“œë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ê±°ë‚˜ call_api ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” call_api ëª¨ì˜ í˜¸ì¶œ ëŒ€ì‹  ì§ì ‘ genai ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ì´ ì–´ë ¤ìš°ë¯€ë¡œ
        # ê¸°ì¡´ ì„œë¹„ìŠ¤ì˜ call_apië¥¼ í™œìš©í•˜ë˜, ë¡œê¹…ì€ ìµœì†Œí™”
        
        response = await gemini_service.call_api(
            request,
            save_log=False,
            step_name="Simulation"
        )
        
        if response.success:
            print("[INFO] LLM ì‘ë‹µ ìˆ˜ì‹  ì„±ê³µ")
            return response.content
        else:
            print(f"[ERROR] LLM í˜¸ì¶œ ì‹¤íŒ¨: {response.error}")
            return None
            
    except Exception as e:
        print(f"[ERROR] LLM í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        # import traceback
        # traceback.print_exc()
        return None

# --- ë©”ì¸ ì‹¤í–‰ ---
import asyncio

async def main():
    print("=== RAG Evidence ì£¼ì… ë° LLM ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ì‹œë®¬ë ˆì´ì…˜ ===")

    # 1. ë°ì´í„° ë¡œë“œ
    step1_data = load_json_log(STEP1_RESULT_PATH)
    step2_1_result = load_json_log(STEP2_1_RESULT_PATH)

    # 2. RAG Evidence ì¶”ì¶œ (ë¡œê·¸ì—ì„œ)
    rag_evidence = extract_rag_evidence(STEP2_1_PROMPT_PATH)

    if not rag_evidence:
        print("âŒ RAG Evidence ì¶”ì¶œ ì‹¤íŒ¨. ì‹œë®¬ë ˆì´ì…˜ ì¤‘ë‹¨.")
        return

    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œë®¬ë ˆì´ì…˜ (í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œë¡œ ë³€ê²½)
    print("\n[INFO] ë°±ì—”ë“œ í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        from app.services.checkup_design.step2_upselling import create_checkup_design_prompt_step2_upselling
        
        # Step 1 ë°ì´í„° ì¤€ë¹„ (Dictë¡œ ë³€í™˜ í•„ìš”)
        if isinstance(step1_data, str):
             step1_data = json.loads(step1_data)
             
        # ğŸ”¥ í˜ë¥´ì†Œë‚˜ ë°ì´í„° ê°•ì œ ì£¼ì… (í…ŒìŠ¤íŠ¸ìš©)
        step1_data["persona"] = {
            "type": "Worrier",
            "description": "ê±´ê°•ì—¼ë ¤í˜•",
            "strategy_key": "reassurance"
        }
        
        # ê°€ì§œ ë°ì´í„° ì¤€ë¹„
        patient_name = "í…ŒìŠ¤íŠ¸í™˜ì"
        patient_age = 55
        patient_gender = "M"
        selected_concerns = []
        
        # í•¨ìˆ˜ í˜¸ì¶œ
        prompt, _ = await create_checkup_design_prompt_step2_upselling(
            step1_result=step1_data,
            step2_1_result=step2_1_result,
            patient_name=patient_name,
            patient_age=patient_age,
            patient_gender=patient_gender,
            selected_concerns=selected_concerns,
            prev_rag_context=rag_evidence  # í•µì‹¬: RAG Evidence ì£¼ì…!
        )
        
        simulated_prompt = prompt
        print(f"âœ… ë°±ì—”ë“œ í•¨ìˆ˜ í˜¸ì¶œ ì„±ê³µ! í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
        
    except ImportError:
        print("âš ï¸ ë°±ì—”ë“œ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨, ê¸°ì¡´ ì‹œë®¬ë ˆì´ì…˜ ë¡œì§ ì‚¬ìš©")
        simulated_prompt = simulate_step2_prompt_creation(step1_data, step2_1_result, rag_evidence)
    except Exception as e:
        print(f"âŒ ë°±ì—”ë“œ í•¨ìˆ˜ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return

    # 4. ê²°ê³¼ ì €ì¥
    output_path = "simulated_step2_prompt_real.txt"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(simulated_prompt)
    print(f"\nâœ… í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ: {output_path}")

    # 5. LLM ì‹¤ì œ í˜¸ì¶œ ë° ê²°ê³¼ ê²€ì¦ (NEW)
    llm_response = await call_llm_with_prompt(simulated_prompt)
    
    if llm_response:
        # JSON íŒŒì‹± ë° ì €ì¥
        try:
            # ë§ˆí¬ë‹¤ìš´ ì œê±° ì²˜ë¦¬
            content = llm_response.strip()
            if content.startswith("```json"): content = content[7:]
            if content.startswith("```"): content = content[3:]
            if content.endswith("```"): content = content[:-3]
            
            result_json = json.loads(content.strip())
            
            result_path = "simulated_step2_result.json"
            with open(result_path, "w", encoding="utf-8") as f:
                json.dump(result_json, f, ensure_ascii=False, indent=2)
                
            print(f"\nâœ… LLM ì‘ë‹µ ì €ì¥ ì™„ë£Œ: {result_path}")
            
            # 6. í’ˆì§ˆ ê²€ì¦ ë¦¬í¬íŠ¸
            print("\nğŸ” [í’ˆì§ˆ ê²€ì¦ ë¦¬í¬íŠ¸]")
            
            strategies = result_json.get("strategies", [])
            print(f"- ìƒì„±ëœ ì „ëµ ê°œìˆ˜: {len(strategies)}ê°œ")
            
            for idx, strategy in enumerate(strategies):
                print(f"\n[ì „ëµ {idx+1}: {strategy.get('target', 'Target ì—†ìŒ')}]")
                
                # Anchor ê²€ì¦
                anchor = strategy.get('step1_anchor', '')
                print(f"  - Anchor: {anchor[:50]}...")
                if "ì‘ì„±" in anchor and "ë¶„ì„" in anchor:
                    print("    âŒ FAIL: ì§€ì‹œë¬¸ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•¨")
                else:
                    print("    âœ… PASS: êµ¬ì²´ì  ë‚´ìš© ìƒì„±ë¨")
                    
                # Evidence ê²€ì¦
                rec = strategy.get('doctor_recommendation', {})
                evidence = rec.get('evidence', '')
                print(f"  - Evidence: {evidence[:50]}...")
                if "ë”°ë¥´ë©´" in evidence or "ëª…ì‹œë˜ì–´" in evidence:
                    print("    âœ… PASS: ì¸ìš©êµ¬ í˜•ì‹ ì¤€ìˆ˜")
                else:
                    print("    âš ï¸ WARN: ì¸ìš©êµ¬ í˜•ì‹ ë¯¸ì¤€ìˆ˜ ê°€ëŠ¥ì„±")

        except Exception as e:
            print(f"âŒ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            print(f"ì›ë³¸ ì‘ë‹µ: {llm_response[:200]}...")

if __name__ == "__main__":
    asyncio.run(main())

