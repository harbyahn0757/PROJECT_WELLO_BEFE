#!/usr/bin/env python3
"""
GPT ëª¨ë¸ í˜¸ì¶œ ì‹œ í† í° ìš©ëŸ‰ ì ê²€ ìŠ¤í¬ë¦½íŠ¸
ê° ë‹¨ê³„ë³„ë¡œ í”„ë¡¬í”„íŠ¸/ì‘ë‹µ í¬ê¸°ì™€ ëª¨ë¸ ì œí•œì„ ë¹„êµí•©ë‹ˆë‹¤.
"""
import json
import os
from typing import Dict, Any

# GPT ëª¨ë¸ë³„ í† í° ì œí•œ (ê³µì‹ ìŠ¤í™)
MODEL_LIMITS = {
    "gpt-4o-mini": {
        "context_window": 128000,  # 128K í† í°
        "max_output": 16384,        # 16K í† í°
        "input_cost_per_1m": 0.15,  # $0.15/1M tokens
        "output_cost_per_1m": 0.60  # $0.60/1M tokens
    },
    "gpt-4o": {
        "context_window": 128000,  # 128K í† í°
        "max_output": 16384,        # 16K í† í°
        "input_cost_per_1m": 2.50,  # $2.50/1M tokens
        "output_cost_per_1m": 10.00 # $10.00/1M tokens
    }
}

def estimate_tokens(text: str) -> int:
    """
    í† í° ìˆ˜ ì¶”ì • (ì˜ì–´: 4ì/í† í°, í•œê¸€: 2.5ì/í† í°)
    ì •í™•í•œ ê³„ì‚°ì€ tiktoken í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì¶”ì •
    """
    # í•œê¸€ê³¼ ì˜ì–´ ë¹„ìœ¨ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ í‰ê· ì ìœ¼ë¡œ
    # í•œê¸€+ì˜ì–´ í˜¼í•© í…ìŠ¤íŠ¸ëŠ” ì•½ 3ìë‹¹ 1í† í°ìœ¼ë¡œ ì¶”ì •
    return len(text) // 3

def check_step_capacity(step_name: str, log_data: Dict[str, Any], model: str) -> Dict[str, Any]:
    """íŠ¹ì • ìŠ¤í…ì˜ ìš©ëŸ‰ ì²´í¬"""
    
    system_msg = log_data.get("system_message", "")
    user_msg = log_data.get("user_message", "")
    
    # í† í° ì¶”ì •
    system_tokens = estimate_tokens(system_msg)
    user_tokens = estimate_tokens(user_msg)
    total_input_tokens = system_tokens + user_tokens
    
    # ëª¨ë¸ ì œí•œ
    limits = MODEL_LIMITS.get(model, MODEL_LIMITS["gpt-4o"])
    context_limit = limits["context_window"]
    output_limit = limits["max_output"]
    max_tokens_requested = log_data.get("max_tokens", 4096)
    
    # ë‚¨ì€ ìš©ëŸ‰ ê³„ì‚°
    remaining_for_output = context_limit - total_input_tokens
    
    # ìš©ëŸ‰ ì²´í¬
    is_input_ok = total_input_tokens < context_limit
    is_output_ok = max_tokens_requested <= output_limit
    is_total_ok = total_input_tokens + max_tokens_requested < context_limit
    
    return {
        "step": step_name,
        "model": model,
        "input": {
            "system_tokens": system_tokens,
            "user_tokens": user_tokens,
            "total_tokens": total_input_tokens,
            "system_chars": len(system_msg),
            "user_chars": len(user_msg),
        },
        "output": {
            "requested_max_tokens": max_tokens_requested,
            "model_max_tokens": output_limit,
        },
        "limits": {
            "context_window": context_limit,
            "remaining_for_output": remaining_for_output,
        },
        "status": {
            "input_ok": is_input_ok,
            "output_ok": is_output_ok,
            "total_ok": is_total_ok,
            "usage_percent": round((total_input_tokens / context_limit) * 100, 2)
        }
    }

def check_response_data(step_name: str, response_file: str) -> Dict[str, Any]:
    """ì‘ë‹µ ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ì „ë‹¬ë˜ëŠ”ì§€ ì²´í¬"""
    
    if not os.path.exists(response_file):
        return {"error": f"ì‘ë‹µ íŒŒì¼ ì—†ìŒ: {response_file}"}
    
    with open(response_file, 'r', encoding='utf-8') as f:
        response_data = json.load(f)
    
    response_text = response_data.get("response", "")
    response_tokens = estimate_tokens(response_text)
    
    # JSON íŒŒì‹± ì²´í¬
    try:
        parsed = json.loads(response_text)
        is_valid_json = True
        json_keys = list(parsed.keys())
    except:
        is_valid_json = False
        json_keys = []
    
    return {
        "step": step_name,
        "response_length": len(response_text),
        "response_tokens": response_tokens,
        "is_valid_json": is_valid_json,
        "json_keys": json_keys,
        "has_required_fields": bool(parsed) if is_valid_json else False
    }

def main():
    log_dir = "logs"
    
    # ìµœì‹  ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
    prompt_files = sorted([f for f in os.listdir(log_dir) if f.startswith("gpt_prompt_")])
    response_files = sorted([f for f in os.listdir(log_dir) if f.startswith("gpt_response_")])
    
    if not prompt_files or not response_files:
        print("âŒ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\n" + "="*80)
    print("ğŸ” GPT ëª¨ë¸ í˜¸ì¶œ ìš©ëŸ‰ ë° ë°ì´í„° ì „ë‹¬ ì ê²€")
    print("="*80)
    
    # STEP 1 ì²´í¬
    print("\n" + "-"*80)
    print("ğŸ“Š STEP 1: ê±´ê°• ë¶„ì„ (Risk Stratification)")
    print("-"*80)
    
    step1_prompt = os.path.join(log_dir, prompt_files[0])
    step1_response = os.path.join(log_dir, response_files[0])
    
    with open(step1_prompt, 'r', encoding='utf-8') as f:
        step1_data = json.load(f)
    
    step1_capacity = check_step_capacity("STEP 1", step1_data, step1_data.get("model", "gpt-4o-mini"))
    step1_response_check = check_response_data("STEP 1", step1_response)
    
    print(f"ëª¨ë¸: {step1_capacity['model']}")
    print(f"ì…ë ¥:")
    print(f"  - ì‹œìŠ¤í…œ ë©”ì‹œì§€: {step1_capacity['input']['system_chars']:,}ì (~{step1_capacity['input']['system_tokens']:,} í† í°)")
    print(f"  - ì‚¬ìš©ì ë©”ì‹œì§€: {step1_capacity['input']['user_chars']:,}ì (~{step1_capacity['input']['user_tokens']:,} í† í°)")
    print(f"  - ì´ ì…ë ¥: ~{step1_capacity['input']['total_tokens']:,} í† í°")
    print(f"ì¶œë ¥:")
    print(f"  - ìš”ì²­í•œ max_tokens: {step1_capacity['output']['requested_max_tokens']:,}")
    print(f"  - ì‹¤ì œ ì‘ë‹µ: {step1_response_check['response_tokens']:,} í† í°")
    print(f"ìš©ëŸ‰:")
    print(f"  - Context Window: {step1_capacity['limits']['context_window']:,} í† í°")
    print(f"  - ì‚¬ìš©ë¥ : {step1_capacity['status']['usage_percent']}%")
    print(f"  - ë‚¨ì€ ì¶œë ¥ ìš©ëŸ‰: {step1_capacity['limits']['remaining_for_output']:,} í† í°")
    print(f"ìƒíƒœ:")
    print(f"  - ì…ë ¥ ìš©ëŸ‰ OK: {'âœ…' if step1_capacity['status']['input_ok'] else 'âŒ'}")
    print(f"  - ì¶œë ¥ ìš©ëŸ‰ OK: {'âœ…' if step1_capacity['status']['output_ok'] else 'âŒ'}")
    print(f"  - ì „ì²´ ìš©ëŸ‰ OK: {'âœ…' if step1_capacity['status']['total_ok'] else 'âŒ'}")
    print(f"ì‘ë‹µ ë°ì´í„°:")
    print(f"  - JSON íŒŒì‹± ì„±ê³µ: {'âœ…' if step1_response_check['is_valid_json'] else 'âŒ'}")
    if step1_response_check['is_valid_json']:
        print(f"  - JSON í‚¤: {', '.join(step1_response_check['json_keys'])}")
    
    # STEP 2-1 ì²´í¬
    print("\n" + "-"*80)
    print("ğŸ“Š STEP 2-1: Priority 1 ê²€ì§„ ì„¤ê³„")
    print("-"*80)
    
    step2_1_prompt = os.path.join(log_dir, prompt_files[1])
    step2_1_response = os.path.join(log_dir, response_files[1])
    
    with open(step2_1_prompt, 'r', encoding='utf-8') as f:
        step2_1_data = json.load(f)
    
    step2_1_capacity = check_step_capacity("STEP 2-1", step2_1_data, step2_1_data.get("model", "gpt-4o"))
    step2_1_response_check = check_response_data("STEP 2-1", step2_1_response)
    
    print(f"ëª¨ë¸: {step2_1_capacity['model']}")
    print(f"ì…ë ¥:")
    print(f"  - ì‹œìŠ¤í…œ ë©”ì‹œì§€: {step2_1_capacity['input']['system_chars']:,}ì (~{step2_1_capacity['input']['system_tokens']:,} í† í°)")
    print(f"  - ì‚¬ìš©ì ë©”ì‹œì§€: {step2_1_capacity['input']['user_chars']:,}ì (~{step2_1_capacity['input']['user_tokens']:,} í† í°)")
    print(f"  - ì´ ì…ë ¥: ~{step2_1_capacity['input']['total_tokens']:,} í† í°")
    print(f"  - âš ï¸ STEP 1 ê²°ê³¼ í¬í•¨ë¨")
    print(f"ì¶œë ¥:")
    print(f"  - ìš”ì²­í•œ max_tokens: {step2_1_capacity['output']['requested_max_tokens']:,}")
    print(f"  - ì‹¤ì œ ì‘ë‹µ: {step2_1_response_check['response_tokens']:,} í† í°")
    print(f"ìš©ëŸ‰:")
    print(f"  - Context Window: {step2_1_capacity['limits']['context_window']:,} í† í°")
    print(f"  - ì‚¬ìš©ë¥ : {step2_1_capacity['status']['usage_percent']}%")
    print(f"  - ë‚¨ì€ ì¶œë ¥ ìš©ëŸ‰: {step2_1_capacity['limits']['remaining_for_output']:,} í† í°")
    print(f"ìƒíƒœ:")
    print(f"  - ì…ë ¥ ìš©ëŸ‰ OK: {'âœ…' if step2_1_capacity['status']['input_ok'] else 'âŒ'}")
    print(f"  - ì¶œë ¥ ìš©ëŸ‰ OK: {'âœ…' if step2_1_capacity['status']['output_ok'] else 'âŒ'}")
    print(f"  - ì „ì²´ ìš©ëŸ‰ OK: {'âœ…' if step2_1_capacity['status']['total_ok'] else 'âŒ'}")
    print(f"ì‘ë‹µ ë°ì´í„°:")
    print(f"  - JSON íŒŒì‹± ì„±ê³µ: {'âœ…' if step2_1_response_check['is_valid_json'] else 'âŒ'}")
    if step2_1_response_check['is_valid_json']:
        print(f"  - JSON í‚¤: {', '.join(step2_1_response_check['json_keys'])}")
    
    # STEP 2-2 ì²´í¬
    print("\n" + "-"*80)
    print("ğŸ“Š STEP 2-2: Priority 2,3 + Upselling")
    print("-"*80)
    
    step2_2_prompt = os.path.join(log_dir, prompt_files[2])
    step2_2_response = os.path.join(log_dir, response_files[2])
    
    with open(step2_2_prompt, 'r', encoding='utf-8') as f:
        step2_2_data = json.load(f)
    
    step2_2_capacity = check_step_capacity("STEP 2-2", step2_2_data, step2_2_data.get("model", "gpt-4o"))
    step2_2_response_check = check_response_data("STEP 2-2", step2_2_response)
    
    print(f"ëª¨ë¸: {step2_2_capacity['model']}")
    print(f"ì…ë ¥:")
    print(f"  - ì‹œìŠ¤í…œ ë©”ì‹œì§€: {step2_2_capacity['input']['system_chars']:,}ì (~{step2_2_capacity['input']['system_tokens']:,} í† í°)")
    print(f"  - ì‚¬ìš©ì ë©”ì‹œì§€: {step2_2_capacity['input']['user_chars']:,}ì (~{step2_2_capacity['input']['user_tokens']:,} í† í°)")
    print(f"  - ì´ ì…ë ¥: ~{step2_2_capacity['input']['total_tokens']:,} í† í°")
    print(f"  - âš ï¸ STEP 1 + STEP 2-1 ê²°ê³¼ í¬í•¨ë¨")
    print(f"ì¶œë ¥:")
    print(f"  - ìš”ì²­í•œ max_tokens: {step2_2_capacity['output']['requested_max_tokens']:,}")
    print(f"  - ì‹¤ì œ ì‘ë‹µ: {step2_2_response_check['response_tokens']:,} í† í°")
    print(f"ìš©ëŸ‰:")
    print(f"  - Context Window: {step2_2_capacity['limits']['context_window']:,} í† í°")
    print(f"  - ì‚¬ìš©ë¥ : {step2_2_capacity['status']['usage_percent']}%")
    print(f"  - ë‚¨ì€ ì¶œë ¥ ìš©ëŸ‰: {step2_2_capacity['limits']['remaining_for_output']:,} í† í°")
    print(f"ìƒíƒœ:")
    print(f"  - ì…ë ¥ ìš©ëŸ‰ OK: {'âœ…' if step2_2_capacity['status']['input_ok'] else 'âŒ'}")
    print(f"  - ì¶œë ¥ ìš©ëŸ‰ OK: {'âœ…' if step2_2_capacity['status']['output_ok'] else 'âŒ'}")
    print(f"  - ì „ì²´ ìš©ëŸ‰ OK: {'âœ…' if step2_2_capacity['status']['total_ok'] else 'âŒ'}")
    print(f"ì‘ë‹µ ë°ì´í„°:")
    print(f"  - JSON íŒŒì‹± ì„±ê³µ: {'âœ…' if step2_2_response_check['is_valid_json'] else 'âŒ'}")
    if step2_2_response_check['is_valid_json']:
        print(f"  - JSON í‚¤: {', '.join(step2_2_response_check['json_keys'])}")
    
    # ë°ì´í„° ì „ë‹¬ ì²´í¬
    print("\n" + "="*80)
    print("ğŸ”— ë°ì´í„° ì „ë‹¬ íë¦„ ê²€ì¦")
    print("="*80)
    
    # STEP 1 â†’ STEP 2-1 ì „ë‹¬ ì²´í¬
    print("\nâœ… STEP 1 â†’ STEP 2-1 ì „ë‹¬:")
    with open(step2_1_prompt, 'r', encoding='utf-8') as f:
        step2_1_full = json.load(f)
    
    step1_in_step2_1 = "STEP 1 ë¶„ì„ ê²°ê³¼" in step2_1_full.get("user_message", "")
    print(f"  - STEP 1 ê²°ê³¼ í¬í•¨: {'âœ…' if step1_in_step2_1 else 'âŒ'}")
    
    if step1_in_step2_1:
        # STEP 1 ê²°ê³¼ê°€ ì •í™•íˆ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
        with open(step1_response, 'r', encoding='utf-8') as f:
            step1_resp = json.load(f)
            step1_parsed = json.loads(step1_resp["response"])
            
        print(f"  - patient_summary ì „ë‹¬: {'âœ…' if 'patient_summary' in str(step2_1_full['user_message']) else 'âŒ'}")
        print(f"  - analysis ì „ë‹¬: {'âœ…' if 'analysis' in str(step2_1_full['user_message']) else 'âŒ'}")
    
    # STEP 2-1 â†’ STEP 2-2 ì „ë‹¬ ì²´í¬
    print("\nâœ… STEP 2-1 â†’ STEP 2-2 ì „ë‹¬:")
    with open(step2_2_prompt, 'r', encoding='utf-8') as f:
        step2_2_full = json.load(f)
    
    step2_1_in_step2_2 = "STEP 2-1 ê²°ê³¼" in step2_2_full.get("user_message", "")
    step1_in_step2_2 = "STEP 1 ë¶„ì„ ê²°ê³¼" in step2_2_full.get("user_message", "")
    
    print(f"  - STEP 1 ê²°ê³¼ í¬í•¨: {'âœ…' if step1_in_step2_2 else 'âŒ'}")
    print(f"  - STEP 2-1 ê²°ê³¼ í¬í•¨: {'âœ…' if step2_1_in_step2_2 else 'âŒ'}")
    
    if step2_1_in_step2_2:
        print(f"  - priority_1 ë°ì´í„° ì „ë‹¬: {'âœ…' if 'priority_1' in str(step2_2_full['user_message']) else 'âŒ'}")
    
    # ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“‹ ìš”ì•½")
    print("="*80)
    
    total_steps_ok = (
        step1_capacity['status']['total_ok'] and
        step2_1_capacity['status']['total_ok'] and
        step2_2_capacity['status']['total_ok']
    )
    
    total_responses_ok = (
        step1_response_check['is_valid_json'] and
        step2_1_response_check['is_valid_json'] and
        step2_2_response_check['is_valid_json']
    )
    
    data_flow_ok = step1_in_step2_1 and step2_1_in_step2_2 and step1_in_step2_2
    
    print(f"1. ëª¨ë“  ìŠ¤í… ìš©ëŸ‰ ì í•©: {'âœ…' if total_steps_ok else 'âŒ'}")
    print(f"2. ëª¨ë“  ì‘ë‹µ JSON íŒŒì‹± ì„±ê³µ: {'âœ…' if total_responses_ok else 'âŒ'}")
    print(f"3. ë°ì´í„° ì „ë‹¬ íë¦„ ì •ìƒ: {'âœ…' if data_flow_ok else 'âŒ'}")
    
    if total_steps_ok and total_responses_ok and data_flow_ok:
        print("\nğŸ‰ ëª¨ë“  ì²´í¬ í•­ëª© í†µê³¼! ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ì¼ë¶€ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    print("\n" + "="*80 + "\n")

if __name__ == "__main__":
    main()

