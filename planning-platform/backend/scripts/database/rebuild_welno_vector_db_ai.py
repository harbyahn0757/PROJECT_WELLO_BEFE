"""
ì›°ë¡œ ë²¡í„° DB AI ëª¨ë¸ ê¸°ë°˜ í†µí•© ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (LlamaParse ë²„ì „)
- LlamaParse: AI ê¸°ë°˜ ë¬¸ì„œ íŒŒì‹± (í‘œ, ì´ë¯¸ì§€, ë ˆì´ì•„ì›ƒ ì™„ë²½ ì§€ì›)
- OpenAI Embedding: text-embedding-ada-002
- ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì› (ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ í›„ ì¶”ê°€)
- íŠ¹ì • í´ë” íƒ€ê²ŸíŒ… ì§€ì› (NFD/NFC ì •ê·œí™” ëŒ€ì‘)
"""

import os
import sys
import time
import gc
import json
import unicodedata
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime

# LlamaIndex & LlamaParse
from llama_index.core import Document, VectorStoreIndex, StorageContext, Settings, load_index_from_storage
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_parse import LlamaParse

# FAISS
import faiss
from llama_index.vector_stores.faiss import FaissVectorStore

# Excel/CSV
import pandas as pd

# ì„ë² ë”© ìƒìˆ˜ (rag_serviceì™€ ë™ì¼ ìœ ì§€)
EMBEDDING_MODEL = "text-embedding-ada-002"
EMBEDDING_DIMENSION = 1536


class WelnoAIVectorDBManager:
    """AI ëª¨ë¸ ê¸°ë°˜ ë²¡í„° DB í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self, target_folders: Optional[Union[str, List[str]]] = None, batch_size: int = 3):
        # ê²½ë¡œ ì„¤ì • (ê¸°ë³¸ê°’: ì „ì²´ raw_data)
        self.raw_data_root = Path("/data/raw_data")
        if isinstance(target_folders, str):
            self.target_folders = [target_folders]
        else:
            self.target_folders = target_folders
            
        self.db_dir = Path("/data/vector_db/welno/faiss_db")
        self.batch_size = batch_size
        self.start_time = time.time()
        
        self.db_dir.mkdir(parents=True, exist_ok=True)
        
        # API í‚¤ ë° ì„¤ì •
        self._load_env()
        
        # LlamaParse ì´ˆê¸°í™” (ê³ ì„±ëŠ¥ ì˜µì…˜ ì ìš©)
        self.parser = LlamaParse(
            api_key=self.llamaindex_api_key,
            result_type="markdown",
            language="ko",
            num_workers=4,
            verbose=True,
            # í‘œì™€ ì´ë¯¸ì§€ ì¶”ì¶œ í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•œ ì¶”ê°€ ì§€ì¹¨
            parsing_instruction="This document contains medical guidelines and health data tables. Please extract all tables accurately in markdown format and describe images or charts if possible."
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ì°¨ì› í˜¸í™˜ ìœ ì§€)
        self.embed_model = OpenAIEmbedding(
            model=EMBEDDING_MODEL,
            api_key=self.openai_api_key
        )
        Settings.embed_model = self.embed_model
        
        print("\n" + "="*80)
        print("ğŸš€ ì›°ë¡œ AI ë²¡í„° DB í†µí•© ê´€ë¦¬ ì‹œìŠ¤í…œ")
        print("="*80)
        print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“ íƒ€ê²Ÿ í´ë”ë“¤: {self.target_folders or 'ì „ì²´'}")
        print(f"ğŸ’¾ DB ê²½ë¡œ: {self.db_dir}")
        print("="*80 + "\n")

    def _load_env(self):
        """í™˜ê²½ë³€ìˆ˜ ë¡œë“œ"""
        env_path = Path(os.environ.get("PROJECT_ROOT", "/home/workspace/PROJECT_WELNO_BEFE")) / "planning-platform/backend/config.env"
        if env_path.exists():
            with open(env_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'): continue
                    if '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key] = value
        
        # ì£¼ì„ ì²˜ë¦¬ëœ í‚¤ ê°•ì œ ë¡œë“œ ì‹œë„
        if not os.environ.get('LLAMAINDEX_API_KEY'):
            with open(env_path, 'r') as f:
                content = f.read()
                import re
                match = re.search(r'#?LLAMAINDEX_API_KEY=(.+)', content)
                if match:
                    os.environ['LLAMAINDEX_API_KEY'] = match.group(1).strip()
        
        if not os.environ.get('OPENAI_API_KEY'):
            with open(env_path, 'r') as f:
                content = f.read()
                import re
                match = re.search(r'#?OPENAI_API_KEY=(.+)', content)
                if match:
                    os.environ['OPENAI_API_KEY'] = match.group(1).strip()

        self.openai_api_key = os.environ.get('OPENAI_API_KEY')
        self.llamaindex_api_key = os.environ.get('LLAMAINDEX_API_KEY')
        
        if not self.openai_api_key: raise ValueError("OPENAI_API_KEY ë¯¸ì„¤ì •")
        if not self.llamaindex_api_key: raise ValueError("LLAMAINDEX_API_KEY ë¯¸ì„¤ì •")

    def log(self, step: str, message: str, level: str = "INFO"):
        icons = {"INFO": "â„¹ï¸", "SUCCESS": "âœ…", "WARNING": "âš ï¸", "ERROR": "âŒ", "PROGRESS": "ğŸ”„", "DB": "ğŸ’¾"}
        icon = icons.get(level, "â„¹ï¸")
        elapsed = time.time() - self.start_time
        print(f"[{elapsed:7.1f}s] {icon} [{step}] {message}", flush=True)

    def normalize_path(self, path_str: str) -> str:
        """NFC ì •ê·œí™”"""
        return unicodedata.normalize('NFC', path_str)

    def find_target_paths(self) -> List[Path]:
        """íƒ€ê²Ÿ í´ë” ì´ë¦„ë“¤ë¡œ ì‹¤ì œ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸° (ì •ê·œí™” ëŒ€ì‘)"""
        if not self.target_folders:
            return [self.raw_data_root]
            
        target_paths = []
        normalized_targets = [self.normalize_path(t) for t in self.target_folders]
        
        # ì „ì²´ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ëŒë©° ì¼ì¹˜í•˜ëŠ” í´ë” ì°¾ê¸°
        for root, dirs, _ in os.walk(self.raw_data_root):
            root_path = Path(root)
            for d in dirs:
                if self.normalize_path(d) in normalized_targets:
                    target_paths.append(root_path / d)
        
        # ì§ì ‘ ê²½ë¡œë¡œë„ í™•ì¸
        for t in self.target_folders:
            p = self.raw_data_root / t
            if p.exists() and p not in target_paths:
                target_paths.append(p)
                
        return list(set(target_paths))

    def scan_files(self) -> List[Dict[str, Any]]:
        """ì§€ì •ëœ íƒ€ê²Ÿ í´ë” ë‚´ íŒŒì¼ ìŠ¤ìº”"""
        target_paths = self.find_target_paths()
        file_list = []
        
        if not target_paths:
            self.log("ERROR", f"íƒ€ê²Ÿ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.target_folders}", "ERROR")
            return []

        for target_path in target_paths:
            self.log("INFO", f"ìŠ¤ìº” ì¤‘: {target_path}", "INFO")
            for root, _, files in os.walk(target_path):
                try:
                    rel_path = Path(root).relative_to(self.raw_data_root)
                except ValueError:
                    rel_path = Path(root)
                    
                path_parts = rel_path.parts
                main_cat = path_parts[0] if path_parts else target_path.name
                sub_cat = "/".join(path_parts[1:]) if len(path_parts) > 1 else "root"
                
                for f in files:
                    if f.startswith('.') or f.startswith('~'): continue
                    file_path = Path(root) / f
                    if file_path.suffix.lower() in ['.pdf', '.xlsx', '.csv', '.png', '.jpg', '.jpeg']:
                        file_list.append({
                            'path': str(file_path),
                            'name': f,
                            'ext': file_path.suffix.lower(),
                            'category': f"{main_cat}/{sub_cat}",
                            'main_category': main_cat,
                            'sub_category': sub_cat
                        })
        
        # ì¤‘ë³µ ì œê±° (ê²½ë¡œ ê¸°ì¤€)
        seen = set()
        unique_files = []
        for f in file_list:
            if f['path'] not in seen:
                unique_files.append(f)
                seen.add(f['path'])
                
        return unique_files

    async def process_batch(self, files: List[Dict[str, Any]], index: VectorStoreIndex):
        """ë°°ì¹˜ ë‹¨ìœ„ íŒŒì‹± ë° ì¸ë±ìŠ¤ ì¶”ê°€"""
        documents = []
        for file_info in files:
            file_path = Path(file_info['path'])
            self.log("PARSE", f"ì²˜ë¦¬ ì¤‘: {file_info['name']}", "PROGRESS")
            
            try:
                if file_info['ext'] in ['.pdf', '.png', '.jpg', '.jpeg']:
                    llama_docs = await self.parser.aload_data(str(file_path))
                    for doc in llama_docs:
                        doc.metadata.update({
                            'file_name': file_info['name'],
                            'file_path': str(file_path),
                            'category': file_info['category'],
                            'main_category': file_info['main_category'],
                            'sub_category': file_info['sub_category'],
                            'doc_type': 'ai_parsed'
                        })
                        documents.append(doc)
                    self.log("PARSE", f"  âœ… {file_info['name']}: {len(llama_docs)}ê°œ ë¬¸ì„œ ì¶”ì¶œ", "SUCCESS")
                
                elif file_info['ext'] in ['.xlsx', '.csv']:
                    if file_info['ext'] == '.xlsx':
                        df_dict = pd.read_excel(file_path, sheet_name=None)
                        for sheet, df in df_dict.items():
                            doc = Document(
                                text=f"Sheet: {sheet}\n{df.to_markdown()}", 
                                metadata={
                                    'file_name': file_info['name'], 
                                    'file_path': str(file_path),
                                    'category': file_info['category'], 
                                    'main_category': file_info['main_category'],
                                    'sub_category': file_info['sub_category'],
                                    'doc_type': 'table',
                                    'sheet_name': sheet
                                }
                            )
                            documents.append(doc)
                    else:
                        df = None
                        for enc in ['utf-8', 'cp949', 'euc-kr']:
                            try:
                                df = pd.read_csv(file_path, encoding=enc)
                                break
                            except: continue
                        if df is not None:
                            documents.append(Document(
                                text=df.to_markdown(),
                                metadata={
                                    'file_name': file_info['name'], 
                                    'file_path': str(file_path),
                                    'category': file_info['category'], 
                                    'main_category': file_info['main_category'],
                                    'sub_category': file_info['sub_category'],
                                    'doc_type': 'table'
                                }
                            ))
                    self.log("PARSE", f"  âœ… {file_info['name']}: í‘œ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ", "SUCCESS")
            except Exception as e:
                self.log("ERROR", f"ì‹¤íŒ¨: {file_info['name']} - {str(e)}", "ERROR")

        if documents:
            self.log("INDEX", f"ì„ë² ë”© ë° ì¸ë±ìŠ¤ ì¶”ê°€ ì¤‘ ({len(documents)}ê°œ ë¬¸ì„œ)...", "PROGRESS")
            # Settings.node_parserë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
            node_parser = getattr(Settings, 'node_parser', SimpleNodeParser.from_defaults())
            nodes = node_parser.get_nodes_from_documents(documents)
            index.insert_nodes(nodes)
            gc.collect()

    async def run(self):
        all_files = self.scan_files()
        self.log("START", f"ì´ {len(all_files)}ê°œ íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ", "INFO")
        
        if not all_files:
            self.log("DONE", "ì¶”ê°€í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", "INFO")
            return

        # ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±
        try:
            self.log("DB", f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„: {self.db_dir}", "PROGRESS")
            if not (self.db_dir / "docstore.json").exists():
                raise FileNotFoundError("ê¸°ì¡´ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                
            vector_store = FaissVectorStore.from_persist_dir(str(self.db_dir))
            storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=str(self.db_dir))
            index = load_index_from_storage(storage_context)
            self.log("DB", f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ (ë…¸ë“œ ìˆ˜: {len(index.docstore.docs)})", "SUCCESS")
        except Exception as e:
            self.log("WARNING", f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨({e}), ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.", "WARNING")
            faiss_index = faiss.IndexFlatL2(EMBEDDING_DIMENSION)
            vector_store = FaissVectorStore(faiss_index=faiss_index)
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            index = VectorStoreIndex([], storage_context=storage_context)

        # ë°°ì¹˜ ì²˜ë¦¬ ë° ì €ì¥
        for i in range(0, len(all_files), self.batch_size):
            batch = all_files[i : i + self.batch_size]
            self.log("BATCH", f"ë°°ì¹˜ {i//self.batch_size + 1} ì‹œì‘ ({len(batch)}ê°œ íŒŒì¼)", "INFO")
            await self.process_batch(batch, index)
            
            # ì €ì¥ (AttributeError ë°©ì§€ ìœ„í•´ ë‚´ë¶€ êµ¬ì¡° ì ‘ê·¼ ê°œì„ )
            index.storage_context.persist(persist_dir=str(self.db_dir))
            
            # FAISS ì¸ë±ìŠ¤ ì¶”ì¶œ ë° ì €ì¥
            v_store = index.storage_context.vector_store
            f_idx = None
            if hasattr(v_store, '_faiss_index'):
                f_idx = v_store._faiss_index
            elif hasattr(v_store, 'faiss_index'):
                f_idx = v_store.faiss_index
            
            if f_idx:
                faiss.write_index(f_idx, str(self.db_dir / "faiss.index"))
                self.log("SAVE", f"ë°°ì¹˜ {i//self.batch_size + 1} ì €ì¥ ì™„ë£Œ", "SUCCESS")
            else:
                self.log("ERROR", "FAISS ì¸ë±ìŠ¤ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "ERROR")

        self.log("COMPLETE", f"ğŸ‰ ì‘ì—… ì™„ë£Œ! (ìµœì¢… ë…¸ë“œ ìˆ˜: {len(index.docstore.docs)})", "SUCCESS")


if __name__ == "__main__":
    import asyncio
    # íƒ€ê²Ÿ í´ë” ë¦¬ìŠ¤íŠ¸ ì„¤ì •
    target_list = ["ì§€ì¹¨ì„œ", "ì¶”ê°€", "ë°ì´í„°ì…‹"]
    manager = WelnoAIVectorDBManager(target_folders=target_list, batch_size=3)
    asyncio.run(manager.run())
