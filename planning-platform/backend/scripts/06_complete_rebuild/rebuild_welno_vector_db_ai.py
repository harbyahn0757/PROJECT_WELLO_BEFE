"""
ì›°ë¡œ ë²¡í„° DB AI ëª¨ë¸ ê¸°ë°˜ í†µí•© ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (LlamaParse ë²„ì „)
- LlamaParse: AI ê¸°ë°˜ ë¬¸ì„œ íŒŒì‹± (í‘œ, ì´ë¯¸ì§€, ë ˆì´ì•„ì›ƒ ì™„ë²½ ì§€ì›)
- OpenAI Embedding: text-embedding-ada-002
- ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì› (ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ í›„ ì¶”ê°€)
- íŠ¹ì • í´ë” íƒ€ê²ŸíŒ… ì§€ì› (NFD/NFC ì •ê·œí™” ëŒ€ì‘)
"""

import os
import sys
import time
import gc
import json
import unicodedata
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# LlamaIndex & LlamaParse
from llama_index.core import Document, VectorStoreIndex, StorageContext, Settings, load_index_from_storage
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_parse import LlamaParse

# FAISS
import faiss
from llama_index.vector_stores.faiss import FaissVectorStore

# Excel/CSV
import pandas as pd


class WelnoAIVectorDBManager:
    """AI ëª¨ë¸ ê¸°ë°˜ ë²¡í„° DB í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self, target_folder_name: Optional[str] = None, batch_size: int = 3):
        # ê²½ë¡œ ì„¤ì • (ê¸°ë³¸ê°’: ì „ì²´ raw_data)
        self.raw_data_root = Path("/data/raw_data")
        self.target_folder_name = target_folder_name
        self.db_dir = Path("/data/vector_db/welno/faiss_db_v3")
        self.batch_size = batch_size
        self.start_time = time.time()
        
        self.db_dir.mkdir(parents=True, exist_ok=True)
        
        # API í‚¤ ë° ì„¤ì •
        self._load_env()
        
        # LlamaParse ì´ˆê¸°í™”
        self.parser = LlamaParse(
            api_key=self.llamaindex_api_key,
            result_type="markdown",
            language="ko",
            num_workers=4,
            verbose=True
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        self.embed_model = OpenAIEmbedding(
            model="text-embedding-ada-002",
            api_key=self.openai_api_key
        )
        Settings.embed_model = self.embed_model
        
        print("\n" + "="*80)
        print("ğŸš€ ì›°ë¡œ AI ë²¡í„° DB í†µí•© ê´€ë¦¬ ì‹œìŠ¤í…œ")
        print("="*80)
        print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“ íƒ€ê²Ÿ í´ë” ëª…: {self.target_folder_name or 'ì „ì²´'}")
        print(f"ğŸ’¾ DB ê²½ë¡œ: {self.db_dir}")
        print("="*80 + "\n")

    def _load_env(self):
        """í™˜ê²½ë³€ìˆ˜ ë¡œë“œ"""
        env_path = Path("/home/workspace/PROJECT_WELLO_BEFE/planning-platform/backend/config.env")
        if env_path.exists():
            with open(env_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'): continue
                    if '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key] = value
        
        # ì£¼ì„ ì²˜ë¦¬ëœ í‚¤ ê°•ì œ ë¡œë“œ ì‹œë„
        if not os.environ.get('LLAMAINDEX_API_KEY'):
            with open(env_path, 'r') as f:
                content = f.read()
                import re
                match = re.search(r'#?LLAMAINDEX_API_KEY=(.+)', content)
                if match:
                    os.environ['LLAMAINDEX_API_KEY'] = match.group(1).strip()

        self.openai_api_key = os.environ.get('OPENAI_API_KEY')
        self.llamaindex_api_key = os.environ.get('LLAMAINDEX_API_KEY')
        
        if not self.openai_api_key: raise ValueError("OPENAI_API_KEY ë¯¸ì„¤ì •")
        if not self.llamaindex_api_key: raise ValueError("LLAMAINDEX_API_KEY ë¯¸ì„¤ì •")

    def log(self, step: str, message: str, level: str = "INFO"):
        icons = {"INFO": "â„¹ï¸", "SUCCESS": "âœ…", "WARNING": "âš ï¸", "ERROR": "âŒ", "PROGRESS": "ğŸ”„", "DB": "ğŸ’¾"}
        icon = icons.get(level, "â„¹ï¸")
        elapsed = time.time() - self.start_time
        print(f"[{elapsed:7.1f}s] {icon} [{step}] {message}", flush=True)

    def normalize_path(self, path_str: str) -> str:
        """NFC ì •ê·œí™”"""
        return unicodedata.normalize('NFC', path_str)

    def find_target_path(self) -> Path:
        """íƒ€ê²Ÿ í´ë” ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ ê²½ë¡œ ì°¾ê¸° (ì •ê·œí™” ëŒ€ì‘)"""
        if not self.target_folder_name:
            return self.raw_data_root
            
        target_norm = self.normalize_path(self.target_folder_name)
        
        for entry in self.raw_data_root.iterdir():
            if entry.is_dir():
                entry_norm = self.normalize_path(entry.name)
                if entry_norm == target_norm:
                    return entry
        
        return self.raw_data_root / self.target_folder_name # Fallback

    def scan_files(self) -> List[Dict[str, Any]]:
        """ì§€ì •ëœ íƒ€ê²Ÿ í´ë” ë‚´ íŒŒì¼ ìŠ¤ìº”"""
        target_path = self.find_target_path()
        file_list = []
        
        if not target_path.exists():
            self.log("ERROR", f"íƒ€ê²Ÿ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {target_path}", "ERROR")
            return []

        for root, _, files in os.walk(target_path):
            rel_path = Path(root).relative_to(self.raw_data_root)
            path_parts = rel_path.parts
            
            main_cat = path_parts[0] if path_parts else target_path.name
            sub_cat = "/".join(path_parts[1:]) if len(path_parts) > 1 else "root"
            
            for f in files:
                if f.startswith('.') or f.startswith('~'): continue
                file_path = Path(root) / f
                if file_path.suffix.lower() in ['.pdf', '.xlsx', '.csv', '.png', '.jpg', '.jpeg']:
                    file_list.append({
                        'path': str(file_path),
                        'name': f,
                        'ext': file_path.suffix.lower(),
                        'category': f"{main_cat}/{sub_cat}",
                        'main_category': main_cat,
                        'sub_category': sub_cat
                    })
        return file_list

    async def process_batch(self, files: List[Dict[str, Any]], index: VectorStoreIndex):
        """ë°°ì¹˜ ë‹¨ìœ„ íŒŒì‹± ë° ì¸ë±ìŠ¤ ì¶”ê°€"""
        documents = []
        for file_info in files:
            file_path = Path(file_info['path'])
            self.log("PARSE", f"ì²˜ë¦¬ ì¤‘: {file_info['name']}", "PROGRESS")
            
            try:
                if file_info['ext'] in ['.pdf', '.png', '.jpg', '.jpeg']:
                    llama_docs = await self.parser.aload_data(str(file_path))
                    for doc in llama_docs:
                        doc.metadata.update({
                            'file_name': file_info['name'],
                            'file_path': str(file_path),
                            'category': file_info['category'],
                            'doc_type': 'ai_parsed'
                        })
                        documents.append(doc)
                    self.log("PARSE", f"  âœ… {file_info['name']}: {len(llama_docs)}ê°œ ë¬¸ì„œ ì¶”ì¶œ", "SUCCESS")
                
                elif file_info['ext'] in ['.xlsx', '.csv']:
                    if file_info['ext'] == '.xlsx':
                        df_dict = pd.read_excel(file_path, sheet_name=None)
                        for sheet, df in df_dict.items():
                            doc = Document(
                                text=f"Sheet: {sheet}\n{df.to_markdown()}", 
                                metadata={'file_name': file_info['name'], 'category': file_info['category'], 'doc_type': 'table'}
                            )
                            documents.append(doc)
                    else:
                        df = None
                        for enc in ['utf-8', 'cp949', 'euc-kr']:
                            try:
                                df = pd.read_csv(file_path, encoding=enc)
                                break
                            except: continue
                        if df is not None:
                            documents.append(Document(
                                text=df.to_markdown(),
                                metadata={'file_name': file_info['name'], 'category': file_info['category'], 'doc_type': 'table'}
                            ))
                    self.log("PARSE", f"  âœ… {file_info['name']}: í‘œ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ", "SUCCESS")
            except Exception as e:
                self.log("ERROR", f"ì‹¤íŒ¨: {file_info['name']} - {str(e)}", "ERROR")

        if documents:
            self.log("INDEX", f"ì„ë² ë”© ë° ì¸ë±ìŠ¤ ì¶”ê°€ ì¤‘ ({len(documents)}ê°œ ë¬¸ì„œ)...", "PROGRESS")
            nodes = Settings.node_parser.get_nodes_from_documents(documents)
            index.insert_nodes(nodes)
            gc.collect()

    async def run(self):
        all_files = self.scan_files()
        self.log("START", f"ì´ {len(all_files)}ê°œ íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ", "INFO")
        
        if not all_files:
            self.log("DONE", "ì¶”ê°€í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", "INFO")
            return

        # ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±
        try:
            self.log("DB", f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„: {self.db_dir}", "PROGRESS")
            vector_store = FaissVectorStore.from_persist_dir(str(self.db_dir))
            storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=str(self.db_dir))
            index = load_index_from_storage(storage_context)
            self.log("DB", f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ (ë…¸ë“œ ìˆ˜: {len(index.docstore.docs)})", "SUCCESS")
        except Exception as e:
            self.log("WARNING", f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨({e}), ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.", "WARNING")
            faiss_index = faiss.IndexFlatL2(1536)
            vector_store = FaissVectorStore(faiss_index=faiss_index)
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            index = VectorStoreIndex([], storage_context=storage_context)

        # ë°°ì¹˜ ì²˜ë¦¬ ë° ì €ì¥
        for i in range(0, len(all_files), self.batch_size):
            batch = all_files[i : i + self.batch_size]
            self.log("BATCH", f"ë°°ì¹˜ {i//self.batch_size + 1} ì‹œì‘", "INFO")
            await self.process_batch(batch, index)
            
            # ì €ì¥ (AttributeError ë°©ì§€ ìœ„í•´ ë‚´ë¶€ êµ¬ì¡° ì ‘ê·¼ ê°œì„ )
            index.storage_context.persist(persist_dir=str(self.db_dir))
            
            # FAISS ì¸ë±ìŠ¤ ì¶”ì¶œ ë° ì €ì¥
            f_idx = None
            if hasattr(vector_store, '_faiss_index'):
                f_idx = vector_store._faiss_index
            elif hasattr(vector_store, 'faiss_index'):
                f_idx = vector_store.faiss_index
            
            if f_idx:
                faiss.write_index(f_idx, str(self.db_dir / "faiss.index"))
                self.log("SAVE", f"ë°°ì¹˜ {i//self.batch_size + 1} ì €ì¥ ì™„ë£Œ", "SUCCESS")
            else:
                self.log("ERROR", "FAISS ì¸ë±ìŠ¤ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "ERROR")

        self.log("COMPLETE", f"ğŸ‰ ì‘ì—… ì™„ë£Œ! (ìµœì¢… ë…¸ë“œ ìˆ˜: {len(index.docstore.docs)})", "SUCCESS")


if __name__ == "__main__":
    import asyncio
    # 'ì§€ì¹¨ì„œ' í´ë” íƒ€ê²ŸíŒ… (ì •ê·œí™” ëŒ€ì‘)
    target_name = "ì§€ì¹¨ì„œ"
    manager = WelnoAIVectorDBManager(target_folder_name=target_name, batch_size=3)
    asyncio.run(manager.run())
