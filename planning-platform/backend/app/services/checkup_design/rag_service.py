"""
RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤ ëª¨ë“ˆ
LlamaIndex ë° Gemini LLM ê¸°ë°˜ ì˜ë£Œ ì§€ì‹ ê²€ìƒ‰ ì—”ì§„

**ë¡œì»¬ FAISS ë²¡í„° DB ì§€ì› ì¶”ê°€**
- ë¡œì»¬ FAISS ë²¡í„° DB (ë¹„ìš© ì ˆê°, ë¹ ë¥¸ ì‘ë‹µ)
"""
import os
import json
import re
from typing import List, Dict, Any, Optional
from ...core.config import settings

# LlamaIndex RAG ê´€ë ¨ ì„í¬íŠ¸
try:
    from llama_index.core import Settings, VectorStoreIndex, StorageContext, load_index_from_storage
    from llama_index.core.llms import CustomLLM
    from llama_index.core.llms.llm import LLM
    from llama_index.core.llms import ChatMessage, MessageRole, CompletionResponse, LLMMetadata
    from llama_index.llms.openai import OpenAI
    from llama_index.embeddings.openai import OpenAIEmbedding
    # FAISS ë²¡í„° ìŠ¤í† ì–´ ì„í¬íŠ¸
    try:
        import faiss
        import pickle
        from pathlib import Path as PathLib
        from llama_index.vector_stores.faiss import FaissVectorStore
        FAISS_AVAILABLE = True
    except ImportError:
        FAISS_AVAILABLE = False
    # Google GeminiëŠ” google-generativeai ì§ì ‘ ì‚¬ìš©í•˜ì—¬ CustomLLMìœ¼ë¡œ ë˜í•‘
    try:
        import google.generativeai as genai
        GEMINI_AVAILABLE = True
    except ImportError:
        GEMINI_AVAILABLE = False
        genai = None
    LLAMAINDEX_AVAILABLE = True
except ImportError as e:
    LLAMAINDEX_AVAILABLE = False
    GEMINI_AVAILABLE = False
    FAISS_AVAILABLE = False
    genai = None
    # ê°œë°œ í™˜ê²½ì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë”ë¯¸ í´ë˜ìŠ¤
    class OpenAI:
        pass
    class CustomLLM:
        pass
    class ChatMessage:
        pass
    class MessageRole:
        pass
    class CompletionResponse:
        pass
    class LLMMetadata:
        pass

# ìƒìˆ˜ ì •ì˜

# ë¡œì»¬ FAISS ë²¡í„° DB ê²½ë¡œ
LOCAL_FAISS_DIR = "/data/vector_db/welno/faiss_db"
LOCAL_FAISS_INDEX_PATH = f"{LOCAL_FAISS_DIR}/faiss.index"
LOCAL_FAISS_METADATA_PATH = f"{LOCAL_FAISS_DIR}/metadata.pkl"

# ì„ë² ë”© ëª¨ë¸Â·ì°¨ì› (ë³‘ì›ë³„ í™•ì¥ ì‹œ ë™ì¼ ê°’ ìœ ì§€ë¡œ í˜¸í™˜)
EMBEDDING_MODEL = "text-embedding-ada-002"
EMBEDDING_DIMENSION = 1536

# ë³‘ì›ë³„ FAISS ë£¨íŠ¸ (ì „ì—­ê³¼ ë¶„ë¦¬, ë³‘ì› RAG ìš°ì„ ìš©)
LOCAL_FAISS_BY_HOSPITAL = os.environ.get("LOCAL_FAISS_BY_HOSPITAL", "/data/vector_db/welno/faiss_db_by_hospital")

# ë¡œì»¬ FAISS ì—”ì§„ ìºì‹œ
_rag_engine_local_cache: Optional[Any] = None

# Gemini CustomLLM í´ë˜ìŠ¤
class GeminiLLM(CustomLLM):
    """Google Geminië¥¼ LlamaIndex CustomLLMìœ¼ë¡œ êµ¬í˜„ (RAG ê²€ìƒ‰ìš©)"""
    
    def __init__(self, api_key: str, model: str = "gemini-3-flash-preview", **kwargs):
        if not GEMINI_AVAILABLE or not genai:
            raise ImportError("google-generativeaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        super().__init__(**kwargs)
        genai.configure(api_key=api_key)
        self._model = genai.GenerativeModel(model)
        self._model_name = model
    
    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=8192,
            num_output=2048,
            is_chat_model=True,
            model_name=self._model_name
        )
    
    def complete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:
        try:
            response = self._model.generate_content(prompt)
            text = response.text if hasattr(response, 'text') else str(response)
            return CompletionResponse(text=text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def acomplete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:
        try:
            response = self._model.generate_content(prompt)
            text = response.text if hasattr(response, 'text') else str(response)
            return CompletionResponse(text=text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    def stream_complete(self, prompt: str, formatted: bool = False, **kwargs):
        try:
            response = self._model.generate_content(prompt, stream=True)
            for chunk in response:
                if hasattr(chunk, 'text') and chunk.text:
                    yield CompletionResponse(text=chunk.text, delta=chunk.text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
from llama_index.core import PromptTemplate

CHAT_SYSTEM_PROMPT_TEMPLATE = (
    "ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ì¹œì ˆí•œ {persona_name}ì…ë‹ˆë‹¤.\n"
    "\n"
    "âš ï¸ **í•µì‹¬ ê·œì¹™**:\n"
    "1. [Context]ì— ì—†ëŠ” ì •ë³´ëŠ” ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”\n"
    "2. ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì¦ìƒë§Œ ë‹¤ë£¨ì„¸ìš” (ì¶”ì¸¡ ê¸ˆì§€)\n"
    "3. í™˜ì ë°ì´í„° ì—†ìœ¼ë©´ \"í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€\n"
    "4. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª…í™•íˆ í‘œí˜„\n"
    "\n"
    "ğŸ’¡ **ë‹µë³€ ìŠ¤íƒ€ì¼**:\n"
    "- [Context]ì— í™˜ì ê±´ê°• ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê°„ë‹¨íˆ ì–¸ê¸‰í•˜ë©° ì‹œì‘\n"
    "- 'ì§€ì¹¨ì„œì— ë”°ë¥´ë©´' ê°™ì€ ë¶€ì—° ì„¤ëª… ìƒëµ, ì „ë¬¸ê°€ë‹µê²Œ ë‹¨í˜¸í•˜ê²Œ ë‹µë³€\n"
    "- í•µì‹¬ ìœ„ì£¼ë¡œ ìš”ì•½, ì‹¤ì²œ ë°©ë²•ì€ ë¶ˆë › í¬ì¸íŠ¸\n"
    "- ê·¼ê±° ë¬¸ì„œ ì–¸ê¸‰ ë¶ˆí•„ìš” (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í‘œì‹œ)\n"
    "\n"
    "ğŸ”— **ë‹µë³€ êµ¬ì¡°** (2ë‹¨ê³„):\n"
    "1ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€ (ë¨¼ì €)\n"
    "2ë‹¨ê³„: ì˜í•™ ë¬¸ì„œì— ì—°ê´€ì„± ëª…ì‹œ ì‹œì—ë§Œ ê³¼ê±° ë°ì´í„° ì–¸ê¸‰ (ë‚˜ì¤‘ì—)\n"
    "   - ì¶œì²˜ ëª…ì‹œ: \"2021ë…„ ê²€ì§„\", \"ë³µì•½ ë‚´ì—­\", \"ì´ì „ ë¬¸ì§„\"\n"
    "   - ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜ì–´ ì‚¬ìš©: \"ì°¸ê³ ë¡œ\", \"ê·¸ëŸ°ë°\", \"ë‹¤ë§Œ\"\n"
    "   ì˜ˆ: \"ì²´ì¤‘ ê´€ë¦¬ëŠ”... (ë‹µë³€) ì°¸ê³ ë¡œ, 2021ë…„ ê²€ì§„ì—ì„œ ì²´ì¤‘ ì¦ê°€ ì‹œ í˜ˆì••ë„ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤.\"\n"
    "\n"
    "ğŸ“‹ **ë°ì´í„° í™•ì¸**:\n"
    "- ì‚¬ìš©ì ë§ê³¼ ê¸°ì¡´ ë°ì´í„°(ê²€ì§„/ë³µì•½/ë¬¸ì§„)ê°€ ë‹¤ë¥¼ ë•Œ í™•ì¸ ì§ˆë¬¸\n"
    "  ì˜ˆ: \"2021ë…„ ê²€ì§„ì—ì„œ ì²´ì¤‘ ê°ì†Œ ì¶”ì„¸ì˜€ëŠ”ë°, ìµœê·¼ ìƒíƒœëŠ” ì–´ë–¤ê°€ìš”?\"\n"
    "- ì‹œìŠ¤í…œì´ ì§ì ‘ ë¬»ëŠ” ë°©ì‹ìœ¼ë¡œ í‘œí˜„\n"
    "\n"
    "ğŸ‘¨â€âš•ï¸ **ìƒë‹´ ìœ ë„**:\n"
    "- ë³µì¡í•œ ì¦ìƒì€ ìƒë‹´ì‚¬ ì—°ê²° ìœ ë„\n"
    "- ì˜ì–‘ì œ/ê±´ê¸°ì‹ ì§ˆë¬¸ ì‹œ PNT ë¬¸ì§„ ì œì•ˆ\n"
    "\n"
    "[Context]\n"
    "{context_str}\n"
    "\n"
    "ì‚¬ìš©ì ì§ˆë¬¸: {query_str}\n"
    "ì „ë¬¸ê°€ ë‹µë³€:"
)

async def init_rag_engine(use_local_vector_db: bool = True):
    """
    RAG Query Engine ì´ˆê¸°í™”
    
    Args:
        use_local_vector_db: Trueë©´ ë¡œì»¬ FAISS ë²¡í„° DB ì‚¬ìš© (ë¹„ìš© ì ˆê°, ë¹ ë¥¸ ì‘ë‹µ, ê¸°ë³¸ê°’)
                            Falseë©´ ë¡œì»¬ FAISS API ì‚¬ìš©
    """
    global _rag_engine_local_cache
    
    # ìºì‹œ í™•ì¸
    if use_local_vector_db:
        if _rag_engine_local_cache is not None:
            print("[INFO] ë¡œì»¬ FAISS ì—”ì§„ ìºì‹œ ì‚¬ìš©")
            return _rag_engine_local_cache
    if not LLAMAINDEX_AVAILABLE:
        print("[WARN] LlamaIndex ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # ===== ë¡œì»¬ FAISS ë²¡í„° DB ì‚¬ìš© =====
        if use_local_vector_db:
            if not FAISS_AVAILABLE:
                print("[WARN] ë¡œì»¬ FAISSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            else:
                print("[INFO] ë¡œì»¬ FAISS ë²¡í„° DB ì´ˆê¸°í™” ì¤‘...")
                
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                if not PathLib(LOCAL_FAISS_INDEX_PATH).exists():
                    print(f"[WARN] FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LOCAL_FAISS_INDEX_PATH}")
                    print("[WARN] ë¡œì»¬ FAISSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                else:
                    faiss_index = faiss.read_index(LOCAL_FAISS_INDEX_PATH)
                    print(f"[INFO] FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {faiss_index.ntotal}ê°œ ë²¡í„°")
                    
                    # ë©”íƒ€ë°ì´í„° ë¡œë“œ (ì„ íƒ ì‚¬í•­ìœ¼ë¡œ ë³€ê²½)
                    if PathLib(LOCAL_FAISS_METADATA_PATH).exists():
                        with open(LOCAL_FAISS_METADATA_PATH, 'rb') as f:
                            metadata = pickle.load(f)
                        total_docs = metadata.get('total_documents', metadata.get('total_nodes', faiss_index.ntotal))
                        print(f"[INFO] ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {total_docs}ê°œ ë¬¸ì„œ")
                    else:
                        print(f"[INFO] ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        total_docs = faiss_index.ntotal
                    
                    # OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ê²€ìƒ‰ìš©)
                        openai_api_key = os.environ.get("OPENAI_API_KEY") or settings.openai_api_key
                        if not openai_api_key or openai_api_key == "dev-openai-key":
                            print("[WARN] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            print("[WARN] ë¡œì»¬ FAISSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            return None
                        else:
                            embed_model = OpenAIEmbedding(
                                model=EMBEDDING_MODEL,
                                api_key=openai_api_key
                            )
                            Settings.embed_model = embed_model
                            
                            # Gemini LLM ì„¤ì • (ë‹µë³€ ìƒì„±ìš©)
                            gemini_api_key = os.environ.get("GOOGLE_GEMINI_API_KEY") or settings.google_gemini_api_key
                            if not gemini_api_key or gemini_api_key == "dev-gemini-key":
                                print("[WARN] Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                print("[WARN] ë¡œì»¬ FAISSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                return None
                            elif not GEMINI_AVAILABLE or not genai:
                                print("[WARN] Google Geminiê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                                print("[WARN] ë¡œì»¬ FAISSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                return None
                            else:
                                llm = GeminiLLM(api_key=gemini_api_key, model="gemini-3-flash-preview")
                                Settings.llm = llm
                                
                                # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                                vector_store = FaissVectorStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                
                                # Storage Context ìƒì„± (docstoreì™€ index_storeë§Œ ë¡œë“œ)
                                print(f"[INFO] Storage Context ë¡œë“œ ì¤‘...")
                                from llama_index.core.storage.docstore import SimpleDocumentStore
                                from llama_index.core.storage.index_store import SimpleIndexStore
                                
                                docstore = SimpleDocumentStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                index_store = SimpleIndexStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                
                                storage_context = StorageContext.from_defaults(
                                    vector_store=vector_store,
                                    docstore=docstore,
                                    index_store=index_store
                                )
                                
                                # ì¸ë±ìŠ¤ ë¡œë“œ (index_id ëª…ì‹œ)
                                try:
                                    # ë¨¼ì € index_id ì—†ì´ ì‹œë„
                                    index = load_index_from_storage(storage_context)
                                except ValueError as e:
                                    if "Please specify index_id" in str(e):
                                        # index_storeì—ì„œ index_id ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                                        from llama_index.core.storage.index_store import SimpleIndexStore
                                        index_structs = storage_context.index_store.index_structs()
                                        
                                        if isinstance(index_structs, dict):
                                            index_ids = list(index_structs.keys())
                                        elif isinstance(index_structs, list):
                                            # listì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª©ì˜ index_id ì‚¬ìš©
                                            index_ids = [struct.index_id for struct in index_structs if hasattr(struct, 'index_id')]
                                        else:
                                            raise ValueError(f"Unexpected index_structs type: {type(index_structs)}")
                                        
                                        if index_ids:
                                            print(f"[INFO] ì—¬ëŸ¬ ì¸ë±ìŠ¤ ë°œê²¬, ì²« ë²ˆì§¸ ì‚¬ìš©: {index_ids[0]}")
                                            index = load_index_from_storage(storage_context, index_id=index_ids[0])
                                        else:
                                            raise ValueError("No index found in storage")
                                    else:
                                        raise
                                
                                # ì¿¼ë¦¬ ì—”ì§„ ìƒì„± (ì„±ëŠ¥ ìµœì í™” ë° ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì ìš©)
                                # íŒŒíŠ¸ë„ˆ ì •ë³´ë¥¼ ìœ„í•´ persona_nameì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš´ í…œí”Œë¦¿ ì‚¬ìš©
                                # (ë‚˜ì¤‘ì— query ì‹œì ì— ë‹¤ì‹œ ë°”ê¿€ ìˆ˜ë„ ìˆì§€ë§Œ ì´ˆê¸°í™” ì‹œì ì— ê¸°ë³¸ ì„¤ì •)
                                default_persona = "ê±´ê°• ìƒë‹´ê°€ AI"
                                system_prompt = CHAT_SYSTEM_PROMPT_TEMPLATE.format(
                                    persona_name=default_persona,
                                    context_str="{context_str}",
                                    query_str="{query_str}"
                                )
                                
                                query_engine = index.as_query_engine(
                                    similarity_top_k=5,
                                    response_mode="compact",
                                    text_qa_template=PromptTemplate(system_prompt)
                                )
                                
                                _rag_engine_local_cache = query_engine
                                print(f"[INFO] âœ… ë¡œì»¬ FAISS RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (ë²¡í„°: {faiss_index.ntotal}ê°œ, ë¬¸ì„œ: {total_docs}ê°œ)")
                                
                                return query_engine
        
    except Exception as e:
        print(f"[ERROR] RAG ì—”ì§„ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def clean_html_content(text: str) -> str:
    """
    HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    return text

def extract_evidence_from_source_nodes(response) -> List[Dict[str, Any]]:
    """LlamaIndex ì‘ë‹µì—ì„œ ì†ŒìŠ¤ ë…¸ë“œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    evidences = []
    if hasattr(response, 'source_nodes'):
        for node in response.source_nodes:
            metadata = node.metadata if hasattr(node, 'metadata') else {}
            text = node.text if hasattr(node, 'text') else ""
            
            # HTML íƒœê·¸ ì •ì œ
            text = clean_html_content(text)
            
            score = node.score if hasattr(node, 'score') else 0.0
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            file_name = metadata.get('file_name', 'Unknown')
            page_label = metadata.get('page_label', 'Unknown')
            
            # ì¸ìš©êµ¬ ì¶”ì¶œ
            citation = text[:100] + "..." if len(text) > 100 else text
            
            evidences.append({
                "source_document": file_name,
                "page": page_label,
                "citation": citation,
                "full_text": text,
                "confidence_score": score,
                "organization": "ì˜í•™íšŒ",
                "year": "2024"
            })
    return evidences

async def get_medical_evidence_from_rag(query_engine, patient_context: Dict, concerns: List[Dict]) -> Dict[str, Any]:
    """
    í™˜ì ì •ë³´ì™€ ì—¼ë ¤ ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ RAG ê²€ìƒ‰ ìˆ˜í–‰
    """
    if not query_engine:
        return {"context_text": "", "structured_evidences": []}
    
    try:
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        query_parts = []
        
        # 1. ë‚˜ì´/ì„±ë³„ ê¸°ë°˜
        age = patient_context.get('age', 40)
        gender = patient_context.get('gender', 'unknown')
        query_parts.append(f"{age}ì„¸ {gender}ì—ê²Œ ê¶Œì¥ë˜ëŠ” í•„ìˆ˜ ê±´ê°•ê²€ì§„ í•­ëª©ê³¼ ì•” ì„ ë³„ê²€ì‚¬ ê¸°ì¤€")
        
        # 2. ì´ìƒ ì†Œê²¬ ê¸°ë°˜
        abnormal_items = patient_context.get('abnormal_items', [])
        for item in abnormal_items:
            name = item.get('name', '')
            status = item.get('status', '')
            if name:
                query_parts.append(f"{name} ìˆ˜ì¹˜ê°€ {status}ì¼ ë•Œ í•„ìš”í•œ ì •ë°€ ê²€ì‚¬ì™€ ì„ìƒì  ì˜ì˜")
        
        # 3. ê°€ì¡±ë ¥ ê¸°ë°˜
        family_history = patient_context.get('family_history', [])
        for fh in family_history:
            query_parts.append(f"{fh} ê°€ì¡±ë ¥ì´ ìˆì„ ë•Œ ê¶Œì¥ë˜ëŠ” ì¡°ê¸° ì„ ë³„ê²€ì‚¬")
            
        # 4. ì—¼ë ¤ í•­ëª© ê¸°ë°˜
        for concern in concerns:
            c_name = concern.get('name', '')
            if c_name:
                query_parts.append(f"{c_name} ê´€ë ¨ ìµœì‹  ì§„ë£Œì§€ì¹¨ê³¼ ê²€ì‚¬ ê¶Œê³ ì•ˆ")
        
        final_query = " \n".join(query_parts)
        print(f"[INFO] RAG ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±: {len(final_query)}ì")
        
        # ê²€ìƒ‰ ì‹¤í–‰ - aretrieve() ì‚¬ìš© (ë²¡í„° ê²€ìƒ‰ë§Œ, LLM ì‘ë‹µ ìƒì„± ì œê±°)
        nodes = await query_engine.aretrieve(final_query)
        
        # ê²°ê³¼ ì²˜ë¦¬ - source_nodesì—ì„œ ì§ì ‘ ì¶”ì¶œ
        structured_evidences = []
        for node in nodes:
            metadata = node.metadata if hasattr(node, 'metadata') else {}
            text = node.text if hasattr(node, 'text') else ""
            
            # HTML íƒœê·¸ ì •ì œ
            text = clean_html_content(text)
            
            score = node.score if hasattr(node, 'score') else 0.0
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            file_name = metadata.get('file_name', 'Unknown')
            page_label = metadata.get('page_label', 'Unknown')
            
            # ì¸ìš©êµ¬ ì¶”ì¶œ
            citation = text[:100] + "..." if len(text) > 100 else text
            
            structured_evidences.append({
                "source_document": file_name,
                "page": page_label,
                "citation": citation,
                "full_text": text,
                "confidence_score": score,
                "organization": "ì˜í•™íšŒ",
                "year": "2024"
            })
        
        # ê° ì—ë¹„ë˜ìŠ¤ì— ì¿¼ë¦¬ ë§¥ë½ ì¶”ê°€
        for ev in structured_evidences:
            ev['query'] = final_query
            ev['category'] = 'Clinical Guideline'
        
        # structured_evidencesë¥¼ context_textë¡œ í¬ë§·íŒ… (LLM ì‘ë‹µ ëŒ€ì‹  ì›ë³¸ ë¬¸ì„œ ì‚¬ìš©)
        # ê°„ë‹¨í•œ í¬ë§·íŒ… (ìˆœí™˜ import ë°©ì§€)
        context_text = ""
        if structured_evidences:
            formatted_parts = []
            for idx, ev in enumerate(structured_evidences[:5], 1):  # ìƒìœ„ 5ê°œë§Œ
                doc_name = ev.get('source_document', 'ë¬¸ì„œëª… ì—†ìŒ')
                citation = ev.get('citation', '')
                if citation:
                    formatted_parts.append(f"{idx}. [{doc_name}]\n\"{citation}\"\n")
            context_text = "\n".join(formatted_parts)
        
        return {
            "context_text": context_text,
            "structured_evidences": structured_evidences
        }
        
    except Exception as e:
        print(f"[ERROR] RAG ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        return {"context_text": "", "structured_evidences": []}

async def search_checkup_knowledge(query: str, use_local_vector_db: bool = True) -> Dict[str, Any]:
    """
    ê²€ì§„ ì§€ì‹ ê²€ìƒ‰ (RAG)
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        use_local_vector_db: Trueë©´ ë¡œì»¬ FAISS ì‚¬ìš©, ë¡œì»¬ FAISS ë²¡í„° DB ì‚¬ìš© ì—¬ë¶€
    """
    query_engine = await init_rag_engine(use_local_vector_db=use_local_vector_db)
    
    if not query_engine:
        return {
            "success": False,
            "error": "RAG ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨",
            "answer": None,
            "sources": []
        }
    
    try:
        # ê²€ìƒ‰ë§Œ ìˆ˜í–‰ (LLM ì‘ë‹µ í•©ì„± ì—†ì´ ì†ŒìŠ¤ë§Œ ë°˜í™˜ â†’ ì†ë„ ëŒ€í­ ê°œì„ )
        nodes = await query_engine.aretrieve(query)
        
        sources = []
        for node in nodes:
            source_info = {
                "text": clean_html_content(node.text)[:500] if hasattr(node, 'text') else "",
                "score": float(node.score) if hasattr(node, 'score') else None,
                "metadata": node.metadata if hasattr(node, 'metadata') else {}
            }
            sources.append(source_info)
        
        return {
            "success": True,
            "answer": None,
            "sources": sources,
            "query": query
        }
    
    except Exception as e:
        print(f"[ERROR] RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "answer": None,
            "sources": []
        }

async def search_hospital_knowledge(hospital_id: str, query: str, partner_id: str = "welno") -> Dict[str, Any]:
    """
    ë³‘ì›ë³„ RAG ê²€ìƒ‰ â€” ë³‘ì› ì „ìš© ì¸ë±ìŠ¤ + ê¸€ë¡œë²Œ ì¸ë±ìŠ¤ ì–‘ìª½ ê²€ìƒ‰.
    ë³‘ì› ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©í•˜ê³ , ê¸€ë¡œë²Œ ì¸ë±ìŠ¤ëŠ” í•­ìƒ ê²€ìƒ‰.
    ê²°ê³¼ë¥¼ score ê¸°ë°˜ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ë°˜í™˜.
    """
    if not query:
        return {"success": False, "sources": []}

    results = []

    # 1. ë³‘ì›ë³„ ì¸ë±ìŠ¤ ê²€ìƒ‰ (ìˆìœ¼ë©´)
    if hospital_id:
        hospital_results = await _search_hospital_faiss(hospital_id, query)
        if hospital_results:
            results.extend(hospital_results)

    # 2. ê¸€ë¡œë²Œ ì¸ë±ìŠ¤ ê²€ìƒ‰ (í•­ìƒ)
    global_results = await search_checkup_knowledge(query)
    if global_results.get("success") and global_results.get("sources"):
        for src in global_results["sources"]:
            results.append({
                "text": src.get("text", ""),
                "score": src.get("score", 0.0),
                "metadata": src.get("metadata", {}),
                "source_type": "global"
            })

    if not results:
        return {"success": False, "sources": []}

    # 3. ì ìˆ˜ ê¸°ë°˜ ì •ë ¬ + ìƒìœ„ 10ê°œ ë°˜í™˜
    results.sort(key=lambda x: x.get("score") or 0, reverse=True)
    top_results = results[:10]

    return {
        "success": True,
        "answer": None,  # answerëŠ” í˜¸ì¶œìê°€ LLMìœ¼ë¡œ ìƒì„±
        "sources": top_results,
        "query": query
    }


async def _search_hospital_faiss(hospital_id: str, query: str) -> List[Dict[str, Any]]:
    """ë³‘ì› ì „ìš© FAISS ì¸ë±ìŠ¤ ê²€ìƒ‰ (ë‚´ë¶€ í—¬í¼)"""
    if not hospital_id:
        return []
    hospital_dir = PathLib(LOCAL_FAISS_BY_HOSPITAL) / hospital_id
    # faiss ë°”ì´ë„ˆë¦¬ ë˜ëŠ” LlamaIndex ë©”íƒ€ë°ì´í„° ì¡´ì¬ í™•ì¸
    faiss_binary = hospital_dir / "faiss.index"
    index_store = hospital_dir / "index_store.json"
    if not faiss_binary.exists() and not index_store.exists():
        return []
    if not FAISS_AVAILABLE:
        return []
    openai_api_key = os.environ.get("OPENAI_API_KEY") or settings.openai_api_key
    if not openai_api_key or openai_api_key == "dev-openai-key":
        return []
    try:
        embed_model = OpenAIEmbedding(model=EMBEDDING_MODEL, api_key=openai_api_key)
        Settings.embed_model = embed_model
        vector_store = FaissVectorStore.from_persist_dir(str(hospital_dir))
        from llama_index.core.storage.docstore import SimpleDocumentStore
        from llama_index.core.storage.index_store import SimpleIndexStore
        docstore = SimpleDocumentStore.from_persist_dir(str(hospital_dir))
        index_store = SimpleIndexStore.from_persist_dir(str(hospital_dir))
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store,
            docstore=docstore,
            index_store=index_store,
        )
        try:
            index = load_index_from_storage(storage_context)
        except ValueError as e:
            if "Please specify index_id" in str(e):
                index_structs = storage_context.index_store.index_structs()
                index_ids = list(index_structs.keys()) if isinstance(index_structs, dict) else [getattr(s, "index_id", None) for s in (index_structs or []) if hasattr(s, "index_id")]
                if index_ids and index_ids[0]:
                    index = load_index_from_storage(storage_context, index_id=index_ids[0])
                else:
                    return []
            else:
                return []
        retriever = index.as_retriever(similarity_top_k=5)
        nodes = await retriever.aretrieve(query)
        results = []
        for node in nodes:
            results.append({
                "text": clean_html_content(node.text)[:500] if hasattr(node, 'text') else "",
                "score": float(node.score) if hasattr(node, 'score') else 0.0,
                "metadata": node.metadata if hasattr(node, 'metadata') else {},
                "source_type": "hospital"
            })
        return results
    except Exception as e:
        print(f"[ERROR] ë³‘ì› RAG ê²€ìƒ‰ ì‹¤íŒ¨ (hospital_id={hospital_id}): {e}")
        return []
