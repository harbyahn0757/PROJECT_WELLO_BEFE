"""
RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤ ëª¨ë“ˆ
LlamaIndex ë° Gemini LLM ê¸°ë°˜ ì˜ë£Œ ì§€ì‹ ê²€ìƒ‰ ì—”ì§„

**ë¡œì»¬ FAISS ë²¡í„° DB ì§€ì› ì¶”ê°€**
- LlamaCloud API (ê¸°ë³¸ê°’)
- ë¡œì»¬ FAISS ë²¡í„° DB (ë¹„ìš© ì ˆê°, ë¹ ë¥¸ ì‘ë‹µ)
"""
import os
import json
import re
from typing import List, Dict, Any, Optional
from app.core.config import settings

# LlamaIndex RAG ê´€ë ¨ ì„í¬íŠ¸
try:
    from llama_index.core import Settings, VectorStoreIndex, StorageContext, load_index_from_storage
    from llama_index.core.llms import CustomLLM
    from llama_index.core.llms.llm import LLM
    from llama_index.core.llms import ChatMessage, MessageRole, CompletionResponse, LLMMetadata
    from llama_index.indices.managed.llama_cloud import LlamaCloudIndex
    from llama_index.llms.openai import OpenAI
    from llama_index.embeddings.openai import OpenAIEmbedding
    # FAISS ë²¡í„° ìŠ¤í† ì–´ ì„í¬íŠ¸
    try:
        import faiss
        import pickle
        from pathlib import Path as PathLib
        from llama_index.vector_stores.faiss import FaissVectorStore
        FAISS_AVAILABLE = True
    except ImportError:
        FAISS_AVAILABLE = False
    # Google GeminiëŠ” google-generativeai ì§ì ‘ ì‚¬ìš©í•˜ì—¬ CustomLLMìœ¼ë¡œ ë˜í•‘
    try:
        import google.generativeai as genai
        GEMINI_AVAILABLE = True
    except ImportError:
        GEMINI_AVAILABLE = False
        genai = None
    LLAMAINDEX_AVAILABLE = True
except ImportError as e:
    LLAMAINDEX_AVAILABLE = False
    GEMINI_AVAILABLE = False
    FAISS_AVAILABLE = False
    genai = None
    # ê°œë°œ í™˜ê²½ì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë”ë¯¸ í´ë˜ìŠ¤
    class LlamaCloudIndex:
        pass
    class OpenAI:
        pass
    class CustomLLM:
        pass
    class ChatMessage:
        pass
    class MessageRole:
        pass
    class CompletionResponse:
        pass
    class LLMMetadata:
        pass

# ìƒìˆ˜ ì •ì˜
LLAMACLOUD_INDEX_NAME = "Dr.Welno"
LLAMACLOUD_PROJECT_NAME = "Default"
LLAMACLOUD_INDEX_ID = "1bcef115-bb95-4d14-9c29-d38bb097a39c"
LLAMACLOUD_PROJECT_ID = "45c4d9d4-ce6b-4f62-ad88-9107fe6de8cc"
LLAMACLOUD_ORGANIZATION_ID = "e4024539-3d26-48b5-8051-9092380c84d2"

# ë¡œì»¬ FAISS ë²¡í„° DB ê²½ë¡œ
LOCAL_FAISS_DIR = "/data/vector_db/welno/faiss_db"
LOCAL_FAISS_INDEX_PATH = f"{LOCAL_FAISS_DIR}/faiss.index"
LOCAL_FAISS_METADATA_PATH = f"{LOCAL_FAISS_DIR}/metadata.pkl"

# ì „ì—­ RAG ì—”ì§„ ìºì‹œ (ê¸°ì¡´ í”Œë¡œìš°ìš©)
_rag_engine_cache: Optional[Any] = None
# í…ŒìŠ¤íŠ¸ìš© RAG ì—”ì§„ ìºì‹œ (ì—˜ë¼ë§ˆ í´ë¼ìš°ë“œ ìŠ¤íƒ€ì¼)
_rag_engine_test_cache: Optional[Any] = None
# ë¡œì»¬ FAISS ì—”ì§„ ìºì‹œ
_rag_engine_local_cache: Optional[Any] = None

# Gemini CustomLLM í´ë˜ìŠ¤
class GeminiLLM(CustomLLM):
    """Google Geminië¥¼ LlamaIndex CustomLLMìœ¼ë¡œ êµ¬í˜„ (RAG ê²€ìƒ‰ìš©)"""
    
    def __init__(self, api_key: str, model: str = "gemini-3-flash-preview", **kwargs):
        if not GEMINI_AVAILABLE or not genai:
            raise ImportError("google-generativeaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        super().__init__(**kwargs)
        genai.configure(api_key=api_key)
        self._model = genai.GenerativeModel(model)
        self._model_name = model
    
    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=8192,
            num_output=2048,
            is_chat_model=True,
            model_name=self._model_name
        )
    
    def complete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:
        try:
            response = self._model.generate_content(prompt)
            text = response.text if hasattr(response, 'text') else str(response)
            return CompletionResponse(text=text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def acomplete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:
        try:
            response = self._model.generate_content(prompt)
            text = response.text if hasattr(response, 'text') else str(response)
            return CompletionResponse(text=text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    def stream_complete(self, prompt: str, formatted: bool = False, **kwargs):
        try:
            response = self._model.generate_content(prompt, stream=True)
            for chunk in response:
                if hasattr(chunk, 'text') and chunk.text:
                    yield CompletionResponse(text=chunk.text, delta=chunk.text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
from llama_index.core import PromptTemplate

CHAT_SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ì¹œì ˆí•œ ê±´ê°• ìƒë‹´ê°€ 'Dr. Welno'ì…ë‹ˆë‹¤.\n"
    "\n"
    "âš ï¸ **í•µì‹¬ ê·œì¹™**:\n"
    "1. [Context]ì— ì—†ëŠ” ì •ë³´ëŠ” ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”\n"
    "2. ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì¦ìƒë§Œ ë‹¤ë£¨ì„¸ìš” (ì¶”ì¸¡ ê¸ˆì§€)\n"
    "3. í™˜ì ë°ì´í„° ì—†ìœ¼ë©´ \"í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€\n"
    "4. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª…í™•íˆ í‘œí˜„\n"
    "\n"
    "ğŸ’¡ **ë‹µë³€ ìŠ¤íƒ€ì¼**:\n"
    "- [Context]ì— í™˜ì ê±´ê°• ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê°„ë‹¨íˆ ì–¸ê¸‰í•˜ë©° ì‹œì‘\n"
    "- 'ì§€ì¹¨ì„œì— ë”°ë¥´ë©´' ê°™ì€ ë¶€ì—° ì„¤ëª… ìƒëµ, ì „ë¬¸ê°€ë‹µê²Œ ë‹¨í˜¸í•˜ê²Œ ë‹µë³€\n"
    "- í•µì‹¬ ìœ„ì£¼ë¡œ ìš”ì•½, ì‹¤ì²œ ë°©ë²•ì€ ë¶ˆë › í¬ì¸íŠ¸\n"
    "- ê·¼ê±° ë¬¸ì„œ ì–¸ê¸‰ ë¶ˆí•„ìš” (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í‘œì‹œ)\n"
    "\n"
    "ğŸ”— **ë‹µë³€ êµ¬ì¡°** (2ë‹¨ê³„):\n"
    "1ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€ (ë¨¼ì €)\n"
    "2ë‹¨ê³„: ì˜í•™ ë¬¸ì„œì— ì—°ê´€ì„± ëª…ì‹œ ì‹œì—ë§Œ ê³¼ê±° ë°ì´í„° ì–¸ê¸‰ (ë‚˜ì¤‘ì—)\n"
    "   - ì¶œì²˜ ëª…ì‹œ: \"2021ë…„ ê²€ì§„\", \"ë³µì•½ ë‚´ì—­\", \"ì´ì „ ë¬¸ì§„\"\n"
    "   - ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜ì–´ ì‚¬ìš©: \"ì°¸ê³ ë¡œ\", \"ê·¸ëŸ°ë°\", \"ë‹¤ë§Œ\"\n"
    "   ì˜ˆ: \"ì²´ì¤‘ ê´€ë¦¬ëŠ”... (ë‹µë³€) ì°¸ê³ ë¡œ, 2021ë…„ ê²€ì§„ì—ì„œ ì²´ì¤‘ ì¦ê°€ ì‹œ í˜ˆì••ë„ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤.\"\n"
    "\n"
    "ğŸ“‹ **ë°ì´í„° í™•ì¸**:\n"
    "- ì‚¬ìš©ì ë§ê³¼ ê¸°ì¡´ ë°ì´í„°(ê²€ì§„/ë³µì•½/ë¬¸ì§„)ê°€ ë‹¤ë¥¼ ë•Œ í™•ì¸ ì§ˆë¬¸\n"
    "  ì˜ˆ: \"2021ë…„ ê²€ì§„ì—ì„œ ì²´ì¤‘ ê°ì†Œ ì¶”ì„¸ì˜€ëŠ”ë°, ìµœê·¼ ìƒíƒœëŠ” ì–´ë–¤ê°€ìš”?\"\n"
    "- ì‹œìŠ¤í…œì´ ì§ì ‘ ë¬»ëŠ” ë°©ì‹ìœ¼ë¡œ í‘œí˜„\n"
    "\n"
    "ğŸ‘¨â€âš•ï¸ **ìƒë‹´ ìœ ë„**:\n"
    "- ë³µì¡í•œ ì¦ìƒì€ ìƒë‹´ì‚¬ ì—°ê²° ìœ ë„\n"
    "- ì˜ì–‘ì œ/ê±´ê¸°ì‹ ì§ˆë¬¸ ì‹œ PNT ë¬¸ì§„ ì œì•ˆ\n"
    "\n"
    "[Context]\n"
    "{context_str}\n"
    "\n"
    "ì‚¬ìš©ì ì§ˆë¬¸: {query_str}\n"
    "ì „ë¬¸ê°€ ë‹µë³€:"
)

async def init_rag_engine(use_elama_model: bool = False, use_local_vector_db: bool = True):
    """
    RAG Query Engine ì´ˆê¸°í™”
    
    Args:
        use_elama_model: Trueë©´ ì—˜ë¼ë§ˆ í´ë¼ìš°ë“œì˜ ëª¨ë¸ ì„¤ì • ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
                        Falseë©´ GeminiLLM ì‚¬ìš© (ê¸°ì¡´ ê²€ì§„ ì„¤ê³„ í”Œë¡œìš°ìš©)
        use_local_vector_db: Trueë©´ ë¡œì»¬ FAISS ë²¡í„° DB ì‚¬ìš© (ë¹„ìš© ì ˆê°, ë¹ ë¥¸ ì‘ë‹µ, ê¸°ë³¸ê°’)
                            Falseë©´ LlamaCloud API ì‚¬ìš©
    """
    global _rag_engine_cache, _rag_engine_test_cache, _rag_engine_local_cache
    
    # ìºì‹œ í™•ì¸
    if use_local_vector_db:
        if _rag_engine_local_cache is not None:
            print("[INFO] ë¡œì»¬ FAISS ì—”ì§„ ìºì‹œ ì‚¬ìš©")
            return _rag_engine_local_cache
    elif use_elama_model:
        if _rag_engine_test_cache is not None:
            return _rag_engine_test_cache
    else:
        if _rag_engine_cache is not None:
            return _rag_engine_cache
    
    if not LLAMAINDEX_AVAILABLE:
        print("[WARN] LlamaIndex ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # ===== ë¡œì»¬ FAISS ë²¡í„° DB ì‚¬ìš© =====
        if use_local_vector_db:
            if not FAISS_AVAILABLE:
                print("[WARN] FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                use_local_vector_db = False  # Fallback to cloud
            else:
                print("[INFO] ë¡œì»¬ FAISS ë²¡í„° DB ì´ˆê¸°í™” ì¤‘...")
                
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                if not PathLib(LOCAL_FAISS_INDEX_PATH).exists():
                    print(f"[WARN] FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LOCAL_FAISS_INDEX_PATH}")
                    print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                    use_local_vector_db = False
                else:
                    faiss_index = faiss.read_index(LOCAL_FAISS_INDEX_PATH)
                    print(f"[INFO] FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {faiss_index.ntotal}ê°œ ë²¡í„°")
                    
                    # ë©”íƒ€ë°ì´í„° ë¡œë“œ (ì„ íƒ ì‚¬í•­ìœ¼ë¡œ ë³€ê²½)
                    if PathLib(LOCAL_FAISS_METADATA_PATH).exists():
                        with open(LOCAL_FAISS_METADATA_PATH, 'rb') as f:
                            metadata = pickle.load(f)
                        total_docs = metadata.get('total_documents', metadata.get('total_nodes', faiss_index.ntotal))
                        print(f"[INFO] ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {total_docs}ê°œ ë¬¸ì„œ")
                    else:
                        print(f"[INFO] ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        total_docs = faiss_index.ntotal
                    
                    # OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ê²€ìƒ‰ìš©)
                        openai_api_key = os.environ.get("OPENAI_API_KEY") or settings.openai_api_key
                        if not openai_api_key or openai_api_key == "dev-openai-key":
                            print("[WARN] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                            use_local_vector_db = False
                        else:
                            embed_model = OpenAIEmbedding(
                                model="text-embedding-ada-002",
                                api_key=openai_api_key
                            )
                            Settings.embed_model = embed_model
                            
                            # Gemini LLM ì„¤ì • (ë‹µë³€ ìƒì„±ìš©)
                            gemini_api_key = os.environ.get("GOOGLE_GEMINI_API_KEY") or settings.google_gemini_api_key
                            if not gemini_api_key or gemini_api_key == "dev-gemini-key":
                                print("[WARN] Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                                use_local_vector_db = False
                            elif not GEMINI_AVAILABLE or not genai:
                                print("[WARN] Google Geminiê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                                print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                                use_local_vector_db = False
                            else:
                                llm = GeminiLLM(api_key=gemini_api_key, model="gemini-3-flash-preview")
                                Settings.llm = llm
                                
                                # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                                vector_store = FaissVectorStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                
                                # Storage Context ìƒì„± (docstoreì™€ index_storeë§Œ ë¡œë“œ)
                                print(f"[INFO] Storage Context ë¡œë“œ ì¤‘...")
                                from llama_index.core.storage.docstore import SimpleDocumentStore
                                from llama_index.core.storage.index_store import SimpleIndexStore
                                
                                docstore = SimpleDocumentStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                index_store = SimpleIndexStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                
                                storage_context = StorageContext.from_defaults(
                                    vector_store=vector_store,
                                    docstore=docstore,
                                    index_store=index_store
                                )
                                
                                # ì¸ë±ìŠ¤ ë¡œë“œ (index_id ëª…ì‹œ)
                                try:
                                    # ë¨¼ì € index_id ì—†ì´ ì‹œë„
                                    index = load_index_from_storage(storage_context)
                                except ValueError as e:
                                    if "Please specify index_id" in str(e):
                                        # index_storeì—ì„œ index_id ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                                        from llama_index.core.storage.index_store import SimpleIndexStore
                                        index_structs = storage_context.index_store.index_structs()
                                        
                                        if isinstance(index_structs, dict):
                                            index_ids = list(index_structs.keys())
                                        elif isinstance(index_structs, list):
                                            # listì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª©ì˜ index_id ì‚¬ìš©
                                            index_ids = [struct.index_id for struct in index_structs if hasattr(struct, 'index_id')]
                                        else:
                                            raise ValueError(f"Unexpected index_structs type: {type(index_structs)}")
                                        
                                        if index_ids:
                                            print(f"[INFO] ì—¬ëŸ¬ ì¸ë±ìŠ¤ ë°œê²¬, ì²« ë²ˆì§¸ ì‚¬ìš©: {index_ids[0]}")
                                            index = load_index_from_storage(storage_context, index_id=index_ids[0])
                                        else:
                                            raise ValueError("No index found in storage")
                                    else:
                                        raise
                                
                                # ì¿¼ë¦¬ ì—”ì§„ ìƒì„± (ì„±ëŠ¥ ìµœì í™” ë° ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì ìš©)
                                query_engine = index.as_query_engine(
                                    similarity_top_k=5,
                                    response_mode="compact",
                                    text_qa_template=PromptTemplate(CHAT_SYSTEM_PROMPT)
                                )
                                
                                _rag_engine_local_cache = query_engine
                                print(f"[INFO] âœ… ë¡œì»¬ FAISS RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (ë²¡í„°: {faiss_index.ntotal}ê°œ, ë¬¸ì„œ: {total_docs}ê°œ)")
                                
                                return query_engine
        
        # ===== LlamaCloud API ì‚¬ìš© (ê¸°ì¡´ ë¡œì§ ë˜ëŠ” Fallback) =====
        if not use_local_vector_db:
            llamaindex_api_key = os.environ.get("LLAMAINDEX_API_KEY") or settings.llamaindex_api_key
            
            if not llamaindex_api_key:
                print("[WARN] LlamaIndex API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            
            index = LlamaCloudIndex(
                index_id=LLAMACLOUD_INDEX_ID,
                api_key=llamaindex_api_key
            )
            
            # GeminiLLMì€ í•­ìƒ í•„ìš” (LlamaCloudIndexê°€ ê¸°ë³¸ì ìœ¼ë¡œ OpenAIë¥¼ ì°¾ìœ¼ë ¤ê³  í•¨)
            gemini_api_key = os.environ.get("GOOGLE_GEMINI_API_KEY") or settings.google_gemini_api_key
            
            if not gemini_api_key:
                print("[WARN] Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            
            if not GEMINI_AVAILABLE or not genai:
                return None
            
            llm = GeminiLLM(api_key=gemini_api_key, model="gemini-3-flash-preview")
            
            if use_elama_model:
                # í…ŒìŠ¤íŠ¸ìš©: GeminiLLM ì‚¬ìš©
                query_engine = index.as_query_engine(
                    llm=llm,
                    similarity_top_k=5,
                    response_mode="tree_summarize"
                )
                _rag_engine_test_cache = query_engine
                print(f"[INFO] RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (í…ŒìŠ¤íŠ¸ìš© - GeminiLLM, LlamaCloud Index ID: {LLAMACLOUD_INDEX_ID})")
            else:
                # ê¸°ì¡´ í”Œë¡œìš°: GeminiLLM ì‚¬ìš© + Settings.llm ì„¤ì •
                Settings.llm = llm
                
                query_engine = index.as_query_engine(
                    similarity_top_k=5,
                    response_mode="tree_summarize"
                )
                
                _rag_engine_cache = query_engine
                print(f"[INFO] RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (LlamaCloud Index ID: {LLAMACLOUD_INDEX_ID})")
            
            return query_engine
        
    except Exception as e:
        print(f"[ERROR] RAG ì—”ì§„ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def clean_html_content(text: str) -> str:
    """
    HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    return text

def extract_evidence_from_source_nodes(response) -> List[Dict[str, Any]]:
    """LlamaIndex ì‘ë‹µì—ì„œ ì†ŒìŠ¤ ë…¸ë“œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    evidences = []
    if hasattr(response, 'source_nodes'):
        for node in response.source_nodes:
            metadata = node.metadata if hasattr(node, 'metadata') else {}
            text = node.text if hasattr(node, 'text') else ""
            
            # HTML íƒœê·¸ ì •ì œ
            text = clean_html_content(text)
            
            score = node.score if hasattr(node, 'score') else 0.0
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            file_name = metadata.get('file_name', 'Unknown')
            page_label = metadata.get('page_label', 'Unknown')
            
            # ì¸ìš©êµ¬ ì¶”ì¶œ
            citation = text[:100] + "..." if len(text) > 100 else text
            
            evidences.append({
                "source_document": file_name,
                "page": page_label,
                "citation": citation,
                "full_text": text,
                "confidence_score": score,
                "organization": "ì˜í•™íšŒ",
                "year": "2024"
            })
    return evidences


async def get_medical_evidence_from_rag(query_engine, patient_context: Dict, concerns: List[Dict]) -> Dict[str, Any]:
    """
    í™˜ì ì •ë³´ì™€ ì—¼ë ¤ ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ RAG ê²€ìƒ‰ ìˆ˜í–‰
    """
    if not query_engine:
        return {"context_text": "", "structured_evidences": []}
    
    try:
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        query_parts = []
        
        # 1. ë‚˜ì´/ì„±ë³„ ê¸°ë°˜
        age = patient_context.get('age', 40)
        gender = patient_context.get('gender', 'unknown')
        query_parts.append(f"{age}ì„¸ {gender}ì—ê²Œ ê¶Œì¥ë˜ëŠ” í•„ìˆ˜ ê±´ê°•ê²€ì§„ í•­ëª©ê³¼ ì•” ì„ ë³„ê²€ì‚¬ ê¸°ì¤€")
        
        # 2. ì´ìƒ ì†Œê²¬ ê¸°ë°˜
        abnormal_items = patient_context.get('abnormal_items', [])
        for item in abnormal_items:
            name = item.get('name', '')
            status = item.get('status', '')
            if name:
                query_parts.append(f"{name} ìˆ˜ì¹˜ê°€ {status}ì¼ ë•Œ í•„ìš”í•œ ì •ë°€ ê²€ì‚¬ì™€ ì„ìƒì  ì˜ì˜")
        
        # 3. ê°€ì¡±ë ¥ ê¸°ë°˜
        family_history = patient_context.get('family_history', [])
        for fh in family_history:
            query_parts.append(f"{fh} ê°€ì¡±ë ¥ì´ ìˆì„ ë•Œ ê¶Œì¥ë˜ëŠ” ì¡°ê¸° ì„ ë³„ê²€ì‚¬")
            
        # 4. ì—¼ë ¤ í•­ëª© ê¸°ë°˜
        for concern in concerns:
            c_name = concern.get('name', '')
            if c_name:
                query_parts.append(f"{c_name} ê´€ë ¨ ìµœì‹  ì§„ë£Œì§€ì¹¨ê³¼ ê²€ì‚¬ ê¶Œê³ ì•ˆ")
        
        final_query = " \n".join(query_parts)
        print(f"[INFO] RAG ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±: {len(final_query)}ì")
        
        # ê²€ìƒ‰ ì‹¤í–‰ - aretrieve() ì‚¬ìš© (ë²¡í„° ê²€ìƒ‰ë§Œ, LLM ì‘ë‹µ ìƒì„± ì œê±°)
        nodes = await query_engine.aretrieve(final_query)
        
        # ê²°ê³¼ ì²˜ë¦¬ - source_nodesì—ì„œ ì§ì ‘ ì¶”ì¶œ
        structured_evidences = []
        for node in nodes:
            metadata = node.metadata if hasattr(node, 'metadata') else {}
            text = node.text if hasattr(node, 'text') else ""
            
            # HTML íƒœê·¸ ì •ì œ
            text = clean_html_content(text)
            
            score = node.score if hasattr(node, 'score') else 0.0
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            file_name = metadata.get('file_name', 'Unknown')
            page_label = metadata.get('page_label', 'Unknown')
            
            # ì¸ìš©êµ¬ ì¶”ì¶œ
            citation = text[:100] + "..." if len(text) > 100 else text
            
            structured_evidences.append({
                "source_document": file_name,
                "page": page_label,
                "citation": citation,
                "full_text": text,
                "confidence_score": score,
                "organization": "ì˜í•™íšŒ",
                "year": "2024"
            })
        
        # ê° ì—ë¹„ë˜ìŠ¤ì— ì¿¼ë¦¬ ë§¥ë½ ì¶”ê°€
        for ev in structured_evidences:
            ev['query'] = final_query
            ev['category'] = 'Clinical Guideline'
        
        # structured_evidencesë¥¼ context_textë¡œ í¬ë§·íŒ… (LLM ì‘ë‹µ ëŒ€ì‹  ì›ë³¸ ë¬¸ì„œ ì‚¬ìš©)
        # ê°„ë‹¨í•œ í¬ë§·íŒ… (ìˆœí™˜ import ë°©ì§€)
        context_text = ""
        if structured_evidences:
            formatted_parts = []
            for idx, ev in enumerate(structured_evidences[:5], 1):  # ìƒìœ„ 5ê°œë§Œ
                doc_name = ev.get('source_document', 'ë¬¸ì„œëª… ì—†ìŒ')
                citation = ev.get('citation', '')
                if citation:
                    formatted_parts.append(f"{idx}. [{doc_name}]\n\"{citation}\"\n")
            context_text = "\n".join(formatted_parts)
        
        return {
            "context_text": context_text,
            "structured_evidences": structured_evidences
        }
        
    except Exception as e:
        print(f"[ERROR] RAG ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        return {"context_text": "", "structured_evidences": []}


async def search_checkup_knowledge(query: str, use_local_vector_db: bool = True) -> Dict[str, Any]:
    """
    ê²€ì§„ ì§€ì‹ ê²€ìƒ‰ (RAG)
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        use_local_vector_db: Trueë©´ ë¡œì»¬ FAISS ì‚¬ìš©, Falseë©´ LlamaCloud ì‚¬ìš©
    """
    query_engine = await init_rag_engine(use_local_vector_db=use_local_vector_db)
    
    if not query_engine:
        return {
            "success": False,
            "error": "RAG ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨",
            "answer": None,
            "sources": []
        }
    
    try:
        # ë¹„ë™ê¸° ê²€ìƒ‰ìœ¼ë¡œ ë³€ê²½
        response = await query_engine.aquery(query)
        
        # ì†ŒìŠ¤ ì¶”ì¶œ
        sources = []
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                source_info = {
                    "text": clean_html_content(node.text)[:500],
                    "score": float(node.score) if hasattr(node, 'score') else None,
                    "metadata": node.metadata if hasattr(node, 'metadata') else {}
                }
                sources.append(source_info)
        
        return {
            "success": True,
            "answer": str(response),
            "sources": sources,
            "query": query
        }
    
    except Exception as e:
        print(f"[ERROR] RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "answer": None,
            "sources": []
        }
