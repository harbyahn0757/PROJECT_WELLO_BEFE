"""
RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤ ëª¨ë“ˆ
LlamaIndex ë° Gemini LLM ê¸°ë°˜ ì˜ë£Œ ì§€ì‹ ê²€ìƒ‰ ì—”ì§„

**ë¡œì»¬ FAISS ë²¡í„° DB ì§€ì› ì¶”ê°€**
- LlamaCloud API (ê¸°ë³¸ê°’)
- ë¡œì»¬ FAISS ë²¡í„° DB (ë¹„ìš© ì ˆê°, ë¹ ë¥¸ ì‘ë‹µ)
"""
import os
import json
import re
from typing import List, Dict, Any, Optional
from app.core.config import settings

# LlamaIndex RAG ê´€ë ¨ ì„í¬íŠ¸
try:
    from llama_index.core import Settings, VectorStoreIndex, StorageContext, load_index_from_storage
    from llama_index.core.llms import CustomLLM
    from llama_index.core.llms.llm import LLM
    from llama_index.core.llms import ChatMessage, MessageRole, CompletionResponse, LLMMetadata
    from llama_index.indices.managed.llama_cloud import LlamaCloudIndex
    from llama_index.llms.openai import OpenAI
    from llama_index.embeddings.openai import OpenAIEmbedding
    # FAISS ë²¡í„° ìŠ¤í† ì–´ ì„í¬íŠ¸
    try:
        import faiss
        import pickle
        from pathlib import Path as PathLib
        from llama_index.vector_stores.faiss import FaissVectorStore
        FAISS_AVAILABLE = True
    except ImportError:
        FAISS_AVAILABLE = False
    # Google GeminiëŠ” google-generativeai ì§ì ‘ ì‚¬ìš©í•˜ì—¬ CustomLLMìœ¼ë¡œ ë˜í•‘
    try:
        import google.generativeai as genai
        GEMINI_AVAILABLE = True
    except ImportError:
        GEMINI_AVAILABLE = False
        genai = None
    LLAMAINDEX_AVAILABLE = True
except ImportError as e:
    LLAMAINDEX_AVAILABLE = False
    GEMINI_AVAILABLE = False
    FAISS_AVAILABLE = False
    genai = None
    # ê°œë°œ í™˜ê²½ì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë”ë¯¸ í´ë˜ìŠ¤
    class LlamaCloudIndex:
        pass
    class OpenAI:
        pass
    class CustomLLM:
        pass
    class ChatMessage:
        pass
    class MessageRole:
        pass
    class CompletionResponse:
        pass
    class LLMMetadata:
        pass

# ìƒìˆ˜ ì •ì˜
LLAMACLOUD_INDEX_NAME = "Dr.Welno"
LLAMACLOUD_PROJECT_NAME = "Default"
LLAMACLOUD_INDEX_ID = "1bcef115-bb95-4d14-9c29-d38bb097a39c"
LLAMACLOUD_PROJECT_ID = "45c4d9d4-ce6b-4f62-ad88-9107fe6de8cc"
LLAMACLOUD_ORGANIZATION_ID = "e4024539-3d26-48b5-8051-9092380c84d2"

# ë¡œì»¬ FAISS ë²¡í„° DB ê²½ë¡œ
LOCAL_FAISS_DIR = "/data/vector_db/welno/faiss_db"
LOCAL_FAISS_INDEX_PATH = f"{LOCAL_FAISS_DIR}/faiss.index"
LOCAL_FAISS_METADATA_PATH = f"{LOCAL_FAISS_DIR}/metadata.pkl"

# ì „ì—­ RAG ì—”ì§„ ìºì‹œ (ê¸°ì¡´ í”Œë¡œìš°ìš©)
_rag_engine_cache: Optional[Any] = None
# í…ŒìŠ¤íŠ¸ìš© RAG ì—”ì§„ ìºì‹œ (ì—˜ë¼ë§ˆ í´ë¼ìš°ë“œ ìŠ¤íƒ€ì¼)
_rag_engine_test_cache: Optional[Any] = None
# ë¡œì»¬ FAISS ì—”ì§„ ìºì‹œ
_rag_engine_local_cache: Optional[Any] = None

# Gemini CustomLLM í´ë˜ìŠ¤
class GeminiLLM(CustomLLM):
    """Google Geminië¥¼ LlamaIndex CustomLLMìœ¼ë¡œ êµ¬í˜„ (RAG ê²€ìƒ‰ìš©)"""
    
    def __init__(self, api_key: str, model: str = "gemini-3-flash-preview", **kwargs):
        if not GEMINI_AVAILABLE or not genai:
            raise ImportError("google-generativeaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        super().__init__(**kwargs)
        genai.configure(api_key=api_key)
        self._model = genai.GenerativeModel(model)
        self._model_name = model
    
    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=8192,
            num_output=2048,
            is_chat_model=True,
            model_name=self._model_name
        )
    
    def complete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:
        try:
            response = self._model.generate_content(prompt)
            text = response.text if hasattr(response, 'text') else str(response)
            return CompletionResponse(text=text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def acomplete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:
        try:
            response = self._model.generate_content(prompt)
            text = response.text if hasattr(response, 'text') else str(response)
            return CompletionResponse(text=text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    def stream_complete(self, prompt: str, formatted: bool = False, **kwargs):
        try:
            response = self._model.generate_content(prompt, stream=True)
            for chunk in response:
                if hasattr(chunk, 'text') and chunk.text:
                    yield CompletionResponse(text=chunk.text, delta=chunk.text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
from llama_index.core import PromptTemplate

CHAT_SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ì¹œì ˆí•œ ê±´ê°• ìƒë‹´ê°€ 'Dr. Welno'ì…ë‹ˆë‹¤.\n"
    "ë‹¤ìŒ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì§€ì¼œ ë‹µë³€í•˜ì„¸ìš”.\n"
    "\n"
    "âš ï¸ **í™˜ê° ë°©ì§€ ê·œì¹™ (ìµœìš°ì„  - ì ˆëŒ€ ìœ„ë°° ê¸ˆì§€)**:\n"
    "1. **ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€**: ì œê³µëœ [Context]ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "2. **ì‚¬ìš©ì ì–¸ê¸‰ ì¦ìƒë§Œ ì‚¬ìš©**: ì‚¬ìš©ìê°€ ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰í•œ ì¦ìƒì´ë‚˜ ë¬¸ì œë§Œ ë‹¤ë£¨ì„¸ìš”. ì‚¬ìš©ìê°€ ë§í•˜ì§€ ì•Šì€ ì¦ìƒ(ì˜ˆ: ë¹„ì—¼, ì½” ì ë§‰ ë¶€ìŒ, ëˆˆ ì¶©í˜ˆ, ê·€ ê°€ë ¤ì›€, ì´ëª… ë“±)ì€ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "3. **í™˜ì ë°ì´í„° í™•ì¸**: íŠ¹ì • í™˜ì(ì˜ˆ: 'ì•ˆê´‘ìˆ˜')ì˜ ë°ì´í„°ë¥¼ ìš”ì²­ë°›ì•˜ì„ ë•Œ, [Context]ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ \"í•´ë‹¹ í™˜ìì˜ ë°ì´í„°ê°€ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ëª…í™•íˆ ë‹µë³€í•˜ì„¸ìš”.\n"
    "4. **ê²€ì‚¬/ê±´ê¸°ì‹ ì¶”ì²œ**: ê²€ì‚¬ í•­ëª©ì´ë‚˜ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì¶”ì²œ ì‹œ, ë°˜ë“œì‹œ PNT ë§¤íŠ¸ë¦­ìŠ¤ ë°ì´í„°ë¥¼ ìš°ì„  ì°¸ì¡°í•˜ê³ , ì—†ëŠ” í•­ëª©ì€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.\n"
    "5. **ì¶œì²˜ ëª…í™•í™”**: ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , \"ì œê³µëœ ì •ë³´ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n"
    "6. **ì¦ìƒ ì¶”ì¸¡ ê¸ˆì§€**: ì‚¬ìš©ìê°€ 'ë¨¸ë¦¬ê°€ ì•„í”„ë‹¤'ê³ ë§Œ ë§í–ˆë‹¤ë©´, ë¹„ì—¼, ì½” ì ë§‰ ë¶€ìŒ, ëˆˆ ì¶©í˜ˆ, ê·€ ê°€ë ¤ì›€, ì´ëª… ë“± ë‹¤ë¥¸ ì¦ìƒì„ ì¶”ì¸¡í•˜ì—¬ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ ì‚¬ìš©ìê°€ ì§ì ‘ ì–¸ê¸‰í•œ ì¦ìƒë§Œ ë‹¤ë£¨ì„¸ìš”.\n"
    "\n"
    "ğŸ’¡ **ë‹µë³€ ìŠ¤íƒ€ì¼**:\n"
    "1. **ë§ì¶¤í˜• ë¸Œë¦¬í•‘**: ë§Œì•½ [Context]ì— 'í™˜ì ìµœê·¼ ê±´ê°• ìƒíƒœ' ì •ë³´ê°€ ìˆë‹¤ë©´, ì²« ì¸ì‚¬ì™€ í•¨ê»˜ í•´ë‹¹ ì •ë³´ë¥¼ ì•„ì£¼ ì§§ê²Œ ì–¸ê¸‰í•˜ë©° ìƒë‹´ì„ ì‹œì‘í•˜ì„¸ìš”.\n"
    "   - **ì£¼ì˜**: ì‹¤ì œë¡œ [Context]ì— ìˆëŠ” ì •ë³´(ì˜ˆ: BMI, í˜ˆì••, ì§ˆí™˜ ì˜ì‹¬ ì†Œê²¬)ë§Œ ì–¸ê¸‰í•˜ì„¸ìš”. 'ë¬¸ì§„í‘œë¥¼ í™•ì¸í–ˆë‹¤'ê±°ë‚˜ [Context]ì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì–´ ë§í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "2. **ì§ì„¤ì  ë‹µë³€**: 'ì§€ì¹¨ì„œì— ë”°ë¥´ë©´', 'ì œê³µëœ ì •ë³´ì— ì˜í•˜ë©´'ê³¼ ê°™ì€ ë¶€ì—° ì„¤ëª…ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”. ì „ë¬¸ê°€ë¡œì„œ ìì—°ìŠ¤ëŸ½ê³  ë‹¨í˜¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
    "3. **ì§ˆë¬¸ ì§‘ì¤‘**: ë¸Œë¦¬í•‘ ì´í›„ì—ëŠ” ì‚¬ìš©ìê°€ í˜„ì¬ ë¬»ê³  ìˆëŠ” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì— ì§‘ì¤‘í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
    "4. **ê°„ê²°ì„±**: ë‹µë³€ì€ í•µì‹¬ ìœ„ì£¼ë¡œ ìš”ì•½í•˜ê³ , êµ¬ì²´ì ì¸ ì‹¤ì²œ ë°©ë²•ì€ ë¶ˆë › í¬ì¸íŠ¸ë¡œ ì •ë¦¬í•˜ì„¸ìš”.\n"
    "5. **ì—ë¹„ë˜ìŠ¤ ì œì™¸**: ê·¼ê±° ë¬¸ì„œë‚˜ ì¶œì²˜ì— ëŒ€í•œ ì–¸ê¸‰ì€ ë‹µë³€ í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ìë™ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤)\n"
    "\n"
    "ğŸ”— **ë§¥ë½ ì—°ê²° ë‹µë³€ êµ¬ì¡° (í•„ìˆ˜)**:\n"
    "ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¤ìŒ 2ë‹¨ê³„ êµ¬ì¡°ë¥¼ ë”°ë¥´ì„¸ìš”:\n"
    "\n"
    "**1ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ ë‹µë³€ (ë¨¼ì €)**\n"
    "- ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ë‚´ìš©ì— ëŒ€í•´ì„œë§Œ ë¨¼ì € ë‹µë³€í•˜ì„¸ìš”. ê²€ì§„ ë°ì´í„°ë‚˜ ë‹¤ë¥¸ ë‚´ìš© ì–¸ê¸‰ ì—†ì´, ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”.\n"
    "- ì˜ˆ: \"ì‚´ì´ ì¢€ ì°ŒëŠ”ê±° ê°™ì€ë°\" â†’ ì²´ì¤‘ ê´€ë¦¬ ë°©ë²•, ìš´ë™, ì‹ë‹¨ ì¡°ì ˆ ë“±ì— ëŒ€í•´ ë¨¼ì € ë‹µë³€\n"
    "\n"
    "**2ë‹¨ê³„: ê²€ì§„ ë°ì´í„° ì—°ê´€ì„± ì–¸ê¸‰ (ë‚˜ì¤‘ì—, ì¡°ê±´ë¶€)**\n"
    "- 1ë‹¨ê³„ ë‹µë³€ í›„, [Context]ì— ìˆëŠ” í™˜ìì˜ ê³¼ê±° ê²€ì§„/ë³µì•½/ë¬¸ì§„ ë°ì´í„°ì™€ì˜ ì—°ê´€ì„±ì„ ì–¸ê¸‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
    "- **í•„ìˆ˜ ì¡°ê±´**: [Context]ì˜ ì˜í•™ ì§€ì‹ ë¬¸ì„œ(ì°¸ê³  ë¬¸í—Œ)ì— ì—°ê´€ì„±ì´ ëª…ì‹œë˜ì–´ ìˆì„ ë•Œë§Œ ì–¸ê¸‰í•˜ì„¸ìš”.\n"
    "- **ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°**: ê°‘ì‘ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì§€ ë§ê³ , \"ì°¸ê³ ë¡œ\", \"ê·¸ëŸ°ë°\", \"ë‹¤ë§Œ\" ê°™ì€ ì—°ê²°ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì„¸ìš”.\n"
    "- **ë°ì´í„° ì¶œì²˜ ëª…í™•íˆ í‘œì‹œ**: ì–´ë–¤ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ í•˜ëŠ”ì§€ ë°˜ë“œì‹œ ëª…ì‹œí•˜ì„¸ìš”.\n"
    "   - ê²€ì§„ ë°ì´í„°: \"2021ë…„ ê²€ì§„ ê²°ê³¼ë¥¼ ë³´ë©´...\", \"2022ë…„ ê±´ê°•ê²€ì§„ì—ì„œ í™•ì¸ëœ...\"\n"
    "   - ë³µì•½ ë‚´ì—­: \"2023ë…„ ë³µì•½ ë‚´ì—­ì„ í™•ì¸í•´ë³´ë‹ˆ...\", \"ê³¼ê±° ì²˜ë°© ê¸°ë¡ì„ ë³´ë©´...\"\n"
    "   - ë¬¸ì§„ ë‚´ì—­: \"ì´ì „ ë¬¸ì§„ì—ì„œ ë§ì”€í•˜ì‹ ...\", \"ë¬¸ì§„í‘œì— ê¸°ë¡ëœ...\"\n"
    "   - ì˜í•™ ì§€ì‹ ë¬¸ì„œ: \"ì˜í•™ ì§€ì¹¨ì„œì— ë”°ë¥´ë©´...\", \"ìµœì‹  ì—°êµ¬ ê²°ê³¼ì— ì˜í•˜ë©´...\"\n"
    "- ì˜í•™ ì§€ì‹ ë¬¸ì„œì— ì—°ê´€ì„±ì´ ì—†ìœ¼ë©´ ê²€ì§„ ë°ì´í„°ë¥¼ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "\n"
    "âš ï¸ **ì¤‘ìš”**: ì´ 2ë‹¨ê³„ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ë˜, ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ë”±ë”±í•˜ê²Œ ë‚˜ì—´í•˜ì§€ ë§ê³  ëŒ€í™”í•˜ë“¯ì´ ì‘ì„±í•˜ì„¸ìš”.\n"
    "\n"
    "ğŸ“‹ **ë°ì´í„° ë¶ˆì¼ì¹˜ ê°ì§€ ë° í™•ì¸ ì§ˆë¬¸ (ì¤‘ìš”)**:\n"
    "1. **ê²€ì§„ ë°ì´í„°, ë³µì•½ ë‚´ì—­, ë¬¸ì§„ ë‚´ì—­ ì¢…í•© í™•ì¸**:\n"
    "   - ì‚¬ìš©ìì˜ ë§ê³¼ ê¸°ì¡´ ë°ì´í„°(ê²€ì§„, ë³µì•½, ë¬¸ì§„)ë¥¼ ë¹„êµí•˜ì„¸ìš”.\n"
    "   - ë°ì´í„°ì™€ ì‚¬ìš©ì ë§ì´ ìœ„ë°°ë  ë•ŒëŠ” í™•ì¸ ì§ˆë¬¸ì„ í•˜ì„¸ìš”.\n"
    "\n"
    "2. **í™•ì¸ ì§ˆë¬¸ ì˜ˆì‹œ**:\n"
    "   - âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: \"2021ë…„, 2022ë…„ ê²€ì§„ ê²°ê³¼ë¥¼ ë³´ë©´ ì²´ì¤‘ì´ ê°ì†Œí•˜ëŠ” ì¶”ì„¸ì˜€ëŠ”ë°, ìš”ì¦˜ì€ ì–´ë–¤ ìƒíƒœì¸ì§€ ê¶ê¸ˆí•©ë‹ˆë‹¤. ìµœê·¼ì— ì²´ì¤‘ì´ ë‹¤ì‹œ ì¦ê°€í•˜ì…¨ë‚˜ìš”?\"\n"
    "   - âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: \"ì´ì „ ë¬¸ì§„ì—ì„œ ìš´ë™ì„ ì£¼ 3íšŒ ì´ìƒ í•˜ì‹ ë‹¤ê³  í•˜ì…¨ëŠ”ë°, ìµœê·¼ì—ëŠ” ìš´ë™ ë¹ˆë„ê°€ ì¤„ì–´ë“œì…¨ë‚˜ìš”?\"\n"
    "   - âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: \"ê³¼ê±° ë³µì•½ ë‚´ì—­ì„ í™•ì¸í•´ë³´ë‹ˆ ê³ í˜ˆì•• ì•½ë¬¼ì„ ë³µìš© ì¤‘ì´ì…¨ëŠ”ë°, í˜„ì¬ë„ ë³µìš©í•˜ê³  ê³„ì‹ ê°€ìš”?\"\n"
    "   - âŒ ì˜ëª»ëœ ì˜ˆ: \"í˜ˆì•• ê´€ë¦¬ ì¸¡ë©´ì—ì„œ...\" (ì‚¬ìš©ìê°€ í˜ˆì••ì„ ë¬¼ì–´ë³´ì§€ ì•Šì•˜ê³ , ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì—°ê²°)\n"
    "\n"
    "3. **ëŒ€í™”í˜• ì •ë³´ ìˆ˜ì§‘ ë° ì§ˆë¬¸ ë°©ì‹**:\n"
    "   - í™•ì¸ ì§ˆë¬¸ì€ ì‹œìŠ¤í…œì´ ì§ì ‘ ë¬¼ì–´ë³´ëŠ” ë°©ì‹ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”.\n"
    "   - âŒ ì˜ëª»ëœ ì˜ˆ: \"ì–´ëŠ ì‹œê°„ëŒ€ì— ê°€ì¥ í”¼ê³¤í•˜ì‹ ê°€ìš”?\" (ì‚¬ìš©ìê°€ ì§ì ‘ ë¬¼ì–´ë³´ëŠ” ê²ƒì²˜ëŸ¼ í‘œí˜„)\n"
    "   - âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: \"ì •í™•í•œ ìƒíƒœ íŒŒì•…ì„ ìœ„í•´ ëª‡ ê°€ì§€ ì§ˆë¬¸ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. í•˜ë£¨ ì¤‘ ì–´ëŠ ì‹œê°„ëŒ€ì— ê°€ì¥ í”¼ê³¤í•˜ì‹ ê°€ìš”?\"\n"
    "   - âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: \"ë” ì •í™•í•œ ìƒë‹´ì„ ìœ„í•´ ë‹¤ìŒ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤: 1) í•˜ë£¨ ì¤‘ ê°€ì¥ í”¼ê³¤í•œ ì‹œê°„ëŒ€, 2) ìµœê·¼ ê²€ì§„ ê²°ê³¼, 3) í‰ì†Œ ì§€ë³‘ ì—¬ë¶€\"\n"
    "   - í™•ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ì‚¬ìš©ì ë‹µë³€ì„ ê¸°ì–µí•˜ê³  ë‹¤ìŒ ë‹µë³€ì— ë°˜ì˜í•˜ì„¸ìš”.\n"
    "   - íˆìŠ¤í† ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”.\n"
    "   - ì„¼ìŠ¤ ìˆê²Œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ë©° ìƒë‹´ì„ ì§„í–‰í•˜ì„¸ìš”.\n"
    "\n"
    "ğŸ” **ì˜í•™ ì§€ì‹ ê¸°ë°˜ ì—°ê´€ì„± í™•ì¸ ê·œì¹™**:\n"
    "- ê²€ì§„ ë°ì´í„°ë¥¼ ì–¸ê¸‰í•˜ê¸° ì „ì—, ë°˜ë“œì‹œ [Context]ì˜ ì˜í•™ ì§€ì‹ ë¬¸ì„œ(ì°¸ê³  ë¬¸í—Œ)ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n"
    "- ì˜í•™ ì§€ì‹ ë¬¸ì„œì— ë‘ í•­ëª©(ì˜ˆ: ì²´ì¤‘ê³¼ í˜ˆì••)ì˜ ì—°ê´€ì„±ì´ ëª…ì‹œë˜ì–´ ìˆì„ ë•Œë§Œ ì–¸ê¸‰í•˜ì„¸ìš”.\n"
    "- ì˜í•™ ì§€ì‹ ë¬¸ì„œì— ì—°ê´€ì„±ì´ ì—†ìœ¼ë©´, ê²€ì§„ ë°ì´í„°ì— ìˆë”ë¼ë„ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "- ì˜ˆ: ì˜í•™ ì§€ì‹ ë¬¸ì„œì— \"ì²´ì¤‘ ì¦ê°€ëŠ” í˜ˆì•• ìƒìŠ¹ê³¼ ì—°ê´€ì´ ìˆë‹¤\"ëŠ” ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ â†’ ê²€ì§„ ë°ì´í„°ì˜ ì²´ì¤‘ê³¼ í˜ˆì••ì„ í•¨ê»˜ ì–¸ê¸‰ ê°€ëŠ¥\n"
    "\n"
    "ğŸ’¬ **ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²° ë°©ë²•**:\n"
    "- 1ë‹¨ê³„ ë‹µë³€ í›„, ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³¼ê±° ë°ì´í„°ë¡œ ì—°ê²°í•˜ì„¸ìš”.\n"
    "- ì˜ˆ: \"ì²´ì¤‘ ê´€ë¦¬ë¥¼ ìœ„í•´ì„œëŠ”... (1ë‹¨ê³„ ë‹µë³€) ì°¸ê³ ë¡œ, 2021ë…„ ê²€ì§„ ê²°ê³¼ë¥¼ ë³´ë©´ ì²´ì¤‘ì´ ì¦ê°€í–ˆì„ ë•Œ í˜ˆì••ë„ í•¨ê»˜ ê²½ê³„ ìˆ˜ì¤€ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. (ì˜í•™ ì§€ì¹¨ì„œì— ì²´ì¤‘ê³¼ í˜ˆì••ì˜ ì—°ê´€ì„±ì´ ëª…ì‹œë˜ì–´ ìˆì„ ë•Œë§Œ)\"\n"
    "- ì˜ˆ: \"ìš´ë™ì„ ê¾¸ì¤€íˆ í•˜ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤... (1ë‹¨ê³„ ë‹µë³€) ê·¸ëŸ°ë°, ì´ì „ ë¬¸ì§„ì—ì„œ ìš´ë™ ë¹ˆë„ê°€ ì£¼ 3íšŒë¼ê³  í•˜ì…¨ëŠ”ë°, ìµœê·¼ì—ëŠ” ì–´ë–¤ê°€ìš”?\"\n"
    "- ê°‘ì‘ìŠ¤ëŸ½ê²Œ \"í˜ˆì•• ê´€ë¦¬ ì¸¡ë©´ì—ì„œ\"ë¼ê³  ì‹œì‘í•˜ì§€ ë§ê³ , ë¨¼ì € ì²´ì¤‘ ê´€ë¦¬ì— ëŒ€í•´ ë‹µë³€í•œ í›„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì„¸ìš”.\n"
    "\n"
    "ğŸ‘¨â€âš•ï¸ **ìƒë‹´ì‚¬ ì—°ê²° ë° PNT ë¬¸ì§„ ìœ ë„**:\n"
    "- ë„ˆë¬´ ì˜í•™ì ìœ¼ë¡œ ì ‘ê·¼í•˜ì§€ ë§ê³ , ìƒë‹´ì‚¬ì™€ì˜ ì—°ê²°ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„í•˜ì„¸ìš”.\n"
    "- ë³µì¡í•œ ì˜í•™ì  ì§„ë‹¨ì´ë‚˜ ì¶”ì¸¡ë³´ë‹¤ëŠ”, ìƒë‹´ì‚¬ì™€ì˜ ìƒë‹´ì„ í†µí•´ ì •í™•í•œ ìƒíƒœ íŒŒì•…ì„ ê¶Œì¥í•˜ì„¸ìš”.\n"
    "- PNT ë¬¸ì§„ì´ í•„ìš”í•œ ê²½ìš° ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„í•˜ì„¸ìš”.\n"
    "- ì˜ˆ: \"ì •í™•í•œ ìƒíƒœ íŒŒì•…ì„ ìœ„í•´ ì „ë¬¸ ìƒë‹´ì‚¬ì™€ì˜ ìƒë‹´ì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤. ë” ì •ë°€í•œ ë§ì¶¤ ì˜ì–‘ ì¹˜ë£Œ(PNT)ë¥¼ ìœ„í•´ ê°„ë‹¨í•œ ë¬¸ì§„ì„ ì§„í–‰í•´ ë³´ì‹œëŠ” ê²ƒë„ ì¢‹ê² ìŠµë‹ˆë‹¤.\"\n"
    "- ì˜ˆ: \"ì´ëŸ° ì¦ìƒë“¤ì€ ì—¬ëŸ¬ ì›ì¸ì´ ìˆì„ ìˆ˜ ìˆì–´, ì „ë¬¸ ìƒë‹´ì‚¬ì™€ í•¨ê»˜ ì •í™•í•œ ì›ì¸ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. PNT ë¬¸ì§„ì„ í†µí•´ ë” ë§ì¶¤í˜• ìƒë‹´ì„ ë°›ì•„ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.\"\n"
    "\n"
    "[Context]\n"
    "{context_str}\n"
    "\n"
    "ì‚¬ìš©ì ì§ˆë¬¸: {query_str}\n"
    "ì „ë¬¸ê°€ ë‹µë³€:"
)

async def init_rag_engine(use_elama_model: bool = False, use_local_vector_db: bool = True):
    """
    RAG Query Engine ì´ˆê¸°í™”
    
    Args:
        use_elama_model: Trueë©´ ì—˜ë¼ë§ˆ í´ë¼ìš°ë“œì˜ ëª¨ë¸ ì„¤ì • ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
                        Falseë©´ GeminiLLM ì‚¬ìš© (ê¸°ì¡´ ê²€ì§„ ì„¤ê³„ í”Œë¡œìš°ìš©)
        use_local_vector_db: Trueë©´ ë¡œì»¬ FAISS ë²¡í„° DB ì‚¬ìš© (ë¹„ìš© ì ˆê°, ë¹ ë¥¸ ì‘ë‹µ, ê¸°ë³¸ê°’)
                            Falseë©´ LlamaCloud API ì‚¬ìš©
    """
    global _rag_engine_cache, _rag_engine_test_cache, _rag_engine_local_cache
    
    # ìºì‹œ í™•ì¸
    if use_local_vector_db:
        if _rag_engine_local_cache is not None:
            print("[INFO] ë¡œì»¬ FAISS ì—”ì§„ ìºì‹œ ì‚¬ìš©")
            return _rag_engine_local_cache
    elif use_elama_model:
        if _rag_engine_test_cache is not None:
            return _rag_engine_test_cache
    else:
        if _rag_engine_cache is not None:
            return _rag_engine_cache
    
    if not LLAMAINDEX_AVAILABLE:
        print("[WARN] LlamaIndex ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # ===== ë¡œì»¬ FAISS ë²¡í„° DB ì‚¬ìš© =====
        if use_local_vector_db:
            if not FAISS_AVAILABLE:
                print("[WARN] FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                use_local_vector_db = False  # Fallback to cloud
            else:
                print("[INFO] ë¡œì»¬ FAISS ë²¡í„° DB ì´ˆê¸°í™” ì¤‘...")
                
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                if not PathLib(LOCAL_FAISS_INDEX_PATH).exists():
                    print(f"[WARN] FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LOCAL_FAISS_INDEX_PATH}")
                    print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                    use_local_vector_db = False
                else:
                    faiss_index = faiss.read_index(LOCAL_FAISS_INDEX_PATH)
                    print(f"[INFO] FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {faiss_index.ntotal}ê°œ ë²¡í„°")
                    
                    # ë©”íƒ€ë°ì´í„° ë¡œë“œ (ì„ íƒ ì‚¬í•­ìœ¼ë¡œ ë³€ê²½)
                    if PathLib(LOCAL_FAISS_METADATA_PATH).exists():
                        with open(LOCAL_FAISS_METADATA_PATH, 'rb') as f:
                            metadata = pickle.load(f)
                        total_docs = metadata.get('total_documents', metadata.get('total_nodes', faiss_index.ntotal))
                        print(f"[INFO] ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {total_docs}ê°œ ë¬¸ì„œ")
                    else:
                        print(f"[INFO] ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        total_docs = faiss_index.ntotal
                    
                    # OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ê²€ìƒ‰ìš©)
                        openai_api_key = os.environ.get("OPENAI_API_KEY") or settings.openai_api_key
                        if not openai_api_key or openai_api_key == "dev-openai-key":
                            print("[WARN] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                            use_local_vector_db = False
                        else:
                            embed_model = OpenAIEmbedding(
                                model="text-embedding-ada-002",
                                api_key=openai_api_key
                            )
                            Settings.embed_model = embed_model
                            
                            # Gemini LLM ì„¤ì • (ë‹µë³€ ìƒì„±ìš©)
                            gemini_api_key = os.environ.get("GOOGLE_GEMINI_API_KEY") or settings.google_gemini_api_key
                            if not gemini_api_key or gemini_api_key == "dev-gemini-key":
                                print("[WARN] Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                                use_local_vector_db = False
                            elif not GEMINI_AVAILABLE or not genai:
                                print("[WARN] Google Geminiê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                                print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                                use_local_vector_db = False
                            else:
                                llm = GeminiLLM(api_key=gemini_api_key, model="gemini-3-flash-preview")
                                Settings.llm = llm
                                
                                # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                                vector_store = FaissVectorStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                
                                # Storage Context ìƒì„± (docstoreì™€ index_storeë§Œ ë¡œë“œ)
                                print(f"[INFO] Storage Context ë¡œë“œ ì¤‘...")
                                from llama_index.core.storage.docstore import SimpleDocumentStore
                                from llama_index.core.storage.index_store import SimpleIndexStore
                                
                                docstore = SimpleDocumentStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                index_store = SimpleIndexStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                
                                storage_context = StorageContext.from_defaults(
                                    vector_store=vector_store,
                                    docstore=docstore,
                                    index_store=index_store
                                )
                                
                                # ì¸ë±ìŠ¤ ë¡œë“œ (index_id ëª…ì‹œ)
                                try:
                                    # ë¨¼ì € index_id ì—†ì´ ì‹œë„
                                    index = load_index_from_storage(storage_context)
                                except ValueError as e:
                                    if "Please specify index_id" in str(e):
                                        # index_storeì—ì„œ index_id ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                                        from llama_index.core.storage.index_store import SimpleIndexStore
                                        index_structs = storage_context.index_store.index_structs()
                                        
                                        if isinstance(index_structs, dict):
                                            index_ids = list(index_structs.keys())
                                        elif isinstance(index_structs, list):
                                            # listì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª©ì˜ index_id ì‚¬ìš©
                                            index_ids = [struct.index_id for struct in index_structs if hasattr(struct, 'index_id')]
                                        else:
                                            raise ValueError(f"Unexpected index_structs type: {type(index_structs)}")
                                        
                                        if index_ids:
                                            print(f"[INFO] ì—¬ëŸ¬ ì¸ë±ìŠ¤ ë°œê²¬, ì²« ë²ˆì§¸ ì‚¬ìš©: {index_ids[0]}")
                                            index = load_index_from_storage(storage_context, index_id=index_ids[0])
                                        else:
                                            raise ValueError("No index found in storage")
                                    else:
                                        raise
                                
                                # ì¿¼ë¦¬ ì—”ì§„ ìƒì„± (ì„±ëŠ¥ ìµœì í™” ë° ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì ìš©)
                                query_engine = index.as_query_engine(
                                    similarity_top_k=5,
                                    response_mode="compact",
                                    text_qa_template=PromptTemplate(CHAT_SYSTEM_PROMPT)
                                )
                                
                                _rag_engine_local_cache = query_engine
                                print(f"[INFO] âœ… ë¡œì»¬ FAISS RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (ë²¡í„°: {faiss_index.ntotal}ê°œ, ë¬¸ì„œ: {total_docs}ê°œ)")
                                
                                return query_engine
        
        # ===== LlamaCloud API ì‚¬ìš© (ê¸°ì¡´ ë¡œì§ ë˜ëŠ” Fallback) =====
        if not use_local_vector_db:
            llamaindex_api_key = os.environ.get("LLAMAINDEX_API_KEY") or settings.llamaindex_api_key
            
            if not llamaindex_api_key:
                print("[WARN] LlamaIndex API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            
            index = LlamaCloudIndex(
                index_id=LLAMACLOUD_INDEX_ID,
                api_key=llamaindex_api_key
            )
            
            # GeminiLLMì€ í•­ìƒ í•„ìš” (LlamaCloudIndexê°€ ê¸°ë³¸ì ìœ¼ë¡œ OpenAIë¥¼ ì°¾ìœ¼ë ¤ê³  í•¨)
            gemini_api_key = os.environ.get("GOOGLE_GEMINI_API_KEY") or settings.google_gemini_api_key
            
            if not gemini_api_key:
                print("[WARN] Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            
            if not GEMINI_AVAILABLE or not genai:
                return None
            
            llm = GeminiLLM(api_key=gemini_api_key, model="gemini-3-flash-preview")
            
            if use_elama_model:
                # í…ŒìŠ¤íŠ¸ìš©: GeminiLLM ì‚¬ìš©
                query_engine = index.as_query_engine(
                    llm=llm,
                    similarity_top_k=5,
                    response_mode="tree_summarize"
                )
                _rag_engine_test_cache = query_engine
                print(f"[INFO] RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (í…ŒìŠ¤íŠ¸ìš© - GeminiLLM, LlamaCloud Index ID: {LLAMACLOUD_INDEX_ID})")
            else:
                # ê¸°ì¡´ í”Œë¡œìš°: GeminiLLM ì‚¬ìš© + Settings.llm ì„¤ì •
                Settings.llm = llm
                
                query_engine = index.as_query_engine(
                    similarity_top_k=5,
                    response_mode="tree_summarize"
                )
                
                _rag_engine_cache = query_engine
                print(f"[INFO] RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (LlamaCloud Index ID: {LLAMACLOUD_INDEX_ID})")
            
            return query_engine
        
    except Exception as e:
        print(f"[ERROR] RAG ì—”ì§„ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def clean_html_content(text: str) -> str:
    """
    HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    return text

def extract_evidence_from_source_nodes(response) -> List[Dict[str, Any]]:
    """LlamaIndex ì‘ë‹µì—ì„œ ì†ŒìŠ¤ ë…¸ë“œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    evidences = []
    if hasattr(response, 'source_nodes'):
        for node in response.source_nodes:
            metadata = node.metadata if hasattr(node, 'metadata') else {}
            text = node.text if hasattr(node, 'text') else ""
            
            # HTML íƒœê·¸ ì •ì œ
            text = clean_html_content(text)
            
            score = node.score if hasattr(node, 'score') else 0.0
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            file_name = metadata.get('file_name', 'Unknown')
            page_label = metadata.get('page_label', 'Unknown')
            
            # ì¸ìš©êµ¬ ì¶”ì¶œ
            citation = text[:100] + "..." if len(text) > 100 else text
            
            evidences.append({
                "source_document": file_name,
                "page": page_label,
                "citation": citation,
                "full_text": text,
                "confidence_score": score,
                "organization": "ì˜í•™íšŒ",
                "year": "2024"
            })
    return evidences


async def get_medical_evidence_from_rag(query_engine, patient_context: Dict, concerns: List[Dict]) -> Dict[str, Any]:
    """
    í™˜ì ì •ë³´ì™€ ì—¼ë ¤ ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ RAG ê²€ìƒ‰ ìˆ˜í–‰
    """
    if not query_engine:
        return {"context_text": "", "structured_evidences": []}
    
    try:
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        query_parts = []
        
        # 1. ë‚˜ì´/ì„±ë³„ ê¸°ë°˜
        age = patient_context.get('age', 40)
        gender = patient_context.get('gender', 'unknown')
        query_parts.append(f"{age}ì„¸ {gender}ì—ê²Œ ê¶Œì¥ë˜ëŠ” í•„ìˆ˜ ê±´ê°•ê²€ì§„ í•­ëª©ê³¼ ì•” ì„ ë³„ê²€ì‚¬ ê¸°ì¤€")
        
        # 2. ì´ìƒ ì†Œê²¬ ê¸°ë°˜
        abnormal_items = patient_context.get('abnormal_items', [])
        for item in abnormal_items:
            name = item.get('name', '')
            status = item.get('status', '')
            if name:
                query_parts.append(f"{name} ìˆ˜ì¹˜ê°€ {status}ì¼ ë•Œ í•„ìš”í•œ ì •ë°€ ê²€ì‚¬ì™€ ì„ìƒì  ì˜ì˜")
        
        # 3. ê°€ì¡±ë ¥ ê¸°ë°˜
        family_history = patient_context.get('family_history', [])
        for fh in family_history:
            query_parts.append(f"{fh} ê°€ì¡±ë ¥ì´ ìˆì„ ë•Œ ê¶Œì¥ë˜ëŠ” ì¡°ê¸° ì„ ë³„ê²€ì‚¬")
            
        # 4. ì—¼ë ¤ í•­ëª© ê¸°ë°˜
        for concern in concerns:
            c_name = concern.get('name', '')
            if c_name:
                query_parts.append(f"{c_name} ê´€ë ¨ ìµœì‹  ì§„ë£Œì§€ì¹¨ê³¼ ê²€ì‚¬ ê¶Œê³ ì•ˆ")
        
        final_query = " \n".join(query_parts)
        print(f"[INFO] RAG ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±: {len(final_query)}ì")
        
        # ê²€ìƒ‰ ì‹¤í–‰
        response = await query_engine.aquery(final_query)
        
        # ê²°ê³¼ ì²˜ë¦¬
        context_text = str(response)
        structured_evidences = extract_evidence_from_source_nodes(response)
        
        # ê° ì—ë¹„ë˜ìŠ¤ì— ì¿¼ë¦¬ ë§¥ë½ ì¶”ê°€
        for ev in structured_evidences:
            ev['query'] = final_query
            ev['category'] = 'Clinical Guideline'
            
        return {
            "context_text": context_text,
            "structured_evidences": structured_evidences
        }
        
    except Exception as e:
        print(f"[ERROR] RAG ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        return {"context_text": "", "structured_evidences": []}


async def search_checkup_knowledge(query: str, use_local_vector_db: bool = True) -> Dict[str, Any]:
    """
    ê²€ì§„ ì§€ì‹ ê²€ìƒ‰ (RAG)
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        use_local_vector_db: Trueë©´ ë¡œì»¬ FAISS ì‚¬ìš©, Falseë©´ LlamaCloud ì‚¬ìš©
    """
    query_engine = await init_rag_engine(use_local_vector_db=use_local_vector_db)
    
    if not query_engine:
        return {
            "success": False,
            "error": "RAG ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨",
            "answer": None,
            "sources": []
        }
    
    try:
        # ë¹„ë™ê¸° ê²€ìƒ‰ìœ¼ë¡œ ë³€ê²½
        response = await query_engine.aquery(query)
        
        # ì†ŒìŠ¤ ì¶”ì¶œ
        sources = []
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                source_info = {
                    "text": clean_html_content(node.text)[:500],
                    "score": float(node.score) if hasattr(node, 'score') else None,
                    "metadata": node.metadata if hasattr(node, 'metadata') else {}
                }
                sources.append(source_info)
        
        return {
            "success": True,
            "answer": str(response),
            "sources": sources,
            "query": query
        }
    
    except Exception as e:
        print(f"[ERROR] RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "answer": None,
            "sources": []
        }
