"""
RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤ ëª¨ë“ˆ
LlamaIndex ë° Gemini LLM ê¸°ë°˜ ì˜ë£Œ ì§€ì‹ ê²€ìƒ‰ ì—”ì§„

**ë¡œì»¬ FAISS ë²¡í„° DB ì§€ì› ì¶”ê°€**
- LlamaCloud API (ê¸°ë³¸ê°’)
- ë¡œì»¬ FAISS ë²¡í„° DB (ë¹„ìš© ì ˆê°, ë¹ ë¥¸ ì‘ë‹µ)
"""
import os
import json
import re
from typing import List, Dict, Any, Optional
from app.core.config import settings

# LlamaIndex RAG ê´€ë ¨ ì„í¬íŠ¸
try:
    from llama_index.core import Settings, VectorStoreIndex, StorageContext, load_index_from_storage
    from llama_index.core.llms import CustomLLM
    from llama_index.core.llms.llm import LLM
    from llama_index.core.llms import ChatMessage, MessageRole, CompletionResponse, LLMMetadata
    from llama_index.indices.managed.llama_cloud import LlamaCloudIndex
    from llama_index.llms.openai import OpenAI
    from llama_index.embeddings.openai import OpenAIEmbedding
    # FAISS ë²¡í„° ìŠ¤í† ì–´ ì„í¬íŠ¸
    try:
        import faiss
        import pickle
        from pathlib import Path as PathLib
        from llama_index.vector_stores.faiss import FaissVectorStore
        FAISS_AVAILABLE = True
    except ImportError:
        FAISS_AVAILABLE = False
    # Google GeminiëŠ” google-generativeai ì§ì ‘ ì‚¬ìš©í•˜ì—¬ CustomLLMìœ¼ë¡œ ë˜í•‘
    try:
        import google.generativeai as genai
        GEMINI_AVAILABLE = True
    except ImportError:
        GEMINI_AVAILABLE = False
        genai = None
    LLAMAINDEX_AVAILABLE = True
except ImportError as e:
    LLAMAINDEX_AVAILABLE = False
    GEMINI_AVAILABLE = False
    FAISS_AVAILABLE = False
    genai = None
    # ê°œë°œ í™˜ê²½ì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë”ë¯¸ í´ë˜ìŠ¤
    class LlamaCloudIndex:
        pass
    class OpenAI:
        pass
    class CustomLLM:
        pass
    class ChatMessage:
        pass
    class MessageRole:
        pass
    class CompletionResponse:
        pass
    class LLMMetadata:
        pass

# ìƒìˆ˜ ì •ì˜
LLAMACLOUD_INDEX_NAME = "Dr.Welno"
LLAMACLOUD_PROJECT_NAME = "Default"
LLAMACLOUD_INDEX_ID = "1bcef115-bb95-4d14-9c29-d38bb097a39c"
LLAMACLOUD_PROJECT_ID = "45c4d9d4-ce6b-4f62-ad88-9107fe6de8cc"
LLAMACLOUD_ORGANIZATION_ID = "e4024539-3d26-48b5-8051-9092380c84d2"

# ë¡œì»¬ FAISS ë²¡í„° DB ê²½ë¡œ
LOCAL_FAISS_DIR = "/data/vector_db/welno/faiss_db"
LOCAL_FAISS_INDEX_PATH = f"{LOCAL_FAISS_DIR}/faiss.index"
LOCAL_FAISS_METADATA_PATH = f"{LOCAL_FAISS_DIR}/metadata.pkl"

# ì „ì—­ RAG ì—”ì§„ ìºì‹œ (ê¸°ì¡´ í”Œë¡œìš°ìš©)
_rag_engine_cache: Optional[Any] = None
# í…ŒìŠ¤íŠ¸ìš© RAG ì—”ì§„ ìºì‹œ (ì—˜ë¼ë§ˆ í´ë¼ìš°ë“œ ìŠ¤íƒ€ì¼)
_rag_engine_test_cache: Optional[Any] = None
# ë¡œì»¬ FAISS ì—”ì§„ ìºì‹œ
_rag_engine_local_cache: Optional[Any] = None

# Gemini CustomLLM í´ë˜ìŠ¤
class GeminiLLM(CustomLLM):
    """Google Geminië¥¼ LlamaIndex CustomLLMìœ¼ë¡œ êµ¬í˜„ (RAG ê²€ìƒ‰ìš©)"""
    
    def __init__(self, api_key: str, model: str = "gemini-3-flash-preview", **kwargs):
        if not GEMINI_AVAILABLE or not genai:
            raise ImportError("google-generativeaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        super().__init__(**kwargs)
        genai.configure(api_key=api_key)
        self._model = genai.GenerativeModel(model)
        self._model_name = model
    
    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=8192,
            num_output=2048,
            is_chat_model=True,
            model_name=self._model_name
        )
    
    def complete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:
        try:
            response = self._model.generate_content(prompt)
            text = response.text if hasattr(response, 'text') else str(response)
            return CompletionResponse(text=text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def acomplete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:
        try:
            response = self._model.generate_content(prompt)
            text = response.text if hasattr(response, 'text') else str(response)
            return CompletionResponse(text=text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    def stream_complete(self, prompt: str, formatted: bool = False, **kwargs):
        try:
            response = self._model.generate_content(prompt, stream=True)
            for chunk in response:
                if hasattr(chunk, 'text') and chunk.text:
                    yield CompletionResponse(text=chunk.text, delta=chunk.text)
        except Exception as e:
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
from llama_index.core import PromptTemplate

CHAT_SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ì¹œì ˆí•œ ê±´ê°• ìƒë‹´ê°€ 'Dr. Welno'ì…ë‹ˆë‹¤.\n"
    "ë‹¤ìŒ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì§€ì¼œ ë‹µë³€í•˜ì„¸ìš”.\n"
    "\n"
    "âš ï¸ **í™˜ê° ë°©ì§€ ê·œì¹™ (ìµœìš°ì„  - ì ˆëŒ€ ìœ„ë°° ê¸ˆì§€)**:\n"
    "1. **ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€**: ì œê³µëœ [Context]ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "2. **ì‚¬ìš©ì ì–¸ê¸‰ ì¦ìƒë§Œ ì‚¬ìš©**: ì‚¬ìš©ìê°€ ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰í•œ ì¦ìƒì´ë‚˜ ë¬¸ì œë§Œ ë‹¤ë£¨ì„¸ìš”. ì‚¬ìš©ìê°€ ë§í•˜ì§€ ì•Šì€ ì¦ìƒ(ì˜ˆ: ë¹„ì—¼, ì½” ì ë§‰ ë¶€ìŒ, ëˆˆ ì¶©í˜ˆ, ê·€ ê°€ë ¤ì›€, ì´ëª… ë“±)ì€ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "3. **í™˜ì ë°ì´í„° í™•ì¸**: íŠ¹ì • í™˜ì(ì˜ˆ: 'ì•ˆê´‘ìˆ˜')ì˜ ë°ì´í„°ë¥¼ ìš”ì²­ë°›ì•˜ì„ ë•Œ, [Context]ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ \"í•´ë‹¹ í™˜ìì˜ ë°ì´í„°ê°€ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ëª…í™•íˆ ë‹µë³€í•˜ì„¸ìš”.\n"
    "4. **ê²€ì‚¬/ê±´ê¸°ì‹ ì¶”ì²œ**: ê²€ì‚¬ í•­ëª©ì´ë‚˜ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì¶”ì²œ ì‹œ, ë°˜ë“œì‹œ PNT ë§¤íŠ¸ë¦­ìŠ¤ ë°ì´í„°ë¥¼ ìš°ì„  ì°¸ì¡°í•˜ê³ , ì—†ëŠ” í•­ëª©ì€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.\n"
    "5. **ì¶œì²˜ ëª…í™•í™”**: ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , \"ì œê³µëœ ì •ë³´ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n"
    "6. **ì¦ìƒ ì¶”ì¸¡ ê¸ˆì§€**: ì‚¬ìš©ìê°€ 'ë¨¸ë¦¬ê°€ ì•„í”„ë‹¤'ê³ ë§Œ ë§í–ˆë‹¤ë©´, ë¹„ì—¼, ì½” ì ë§‰ ë¶€ìŒ, ëˆˆ ì¶©í˜ˆ, ê·€ ê°€ë ¤ì›€, ì´ëª… ë“± ë‹¤ë¥¸ ì¦ìƒì„ ì¶”ì¸¡í•˜ì—¬ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ ì‚¬ìš©ìê°€ ì§ì ‘ ì–¸ê¸‰í•œ ì¦ìƒë§Œ ë‹¤ë£¨ì„¸ìš”.\n"
    "\n"
    "ğŸ’¡ **ë‹µë³€ ìŠ¤íƒ€ì¼**:\n"
    "1. **ë§ì¶¤í˜• ë¸Œë¦¬í•‘**: ë§Œì•½ [Context]ì— 'í™˜ì ìµœê·¼ ê±´ê°• ìƒíƒœ' ì •ë³´ê°€ ìˆë‹¤ë©´, ì²« ì¸ì‚¬ì™€ í•¨ê»˜ í•´ë‹¹ ì •ë³´ë¥¼ ì•„ì£¼ ì§§ê²Œ ì–¸ê¸‰í•˜ë©° ìƒë‹´ì„ ì‹œì‘í•˜ì„¸ìš”.\n"
    "   - **ì£¼ì˜**: ì‹¤ì œë¡œ [Context]ì— ìˆëŠ” ì •ë³´(ì˜ˆ: BMI, í˜ˆì••, ì§ˆí™˜ ì˜ì‹¬ ì†Œê²¬)ë§Œ ì–¸ê¸‰í•˜ì„¸ìš”. 'ë¬¸ì§„í‘œë¥¼ í™•ì¸í–ˆë‹¤'ê±°ë‚˜ [Context]ì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì–´ ë§í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "2. **ì§ì„¤ì  ë‹µë³€**: 'ì§€ì¹¨ì„œì— ë”°ë¥´ë©´', 'ì œê³µëœ ì •ë³´ì— ì˜í•˜ë©´'ê³¼ ê°™ì€ ë¶€ì—° ì„¤ëª…ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”. ì „ë¬¸ê°€ë¡œì„œ ìì—°ìŠ¤ëŸ½ê³  ë‹¨í˜¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
    "3. **ì§ˆë¬¸ ì§‘ì¤‘**: ë¸Œë¦¬í•‘ ì´í›„ì—ëŠ” ì‚¬ìš©ìê°€ í˜„ì¬ ë¬»ê³  ìˆëŠ” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì— ì§‘ì¤‘í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
    "4. **ê°„ê²°ì„±**: ë‹µë³€ì€ í•µì‹¬ ìœ„ì£¼ë¡œ ìš”ì•½í•˜ê³ , êµ¬ì²´ì ì¸ ì‹¤ì²œ ë°©ë²•ì€ ë¶ˆë › í¬ì¸íŠ¸ë¡œ ì •ë¦¬í•˜ì„¸ìš”.\n"
    "5. **ì—ë¹„ë˜ìŠ¤ ì œì™¸**: ê·¼ê±° ë¬¸ì„œë‚˜ ì¶œì²˜ì— ëŒ€í•œ ì–¸ê¸‰ì€ ë‹µë³€ í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "\n"
    "[Context]\n"
    "{context_str}\n"
    "\n"
    "ì‚¬ìš©ì ì§ˆë¬¸: {query_str}\n"
    "ì „ë¬¸ê°€ ë‹µë³€:"
)

async def init_rag_engine(use_elama_model: bool = False, use_local_vector_db: bool = True):
    """
    RAG Query Engine ì´ˆê¸°í™”
    
    Args:
        use_elama_model: Trueë©´ ì—˜ë¼ë§ˆ í´ë¼ìš°ë“œì˜ ëª¨ë¸ ì„¤ì • ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
                        Falseë©´ GeminiLLM ì‚¬ìš© (ê¸°ì¡´ ê²€ì§„ ì„¤ê³„ í”Œë¡œìš°ìš©)
        use_local_vector_db: Trueë©´ ë¡œì»¬ FAISS ë²¡í„° DB ì‚¬ìš© (ë¹„ìš© ì ˆê°, ë¹ ë¥¸ ì‘ë‹µ, ê¸°ë³¸ê°’)
                            Falseë©´ LlamaCloud API ì‚¬ìš©
    """
    global _rag_engine_cache, _rag_engine_test_cache, _rag_engine_local_cache
    
    # ìºì‹œ í™•ì¸
    if use_local_vector_db:
        if _rag_engine_local_cache is not None:
            print("[INFO] ë¡œì»¬ FAISS ì—”ì§„ ìºì‹œ ì‚¬ìš©")
            return _rag_engine_local_cache
    elif use_elama_model:
        if _rag_engine_test_cache is not None:
            return _rag_engine_test_cache
    else:
        if _rag_engine_cache is not None:
            return _rag_engine_cache
    
    if not LLAMAINDEX_AVAILABLE:
        print("[WARN] LlamaIndex ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # ===== ë¡œì»¬ FAISS ë²¡í„° DB ì‚¬ìš© =====
        if use_local_vector_db:
            if not FAISS_AVAILABLE:
                print("[WARN] FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                use_local_vector_db = False  # Fallback to cloud
            else:
                print("[INFO] ë¡œì»¬ FAISS ë²¡í„° DB ì´ˆê¸°í™” ì¤‘...")
                
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                if not PathLib(LOCAL_FAISS_INDEX_PATH).exists():
                    print(f"[WARN] FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LOCAL_FAISS_INDEX_PATH}")
                    print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                    use_local_vector_db = False
                else:
                    faiss_index = faiss.read_index(LOCAL_FAISS_INDEX_PATH)
                    print(f"[INFO] FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {faiss_index.ntotal}ê°œ ë²¡í„°")
                    
                    # ë©”íƒ€ë°ì´í„° ë¡œë“œ (ì„ íƒ ì‚¬í•­ìœ¼ë¡œ ë³€ê²½)
                    if PathLib(LOCAL_FAISS_METADATA_PATH).exists():
                        with open(LOCAL_FAISS_METADATA_PATH, 'rb') as f:
                            metadata = pickle.load(f)
                        total_docs = metadata.get('total_documents', metadata.get('total_nodes', faiss_index.ntotal))
                        print(f"[INFO] ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {total_docs}ê°œ ë¬¸ì„œ")
                    else:
                        print(f"[INFO] ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        total_docs = faiss_index.ntotal
                    
                    # OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ê²€ìƒ‰ìš©)
                        openai_api_key = os.environ.get("OPENAI_API_KEY") or settings.openai_api_key
                        if not openai_api_key or openai_api_key == "dev-openai-key":
                            print("[WARN] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                            use_local_vector_db = False
                        else:
                            embed_model = OpenAIEmbedding(
                                model="text-embedding-ada-002",
                                api_key=openai_api_key
                            )
                            Settings.embed_model = embed_model
                            
                            # Gemini LLM ì„¤ì • (ë‹µë³€ ìƒì„±ìš©)
                            gemini_api_key = os.environ.get("GOOGLE_GEMINI_API_KEY") or settings.google_gemini_api_key
                            if not gemini_api_key or gemini_api_key == "dev-gemini-key":
                                print("[WARN] Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                                use_local_vector_db = False
                            elif not GEMINI_AVAILABLE or not genai:
                                print("[WARN] Google Geminiê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                                print("[INFO] LlamaCloudë¡œ fallbackí•©ë‹ˆë‹¤.")
                                use_local_vector_db = False
                            else:
                                llm = GeminiLLM(api_key=gemini_api_key, model="gemini-3-flash-preview")
                                Settings.llm = llm
                                
                                # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                                vector_store = FaissVectorStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                
                                # Storage Context ìƒì„± (docstoreì™€ index_storeë§Œ ë¡œë“œ)
                                print(f"[INFO] Storage Context ë¡œë“œ ì¤‘...")
                                from llama_index.core.storage.docstore import SimpleDocumentStore
                                from llama_index.core.storage.index_store import SimpleIndexStore
                                
                                docstore = SimpleDocumentStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                index_store = SimpleIndexStore.from_persist_dir(str(PathLib(LOCAL_FAISS_DIR)))
                                
                                storage_context = StorageContext.from_defaults(
                                    vector_store=vector_store,
                                    docstore=docstore,
                                    index_store=index_store
                                )
                                
                                # ì¸ë±ìŠ¤ ë¡œë“œ (index_id ëª…ì‹œ)
                                try:
                                    # ë¨¼ì € index_id ì—†ì´ ì‹œë„
                                    index = load_index_from_storage(storage_context)
                                except ValueError as e:
                                    if "Please specify index_id" in str(e):
                                        # index_storeì—ì„œ index_id ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                                        from llama_index.core.storage.index_store import SimpleIndexStore
                                        index_structs = storage_context.index_store.index_structs()
                                        
                                        if isinstance(index_structs, dict):
                                            index_ids = list(index_structs.keys())
                                        elif isinstance(index_structs, list):
                                            # listì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª©ì˜ index_id ì‚¬ìš©
                                            index_ids = [struct.index_id for struct in index_structs if hasattr(struct, 'index_id')]
                                        else:
                                            raise ValueError(f"Unexpected index_structs type: {type(index_structs)}")
                                        
                                        if index_ids:
                                            print(f"[INFO] ì—¬ëŸ¬ ì¸ë±ìŠ¤ ë°œê²¬, ì²« ë²ˆì§¸ ì‚¬ìš©: {index_ids[0]}")
                                            index = load_index_from_storage(storage_context, index_id=index_ids[0])
                                        else:
                                            raise ValueError("No index found in storage")
                                    else:
                                        raise
                                
                                # ì¿¼ë¦¬ ì—”ì§„ ìƒì„± (ì„±ëŠ¥ ìµœì í™” ë° ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì ìš©)
                                query_engine = index.as_query_engine(
                                    similarity_top_k=5,
                                    response_mode="compact",
                                    text_qa_template=PromptTemplate(CHAT_SYSTEM_PROMPT)
                                )
                                
                                _rag_engine_local_cache = query_engine
                                print(f"[INFO] âœ… ë¡œì»¬ FAISS RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (ë²¡í„°: {faiss_index.ntotal}ê°œ, ë¬¸ì„œ: {total_docs}ê°œ)")
                                
                                return query_engine
        
        # ===== LlamaCloud API ì‚¬ìš© (ê¸°ì¡´ ë¡œì§ ë˜ëŠ” Fallback) =====
        if not use_local_vector_db:
            llamaindex_api_key = os.environ.get("LLAMAINDEX_API_KEY") or settings.llamaindex_api_key
            
            if not llamaindex_api_key:
                print("[WARN] LlamaIndex API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            
            index = LlamaCloudIndex(
                index_id=LLAMACLOUD_INDEX_ID,
                api_key=llamaindex_api_key
            )
            
            # GeminiLLMì€ í•­ìƒ í•„ìš” (LlamaCloudIndexê°€ ê¸°ë³¸ì ìœ¼ë¡œ OpenAIë¥¼ ì°¾ìœ¼ë ¤ê³  í•¨)
            gemini_api_key = os.environ.get("GOOGLE_GEMINI_API_KEY") or settings.google_gemini_api_key
            
            if not gemini_api_key:
                print("[WARN] Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            
            if not GEMINI_AVAILABLE or not genai:
                return None
            
            llm = GeminiLLM(api_key=gemini_api_key, model="gemini-3-flash-preview")
            
            if use_elama_model:
                # í…ŒìŠ¤íŠ¸ìš©: GeminiLLM ì‚¬ìš©
                query_engine = index.as_query_engine(
                    llm=llm,
                    similarity_top_k=5,
                    response_mode="tree_summarize"
                )
                _rag_engine_test_cache = query_engine
                print(f"[INFO] RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (í…ŒìŠ¤íŠ¸ìš© - GeminiLLM, LlamaCloud Index ID: {LLAMACLOUD_INDEX_ID})")
            else:
                # ê¸°ì¡´ í”Œë¡œìš°: GeminiLLM ì‚¬ìš© + Settings.llm ì„¤ì •
                Settings.llm = llm
                
                query_engine = index.as_query_engine(
                    similarity_top_k=5,
                    response_mode="tree_summarize"
                )
                
                _rag_engine_cache = query_engine
                print(f"[INFO] RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (LlamaCloud Index ID: {LLAMACLOUD_INDEX_ID})")
            
            return query_engine
        
    except Exception as e:
        print(f"[ERROR] RAG ì—”ì§„ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def clean_html_content(text: str) -> str:
    """
    HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    return text

def extract_evidence_from_source_nodes(response) -> List[Dict[str, Any]]:
    """LlamaIndex ì‘ë‹µì—ì„œ ì†ŒìŠ¤ ë…¸ë“œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    evidences = []
    if hasattr(response, 'source_nodes'):
        for node in response.source_nodes:
            metadata = node.metadata if hasattr(node, 'metadata') else {}
            text = node.text if hasattr(node, 'text') else ""
            
            # HTML íƒœê·¸ ì •ì œ
            text = clean_html_content(text)
            
            score = node.score if hasattr(node, 'score') else 0.0
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            file_name = metadata.get('file_name', 'Unknown')
            page_label = metadata.get('page_label', 'Unknown')
            
            # ì¸ìš©êµ¬ ì¶”ì¶œ
            citation = text[:100] + "..." if len(text) > 100 else text
            
            evidences.append({
                "source_document": file_name,
                "page": page_label,
                "citation": citation,
                "full_text": text,
                "confidence_score": score,
                "organization": "ì˜í•™íšŒ",
                "year": "2024"
            })
    return evidences


async def get_medical_evidence_from_rag(query_engine, patient_context: Dict, concerns: List[Dict]) -> Dict[str, Any]:
    """
    í™˜ì ì •ë³´ì™€ ì—¼ë ¤ ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ RAG ê²€ìƒ‰ ìˆ˜í–‰
    """
    if not query_engine:
        return {"context_text": "", "structured_evidences": []}
    
    try:
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        query_parts = []
        
        # 1. ë‚˜ì´/ì„±ë³„ ê¸°ë°˜
        age = patient_context.get('age', 40)
        gender = patient_context.get('gender', 'unknown')
        query_parts.append(f"{age}ì„¸ {gender}ì—ê²Œ ê¶Œì¥ë˜ëŠ” í•„ìˆ˜ ê±´ê°•ê²€ì§„ í•­ëª©ê³¼ ì•” ì„ ë³„ê²€ì‚¬ ê¸°ì¤€")
        
        # 2. ì´ìƒ ì†Œê²¬ ê¸°ë°˜
        abnormal_items = patient_context.get('abnormal_items', [])
        for item in abnormal_items:
            name = item.get('name', '')
            status = item.get('status', '')
            if name:
                query_parts.append(f"{name} ìˆ˜ì¹˜ê°€ {status}ì¼ ë•Œ í•„ìš”í•œ ì •ë°€ ê²€ì‚¬ì™€ ì„ìƒì  ì˜ì˜")
        
        # 3. ê°€ì¡±ë ¥ ê¸°ë°˜
        family_history = patient_context.get('family_history', [])
        for fh in family_history:
            query_parts.append(f"{fh} ê°€ì¡±ë ¥ì´ ìˆì„ ë•Œ ê¶Œì¥ë˜ëŠ” ì¡°ê¸° ì„ ë³„ê²€ì‚¬")
            
        # 4. ì—¼ë ¤ í•­ëª© ê¸°ë°˜
        for concern in concerns:
            c_name = concern.get('name', '')
            if c_name:
                query_parts.append(f"{c_name} ê´€ë ¨ ìµœì‹  ì§„ë£Œì§€ì¹¨ê³¼ ê²€ì‚¬ ê¶Œê³ ì•ˆ")
        
        final_query = " \n".join(query_parts)
        print(f"[INFO] RAG ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±: {len(final_query)}ì")
        
        # ê²€ìƒ‰ ì‹¤í–‰
        response = await query_engine.aquery(final_query)
        
        # ê²°ê³¼ ì²˜ë¦¬
        context_text = str(response)
        structured_evidences = extract_evidence_from_source_nodes(response)
        
        # ê° ì—ë¹„ë˜ìŠ¤ì— ì¿¼ë¦¬ ë§¥ë½ ì¶”ê°€
        for ev in structured_evidences:
            ev['query'] = final_query
            ev['category'] = 'Clinical Guideline'
            
        return {
            "context_text": context_text,
            "structured_evidences": structured_evidences
        }
        
    except Exception as e:
        print(f"[ERROR] RAG ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        return {"context_text": "", "structured_evidences": []}


async def search_checkup_knowledge(query: str, use_local_vector_db: bool = True) -> Dict[str, Any]:
    """
    ê²€ì§„ ì§€ì‹ ê²€ìƒ‰ (RAG)
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        use_local_vector_db: Trueë©´ ë¡œì»¬ FAISS ì‚¬ìš©, Falseë©´ LlamaCloud ì‚¬ìš©
    """
    query_engine = await init_rag_engine(use_local_vector_db=use_local_vector_db)
    
    if not query_engine:
        return {
            "success": False,
            "error": "RAG ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨",
            "answer": None,
            "sources": []
        }
    
    try:
        # ë¹„ë™ê¸° ê²€ìƒ‰ìœ¼ë¡œ ë³€ê²½
        response = await query_engine.aquery(query)
        
        # ì†ŒìŠ¤ ì¶”ì¶œ
        sources = []
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                source_info = {
                    "text": clean_html_content(node.text)[:500],
                    "score": float(node.score) if hasattr(node, 'score') else None,
                    "metadata": node.metadata if hasattr(node, 'metadata') else {}
                }
                sources.append(source_info)
        
        return {
            "success": True,
            "answer": str(response),
            "sources": sources,
            "query": query
        }
    
    except Exception as e:
        print(f"[ERROR] RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "answer": None,
            "sources": []
        }
