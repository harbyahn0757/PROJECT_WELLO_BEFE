import os
import json
import logging
import asyncio
from typing import Optional, List, Dict, Any, Union
from dataclasses import dataclass
from datetime import datetime, timedelta

import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from google.generativeai import caching

from app.core.config import settings

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

@dataclass
class GeminiRequest:
    """Gemini API ìš”ì²­ ë°ì´í„° í´ë˜ìŠ¤"""
    prompt: str
    model: str = "gemini-1.5-pro"  # ê¸°ë³¸ ëª¨ë¸
    temperature: float = 0.3
    max_tokens: int = 4096
    response_format: Optional[Dict[str, Any]] = None  # JSON ì‘ë‹µ ìš”ì²­ ì‹œ {"type": "json_object"}
    chat_history: Optional[List[Dict[str, str]]] = None  # ì„¸ì…˜ íˆìŠ¤í† ë¦¬ (role, content)
    system_instruction: Optional[str] = None  # Context Cachingìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (optional)

@dataclass
class GeminiResponse:
    """Gemini API ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    content: Optional[str] = None
    success: bool = False
    error: Optional[str] = None
    usage: Optional[Dict[str, int]] = None

class GeminiService:
    """Google Gemini ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self._api_key: Optional[str] = None
        self._initialized: bool = False
        self._chat_sessions: Dict[str, Any] = {}  # ì„¸ì…˜ë³„ ChatSession ì €ì¥
        self._content_caches: Dict[str, Any] = {}  # ì„¸ì…˜ë³„ CachedContent ì €ì¥
        self._cache_enabled: bool = True  # Context Caching í™œì„±í™” ì—¬ë¶€
        
    async def initialize(self):
        """Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if self._initialized:
            return

        self._api_key = settings.google_gemini_api_key
        
        if self._api_key and self._api_key != "dev-gemini-key":
            genai.configure(api_key=self._api_key)
            self._initialized = True
            logger.info("âœ… [Gemini Service] ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ [Gemini Service] API í‚¤ ì—†ìŒ ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•ŠìŒ")
            self._initialized = False
        
    async def call_api(
        self,
        request: GeminiRequest,
        save_log: bool = True,
        patient_uuid: Optional[str] = None,
        session_id: Optional[str] = None,
        step_number: Optional[str] = None,
        step_name: Optional[str] = None
    ) -> GeminiResponse:
        """Gemini API í˜¸ì¶œ"""
        
        if not self._initialized:
            await self.initialize()
            
        if not self._initialized:
            return GeminiResponse(success=False, error="Gemini ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        try:
            # ëª¨ë¸ ì„¤ì •
            generation_config = {
                "temperature": request.temperature,
                "max_output_tokens": request.max_tokens,
            }
            
            # JSON ì‘ë‹µì„ ê°•ì œí•˜ë ¤ë©´ í”„ë¡¬í”„íŠ¸ì— ì§€ì‹œí•˜ê±°ë‚˜ response_mime_type ì„¤ì • (1.5 Proë¶€í„° ì§€ì›)
            if request.response_format and request.response_format.get("type") == "json_object":
                generation_config["response_mime_type"] = "application/json"

            model = genai.GenerativeModel(
                model_name=request.model,
                generation_config=generation_config
            )

            # ì•ˆì „ ì„¤ì • (ì°¨ë‹¨ ìµœì†Œí™”)
            safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }

            # ë¹„ë™ê¸° í˜¸ì¶œ (asyncio.to_threadë¡œ ë˜í•‘, genai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ê¸°ë³¸ì ìœ¼ë¡œ ë™ê¸°ì‹ì´ë¯€ë¡œ)
            logger.info(f"ğŸ“¡ [Gemini Service] API í˜¸ì¶œ ì¤‘... (Model: {request.model})")
            
            response = await asyncio.to_thread(
                model.generate_content,
                request.prompt,
                safety_settings=safety_settings
            )
            
            response_text = response.text
            
            # ë¡œê¹… ì €ì¥
            if save_log and patient_uuid:
                from app.services.session_logger import get_session_logger
                session_logger = get_session_logger()
                
                # ë¡œê·¸ì— ì €ì¥í•  ìš”ì²­ ë°ì´í„° êµ¬ì„±
                log_request_data = {
                    "model": request.model,
                    "prompt": request.prompt,
                    "temperature": request.temperature
                }
                
                # ë¡œê·¸ì— ì €ì¥í•  ì‘ë‹µ ë°ì´í„° êµ¬ì„±
                log_response_data = {
                    "content": response_text,
                    "usage": {
                         # GeminiëŠ” ì •í™•í•œ í† í° ì‚¬ìš©ëŸ‰ì„ ì œê³µí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ (ë©”íƒ€ë°ì´í„° í™•ì¸ í•„ìš”)
                        "prompt_tokens": 0, 
                        "completion_tokens": 0,
                        "total_tokens": 0
                    }
                }
                if response.usage_metadata:
                     log_response_data["usage"] = {
                        "prompt_tokens": response.usage_metadata.prompt_token_count,
                        "completion_tokens": response.usage_metadata.candidates_token_count,
                        "total_tokens": response.usage_metadata.total_token_count
                     }

                session_logger.log_step(
                    patient_uuid=patient_uuid,
                    step_number=step_number or "unknown",
                    step_name=step_name or "Gemini Analysis",
                    request_data=log_request_data,
                    response_data=log_response_data,
                    session_id=session_id
                )

            return GeminiResponse(
                content=response_text,
                success=True,
                usage={
                    "total_tokens": response.usage_metadata.total_token_count if response.usage_metadata else 0
                }
            )

        except Exception as e:
            logger.error(f"âŒ [Gemini Service] API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            return GeminiResponse(success=False, error=str(e))

    async def stream_api(self, request: GeminiRequest, session_id: Optional[str] = None):
        """
        Gemini API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        
        ê¸°ëŠ¥:
        - ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì§€ì› (ë©€í‹°í„´ ëŒ€í™”)
        - Context Caching ìë™ í™œì„±í™” (ì¡°ê±´ ì¶©ì¡± ì‹œ)
        - Graceful degradation (ìºì‹± ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëª¨ë“œ)
        """
        if not self._initialized:
            await self.initialize()
            
        if not self._initialized:
            yield "Gemini ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return

        try:
            generation_config = {
                "temperature": request.temperature,
                "max_output_tokens": request.max_tokens,
            }
            
            # Context Caching ì‹œë„ (ì²« ë©”ì‹œì§€ + system_instruction ìˆì„ ë•Œë§Œ)
            cached_content = None
            is_first_message = not (request.chat_history and len(request.chat_history) > 0)
            
            if request.system_instruction and session_id and is_first_message:
                cached_content = await self._get_or_create_cache(
                    system_prompt=request.system_instruction,
                    model_name=request.model,
                    cache_key=session_id
                )
            
            # ëª¨ë¸ ìƒì„± (ìºì‹œ ì‚¬ìš© or ì¼ë°˜)
            if cached_content:
                model = genai.GenerativeModel.from_cached_content(
                    cached_content=cached_content,
                    generation_config=generation_config
                )
                cache_status = "cached"
            else:
                model = genai.GenerativeModel(
                    model_name=request.model,
                    generation_config=generation_config
                )
                cache_status = "normal"

            # ì•ˆì „ ì„¤ì • (ì˜ë£Œ ì½˜í…ì¸ ë¥¼ ìœ„í•´ ì°¨ë‹¨ ìµœì†Œí™”)
            safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }

            logger.info(f"ğŸ“¡ [Gemini] {request.model} í˜¸ì¶œ (session: {session_id[:8] if session_id else 'None'}..., mode: {cache_status})")
            
            # íˆìŠ¤í† ë¦¬ ìˆìœ¼ë©´ Chat ëª¨ë“œ, ì—†ìœ¼ë©´ ë‹¨ì¼ ìƒì„±
            if is_first_message:
                response = model.generate_content(
                    request.prompt,
                    safety_settings=safety_settings,
                    stream=True
                )
            else:
                chat_session = model.start_chat(history=request.chat_history)
                response = chat_session.send_message(
                    request.prompt,
                    safety_settings=safety_settings,
                    stream=True
                )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            for chunk in response:
                if chunk.text:
                    yield chunk.text

        except Exception as e:
            logger.error(f"âŒ [Gemini] í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            yield f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def _format_chat_history(self, history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ Gemini Chat í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        formatted = []
        for msg in history:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if not content:
                continue
            
            # Gemini Chat í˜•ì‹: "user" ë˜ëŠ” "model"
            if role == "assistant":
                role = "model"
            elif role != "user":
                continue  # userì™€ assistantë§Œ ì§€ì›
            
            formatted.append({
                "role": role,
                "parts": [content]  # Gemini Chat API í˜•ì‹: partsëŠ” ë¦¬ìŠ¤íŠ¸
            })
        
        return formatted
    
    async def _get_or_create_cache(
        self, 
        system_prompt: str, 
        model_name: str,
        cache_key: Optional[str] = None
    ) -> Optional[Any]:
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìºì‹±í•˜ì—¬ ì¬ì‚¬ìš© (Graceful degradation)
        
        ìºì‹± ì¡°ê±´:
        - ìµœì†Œ 1,024 í† í° ì´ìƒ (Gemini 3 Flash ìš”êµ¬ì‚¬í•­)
        - ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ non-cached ë°©ì‹ìœ¼ë¡œ fallback
        - ê¸°ì¡´ ê¸°ëŠ¥ì— ì „í˜€ ì˜í–¥ ì—†ìŒ
        
        Args:
            system_prompt: ìºì‹±í•  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            model_name: ëª¨ë¸ ì´ë¦„
            cache_key: ìºì‹œ ì‹ë³„ì (ì„¸ì…˜ ID ë“±)
        
        Returns:
            CachedContent ê°ì²´ ë˜ëŠ” None (ìºì‹± ë¶ˆê°€ ì‹œ)
        """
        if not self._cache_enabled or not cache_key:
            return None
        
        try:
            # ê¸°ì¡´ ìºì‹œ ì¬ì‚¬ìš© (ìˆìœ¼ë©´)
            if cache_key in self._content_caches:
                cached = self._content_caches[cache_key]
                try:
                    # ìºì‹œ ìœ íš¨ì„± í™•ì¸
                    if hasattr(cached, 'expire_time') and cached.expire_time:
                        if datetime.now() < cached.expire_time:
                            logger.debug(f"â™»ï¸ [Cache] ê¸°ì¡´ ìºì‹œ ì¬ì‚¬ìš©: {cache_key[:8]}...")
                            return cached
                    
                    # ë§Œë£Œëœ ìºì‹œ ì •ë¦¬
                    await asyncio.to_thread(cached.delete)
                    del self._content_caches[cache_key]
                    logger.debug(f"ğŸ—‘ï¸ [Cache] ë§Œë£Œëœ ìºì‹œ ì •ë¦¬")
                except:
                    # ì •ë¦¬ ì‹¤íŒ¨í•´ë„ ë¬´ì‹œí•˜ê³  ì§„í–‰
                    pass
            
            # í† í° ìˆ˜ ì¶”ì • (4ì â‰ˆ 1í† í°, ë³´ìˆ˜ì  ì¶”ì •)
            estimated_tokens = len(system_prompt) // 4
            
            # ìµœì†Œ í† í° ìˆ˜ ì²´í¬ (Gemini 3 Flash: 1,024 í† í°)
            if estimated_tokens < 1024:
                logger.debug(f"â­ï¸ [Cache] í† í° ë¶€ì¡± ({estimated_tokens} < 1024), ì¼ë°˜ ëª¨ë“œ ì‚¬ìš©")
                return None
            
            # ìƒˆ ìºì‹œ ìƒì„± ì‹œë„
            logger.debug(f"ğŸ“¦ [Cache] ìƒˆ ìºì‹œ ìƒì„± ì¤‘... (~{estimated_tokens} tokens)")
            
            cached_content = await asyncio.to_thread(
                caching.CachedContent.create,
                model=model_name,
                display_name=f"welno_rag_{cache_key[:16]}",
                system_instruction=system_prompt,
                ttl=timedelta(hours=1)
            )
            
            self._content_caches[cache_key] = cached_content
            logger.info(f"âœ… [Cache] ìºì‹œ ìƒì„± ì™„ë£Œ (30-50% ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ)")
            
            return cached_content
            
        except Exception as e:
            # ëª¨ë“  ìºì‹± ì—ëŸ¬ëŠ” ì¡°ìš©íˆ ë¬´ì‹œí•˜ê³  ì¼ë°˜ ëª¨ë“œë¡œ ì§„í–‰
            logger.debug(f"â­ï¸ [Cache] ìºì‹± ë¶ˆê°€ (ì¼ë°˜ ëª¨ë“œ): {str(e)[:50]}...")
            return None
    
    async def clear_cache(self, cache_key: str):
        """íŠ¹ì • ì„¸ì…˜ì˜ ìºì‹œ ì‚­ì œ"""
        if cache_key in self._content_caches:
            try:
                cached = self._content_caches[cache_key]
                await asyncio.to_thread(cached.delete)
                del self._content_caches[cache_key]
                logger.info(f"ğŸ—‘ï¸ [Context Cache] ìºì‹œ ì‚­ì œ ì™„ë£Œ: {cache_key}")
            except Exception as e:
                logger.warning(f"âš ï¸ [Context Cache] ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
gemini_service = GeminiService()

