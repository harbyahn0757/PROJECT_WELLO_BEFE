"""
íŒŒì¼ ìš°ì„  ì €ì¥ í›„ DB ì…ë ¥ ì „ëµì„ ìœ„í•œ ë°ì´í„° ì„œë¹„ìŠ¤
ì•ˆì •ì„±ê³¼ ë°ì´í„° ë¬´ê²°ì„±ì„ ë³´ì¥í•˜ëŠ” 2ë‹¨ê³„ ì €ì¥ ë°©ì‹
"""

import os
import json
import hashlib
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
import asyncio
import shutil

from app.services.wello_data_service import WelloDataService


class FileFirstDataService:
    """íŒŒì¼ ìš°ì„  ì €ì¥ í›„ DB ì…ë ¥ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.base_dir = Path("/home/workspace/PROJECT_WELLO_BEFE/tilko_data")
        self.pending_dir = self.base_dir / "pending"  # DB ì…ë ¥ ëŒ€ê¸° ì¤‘
        self.completed_dir = self.base_dir / "completed"  # DB ì…ë ¥ ì™„ë£Œ
        self.failed_dir = self.base_dir / "failed"  # DB ì…ë ¥ ì‹¤íŒ¨
        self.backup_dir = self.base_dir / "backup"  # ë°±ì—…
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for directory in [self.pending_dir, self.completed_dir, self.failed_dir, self.backup_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        self.wello_service = WelloDataService()
    
    def _generate_file_hash(self, data: Dict[str, Any]) -> str:
        """ë°ì´í„°ì˜ í•´ì‹œê°’ ìƒì„± (ë¬´ê²°ì„± ê²€ì¦ìš©)"""
        data_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(data_str.encode('utf-8')).hexdigest()
    
    def _create_data_package(self, session_id: str, data_type: str, raw_data: Dict[str, Any], 
                           patient_uuid: str = None, hospital_id: str = None) -> Dict[str, Any]:
        """ë°ì´í„° íŒ¨í‚¤ì§€ ìƒì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)"""
        timestamp = datetime.now()
        
        package = {
            "metadata": {
                "session_id": session_id,
                "data_type": data_type,  # 'health_data', 'prescription_data', 'patient_data'
                "patient_uuid": patient_uuid,
                "hospital_id": hospital_id,
                "created_at": timestamp.isoformat(),
                "file_version": "1.0",
                "status": "pending"  # pending, processing, completed, failed
            },
            "raw_data": raw_data,
            "checksum": None  # ì•„ë˜ì—ì„œ ê³„ì‚°
        }
        
        # ì²´í¬ì„¬ ê³„ì‚° (ë©”íƒ€ë°ì´í„° + raw_data)
        package["checksum"] = self._generate_file_hash(package)
        
        return package
    
    async def save_data_to_file_first(self, session_id: str, data_type: str, raw_data: Dict[str, Any],
                                    patient_uuid: str = None, hospital_id: str = None) -> Tuple[bool, str]:
        """
        1ë‹¨ê³„: ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ë¨¼ì € ì €ì¥
        Returns: (ì„±ê³µì—¬ë¶€, íŒŒì¼ê²½ë¡œ)
        """
        try:
            # ë°ì´í„° íŒ¨í‚¤ì§€ ìƒì„±
            data_package = self._create_data_package(session_id, data_type, raw_data, patient_uuid, hospital_id)
            
            # íŒŒì¼ëª… ìƒì„±: timestamp_sessionId_dataType.json
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ë°€ë¦¬ì´ˆ í¬í•¨
            filename = f"{timestamp}_{session_id}_{data_type}.json"
            file_path = self.pending_dir / filename
            
            # íŒŒì¼ ì €ì¥
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data_package, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… [íŒŒì¼ì €ì¥] {data_type} ë°ì´í„° íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            print(f"   ğŸ“ ê²½ë¡œ: {file_path}")
            print(f"   ğŸ”’ ì²´í¬ì„¬: {data_package['checksum'][:16]}...")
            
            return True, str(file_path)
            
        except Exception as e:
            print(f"âŒ [íŒŒì¼ì €ì¥] {data_type} ë°ì´í„° íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False, ""
    
    async def process_pending_files_to_db(self, max_files: int = 10) -> Dict[str, int]:
        """
        2ë‹¨ê³„: pending í´ë”ì˜ íŒŒì¼ë“¤ì„ DBì— ì €ì¥
        Returns: ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        results = {"success": 0, "failed": 0, "skipped": 0}
        
        try:
            # pending í´ë”ì˜ JSON íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸° (ì˜¤ë˜ëœ ìˆœì„œëŒ€ë¡œ)
            pending_files = sorted(
                [f for f in self.pending_dir.glob("*.json")],
                key=lambda x: x.stat().st_mtime
            )[:max_files]
            
            print(f"ğŸ”„ [DBì €ì¥] pending íŒŒì¼ {len(pending_files)}ê°œ ì²˜ë¦¬ ì‹œì‘")
            
            for file_path in pending_files:
                try:
                    # íŒŒì¼ ì½ê¸° ë° ê²€ì¦
                    success, data_package = await self._load_and_validate_file(file_path)
                    if not success:
                        results["skipped"] += 1
                        continue
                    
                    # DB ì €ì¥ ì‹œë„
                    db_success = await self._save_package_to_db(data_package)
                    
                    if db_success:
                        # ì„±ê³µ: completed í´ë”ë¡œ ì´ë™
                        await self._move_file_to_completed(file_path, data_package)
                        results["success"] += 1
                        print(f"âœ… [DBì €ì¥] íŒŒì¼ ì²˜ë¦¬ ì„±ê³µ: {file_path.name}")
                    else:
                        # ì‹¤íŒ¨: failed í´ë”ë¡œ ì´ë™
                        await self._move_file_to_failed(file_path, data_package)
                        results["failed"] += 1
                        print(f"âŒ [DBì €ì¥] íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path.name}")
                        
                except Exception as e:
                    print(f"âŒ [DBì €ì¥] íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({file_path.name}): {e}")
                    results["failed"] += 1
            
            print(f"ğŸ [DBì €ì¥] ì²˜ë¦¬ ì™„ë£Œ - ì„±ê³µ: {results['success']}, ì‹¤íŒ¨: {results['failed']}, ê±´ë„ˆëœ€: {results['skipped']}")
            
        except Exception as e:
            print(f"âŒ [DBì €ì¥] pending íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
        
        return results
    
    async def _load_and_validate_file(self, file_path: Path) -> Tuple[bool, Optional[Dict[str, Any]]]:
        """íŒŒì¼ ë¡œë“œ ë° ë¬´ê²°ì„± ê²€ì¦"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data_package = json.load(f)
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if not all(key in data_package for key in ["metadata", "raw_data", "checksum"]):
                print(f"âš ï¸ [íŒŒì¼ê²€ì¦] í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {file_path.name}")
                return False, None
            
            # ì²´í¬ì„¬ ê²€ì¦
            original_checksum = data_package["checksum"]
            data_package_copy = data_package.copy()
            data_package_copy["checksum"] = None
            calculated_checksum = self._generate_file_hash(data_package_copy)
            
            if original_checksum != calculated_checksum:
                print(f"âŒ [íŒŒì¼ê²€ì¦] ì²´í¬ì„¬ ë¶ˆì¼ì¹˜: {file_path.name}")
                print(f"   ì›ë³¸: {original_checksum[:16]}...")
                print(f"   ê³„ì‚°: {calculated_checksum[:16]}...")
                return False, None
            
            print(f"âœ… [íŒŒì¼ê²€ì¦] ë¬´ê²°ì„± í™•ì¸ ì™„ë£Œ: {file_path.name}")
            return True, data_package
            
        except Exception as e:
            print(f"âŒ [íŒŒì¼ê²€ì¦] íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path.name}): {e}")
            return False, None
    
    async def _save_package_to_db(self, data_package: Dict[str, Any]) -> bool:
        """ë°ì´í„° íŒ¨í‚¤ì§€ë¥¼ DBì— ì €ì¥"""
        try:
            metadata = data_package["metadata"]
            raw_data = data_package["raw_data"]
            
            data_type = metadata["data_type"]
            patient_uuid = metadata.get("patient_uuid")
            hospital_id = metadata.get("hospital_id")
            session_id = metadata["session_id"]
            
            if data_type == "patient_data":
                # í™˜ì ì •ë³´ ì €ì¥
                result = await self.wello_service.save_patient_data(
                    patient_uuid, hospital_id, raw_data, session_id
                )
                return result is not None
                
            elif data_type == "health_data":
                # ê±´ê°•ê²€ì§„ ë°ì´í„° ì €ì¥
                result = await self.wello_service.save_health_data(
                    patient_uuid, hospital_id, raw_data, session_id
                )
                return result
                
            elif data_type == "prescription_data":
                # ì²˜ë°©ì „ ë°ì´í„° ì €ì¥
                result = await self.wello_service.save_prescription_data(
                    patient_uuid, hospital_id, raw_data, session_id
                )
                return result
                
            else:
                print(f"âš ï¸ [DBì €ì¥] ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° íƒ€ì…: {data_type}")
                return False
                
        except Exception as e:
            print(f"âŒ [DBì €ì¥] ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    async def _move_file_to_completed(self, file_path: Path, data_package: Dict[str, Any]):
        """ì„±ê³µí•œ íŒŒì¼ì„ completed í´ë”ë¡œ ì´ë™"""
        try:
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            data_package["metadata"]["status"] = "completed"
            data_package["metadata"]["completed_at"] = datetime.now().isoformat()
            
            # ìƒˆ íŒŒì¼ ê²½ë¡œ
            new_path = self.completed_dir / file_path.name
            
            # ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ë¡œ ì €ì¥
            with open(new_path, 'w', encoding='utf-8') as f:
                json.dump(data_package, f, ensure_ascii=False, indent=2)
            
            # ì›ë³¸ íŒŒì¼ ì‚­ì œ
            file_path.unlink()
            
        except Exception as e:
            print(f"âŒ [íŒŒì¼ì´ë™] completed ì´ë™ ì‹¤íŒ¨: {e}")
    
    async def _move_file_to_failed(self, file_path: Path, data_package: Dict[str, Any]):
        """ì‹¤íŒ¨í•œ íŒŒì¼ì„ failed í´ë”ë¡œ ì´ë™"""
        try:
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            data_package["metadata"]["status"] = "failed"
            data_package["metadata"]["failed_at"] = datetime.now().isoformat()
            data_package["metadata"]["retry_count"] = data_package["metadata"].get("retry_count", 0) + 1
            
            # ìƒˆ íŒŒì¼ ê²½ë¡œ
            new_path = self.failed_dir / file_path.name
            
            # ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ë¡œ ì €ì¥
            with open(new_path, 'w', encoding='utf-8') as f:
                json.dump(data_package, f, ensure_ascii=False, indent=2)
            
            # ì›ë³¸ íŒŒì¼ ì‚­ì œ
            file_path.unlink()
            
        except Exception as e:
            print(f"âŒ [íŒŒì¼ì´ë™] failed ì´ë™ ì‹¤íŒ¨: {e}")
    
    async def retry_failed_files(self, max_retries: int = 3) -> Dict[str, int]:
        """ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ ì¬ì‹œë„"""
        results = {"retry_success": 0, "retry_failed": 0, "max_retries_exceeded": 0}
        
        try:
            failed_files = list(self.failed_dir.glob("*.json"))
            print(f"ğŸ”„ [ì¬ì‹œë„] ì‹¤íŒ¨ íŒŒì¼ {len(failed_files)}ê°œ ì¬ì‹œë„ ì‹œì‘")
            
            for file_path in failed_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data_package = json.load(f)
                    
                    retry_count = data_package["metadata"].get("retry_count", 0)
                    
                    if retry_count >= max_retries:
                        results["max_retries_exceeded"] += 1
                        continue
                    
                    # DB ì €ì¥ ì¬ì‹œë„
                    db_success = await self._save_package_to_db(data_package)
                    
                    if db_success:
                        # ì„±ê³µ: completedë¡œ ì´ë™
                        await self._move_file_to_completed(file_path, data_package)
                        results["retry_success"] += 1
                    else:
                        # ì‹¤íŒ¨: ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€
                        data_package["metadata"]["retry_count"] = retry_count + 1
                        data_package["metadata"]["last_retry_at"] = datetime.now().isoformat()
                        
                        with open(file_path, 'w', encoding='utf-8') as f:
                            json.dump(data_package, f, ensure_ascii=False, indent=2)
                        
                        results["retry_failed"] += 1
                        
                except Exception as e:
                    print(f"âŒ [ì¬ì‹œë„] íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path.name}): {e}")
                    results["retry_failed"] += 1
            
            print(f"ğŸ [ì¬ì‹œë„] ì™„ë£Œ - ì„±ê³µ: {results['retry_success']}, ì‹¤íŒ¨: {results['retry_failed']}, ìµœëŒ€ì¬ì‹œë„ì´ˆê³¼: {results['max_retries_exceeded']}")
            
        except Exception as e:
            print(f"âŒ [ì¬ì‹œë„] ì „ì²´ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        return results
    
    async def get_status_summary(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ìš”ì•½"""
        try:
            pending_count = len(list(self.pending_dir.glob("*.json")))
            completed_count = len(list(self.completed_dir.glob("*.json")))
            failed_count = len(list(self.failed_dir.glob("*.json")))
            
            return {
                "pending_files": pending_count,
                "completed_files": completed_count,
                "failed_files": failed_count,
                "total_files": pending_count + completed_count + failed_count,
                "directories": {
                    "pending": str(self.pending_dir),
                    "completed": str(self.completed_dir),
                    "failed": str(self.failed_dir),
                    "backup": str(self.backup_dir)
                }
            }
        except Exception as e:
            print(f"âŒ [ìƒíƒœì¡°íšŒ] ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    async def cleanup_old_files(self, days_old: int = 30):
        """ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ (completed, backup í´ë”)"""
        try:
            from datetime import timedelta
            cutoff_date = datetime.now() - timedelta(days=days_old)
            
            cleaned_count = 0
            
            for directory in [self.completed_dir, self.backup_dir]:
                for file_path in directory.glob("*.json"):
                    file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if file_time < cutoff_date:
                        file_path.unlink()
                        cleaned_count += 1
            
            print(f"ğŸ§¹ [ì •ë¦¬] {days_old}ì¼ ì´ìƒ ëœ íŒŒì¼ {cleaned_count}ê°œ ì‚­ì œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ [ì •ë¦¬] íŒŒì¼ ì •ë¦¬ ì˜¤ë¥˜: {e}")

