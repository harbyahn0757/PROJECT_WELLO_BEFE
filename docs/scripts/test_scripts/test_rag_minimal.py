#!/usr/bin/env python3
"""
ìµœì†Œí•œì˜ RAG í…ŒìŠ¤íŠ¸ - OpenAIë§Œ ì‚¬ìš©
aquery vs retrieve ì§ì ‘ ë¹„êµ
"""
import asyncio
import time
import sys
import os
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
from dotenv import load_dotenv
load_dotenv('/home/workspace/PROJECT_WELLO_BEFE/planning-platform/backend/.env.local')

# ë°±ì—”ë“œ ê²½ë¡œ
sys.path.insert(0, '/home/workspace/PROJECT_WELLO_BEFE/planning-platform/backend')

async def test_with_openai():
    """OpenAIë¡œ ì§ì ‘ í…ŒìŠ¤íŠ¸"""
    
    print("=" * 80)
    print("ğŸ§ª RAG ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (OpenAI ì„ë² ë”©)")
    print("=" * 80)
    print()
    
    # LlamaIndex ì„í¬íŠ¸
    try:
        from llama_index.core import load_index_from_storage, StorageContext, Settings
        from llama_index.embeddings.openai import OpenAIEmbedding
        from llama_index.llms.openai import OpenAI
        import faiss
        from pathlib import Path as PathLib
        
        # FAISS ë²¡í„° ìŠ¤í† ì–´
        from llama_index.vector_stores.faiss import FaissVectorStore
        from llama_index.core.storage.docstore import SimpleDocumentStore
        from llama_index.core.storage.index_store import SimpleIndexStore
        
    except ImportError as e:
        print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ: {e}")
        return False
    
    # ì„¤ì •
    LOCAL_FAISS_DIR = "/data/vector_db/welno/faiss_db"
    LOCAL_FAISS_INDEX_PATH = f"{LOCAL_FAISS_DIR}/faiss.index"
    
    print("ğŸ“š FAISS ì¸ë±ìŠ¤ ë¡œë“œ...")
    
    try:
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        faiss_index = faiss.read_index(LOCAL_FAISS_INDEX_PATH)
        print(f"âœ… FAISS ì¸ë±ìŠ¤: {faiss_index.ntotal}ê°œ ë²¡í„°")
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vector_store = FaissVectorStore(faiss_index=faiss_index)
        docstore = SimpleDocumentStore.from_persist_dir(LOCAL_FAISS_DIR)
        index_store = SimpleIndexStore.from_persist_dir(LOCAL_FAISS_DIR)
        
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store,
            docstore=docstore,
            index_store=index_store
        )
        
        # ì¸ë±ìŠ¤ ë¡œë“œ
        try:
            index = load_index_from_storage(storage_context)
        except ValueError:
            # ì—¬ëŸ¬ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš©
            index_structs = storage_context.index_store.index_structs()
            if isinstance(index_structs, dict):
                index_id = list(index_structs.keys())[0]
            else:
                index_id = index_structs[0].index_id
            print(f"   ì¸ë±ìŠ¤ ID: {index_id}")
            index = load_index_from_storage(storage_context, index_id=index_id)
        
        print("âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        
        # OpenAI ì„¤ì •
        Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0.0)
        Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
        
        # Query Engine ìƒì„±
        query_engine = index.as_query_engine(
            similarity_top_k=5,
            response_mode="compact"
        )
        
        print("âœ… Query Engine ìƒì„± ì™„ë£Œ")
        print()
        
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_query = "44ì„¸ ë‚¨ì„± ê³ í˜ˆì•• ê´€ë ¨ ê¶Œì¥ ê²€ì§„"
    
    print(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")
    print()
    
    # ========================================
    # Test 1: aquery()
    # ========================================
    print("-" * 80)
    print("1ï¸âƒ£  aquery() - LLM ì‘ë‹µ ìƒì„± í¬í•¨")
    print("-" * 80)
    
    try:
        start = time.time()
        response = await query_engine.aquery(test_query)
        elapsed_aquery = time.time() - start
        
        sources = response.source_nodes if hasattr(response, 'source_nodes') else []
        
        print(f"â±ï¸  ì‹œê°„: {elapsed_aquery:.3f}ì´ˆ")
        print(f"ğŸ“ ì‘ë‹µ: {len(str(response))}ì")
        print(f"ğŸ“š ë¬¸ì„œ: {len(sources)}ê°œ")
        
        if sources:
            print(f"   ì²« ë¬¸ì„œ: {sources[0].text[:80]}...")
        print()
        
    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {e}")
        return False
    
    # ========================================
    # Test 2: retrieve()
    # ========================================
    print("-" * 80)
    print("2ï¸âƒ£  retrieve() - ë²¡í„° ê²€ìƒ‰ë§Œ")
    print("-" * 80)
    
    try:
        start = time.time()
        nodes = await query_engine.aretrieve(test_query)
        elapsed_retrieve = time.time() - start
        
        print(f"â±ï¸  ì‹œê°„: {elapsed_retrieve:.3f}ì´ˆ")
        print(f"ğŸ“š ë¬¸ì„œ: {len(nodes)}ê°œ")
        
        if nodes:
            print(f"   ì²« ë¬¸ì„œ: {nodes[0].text[:80]}...")
        print()
        
    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {e}")
        return False
    
    # ========================================
    # ê²°ê³¼
    # ========================================
    print("=" * 80)
    print("ğŸ“Š ê²°ê³¼")
    print("=" * 80)
    print()
    print(f"â±ï¸  aquery():   {elapsed_aquery:.3f}ì´ˆ")
    print(f"â±ï¸  retrieve(): {elapsed_retrieve:.3f}ì´ˆ")
    print(f"ğŸ“ˆ ê°œì„ :       {elapsed_aquery - elapsed_retrieve:.3f}ì´ˆ ({(1-elapsed_retrieve/elapsed_aquery)*100:.1f}%)")
    print(f"ğŸ“š ë¬¸ì„œ:       aquery {len(sources)}ê°œ vs retrieve {len(nodes)}ê°œ")
    print()
    
    if elapsed_retrieve < elapsed_aquery * 0.5:
        print("âœ… retrieve()ê°€ 50% ì´ìƒ ë¹ ë¦„!")
        print("âœ… ê²€ì§„ ì„¤ê³„ì— ì¦‰ì‹œ ì ìš© ê°€ëŠ¥")
        return True
    else:
        print(f"âš ï¸  ê°œì„  íš¨ê³¼: {(1-elapsed_retrieve/elapsed_aquery)*100:.1f}%")
        return True

if __name__ == "__main__":
    try:
        result = asyncio.run(test_with_openai())
        sys.exit(0 if result else 1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
